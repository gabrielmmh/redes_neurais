{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Template de Entrega","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#grupokit-x","title":"Grupo/Kit X","text":"<ol> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> <li>Maria Oliveira</li> <li>Grupo K<ul> <li>Jo\u00e3o da Silva</li> <li>Pedro de Souza</li> </ul> </li> </ol> <p>Instru\u00e7\u00f5es</p> <p>Voc\u00eas devem utilizar este template como um bloco de notas para registrar o que foi feito e o que falta fazer. Voc\u00eas devem adicionar as informa\u00e7\u00f5es necess\u00e1rias. O template deve ser editado e atualizado a cada entrega, registrando assim a data de entrega e o que foi feito at\u00e9 o momento via Git.</p>"},{"location":"#entregas","title":"Entregas","text":"<ul> <li> Roteiro 1 - Data 23/02/2025</li> <li> Roteiro 2</li> <li> Roteiro 3</li> <li> Roteiro 4</li> <li> Projeto</li> </ul>"},{"location":"#diagramas","title":"Diagramas","text":"<p>Use o Mermaid para criar os diagramas de documenta\u00e7\u00e3o.</p> <p>Mermaid Live Editor</p> <pre><code>flowchart TD\n    Deployment:::orange --&gt;|defines| ReplicaSet\n    ReplicaSet --&gt;|manages| pod((Pod))\n    pod:::red --&gt;|runs| Container\n    Deployment --&gt;|scales| pod\n    Deployment --&gt;|updates| pod\n\n    Service:::orange --&gt;|exposes| pod\n\n    subgraph  \n        ConfigMap:::orange\n        Secret:::orange\n    end\n\n    ConfigMap --&gt; Deployment\n    Secret --&gt; Deployment\n    classDef red fill:#f55\n    classDef orange fill:#ffa500</code></pre>"},{"location":"#codigos","title":"C\u00f3digos","text":"De um arquivo remotoAnota\u00e7\u00f5es no c\u00f3digo main.yaml<pre><code>name: ci\non:\n  - push\n  - pull_request\n\n# Environment\nenv:\n  CI: true\n  PYTHON_VERSION: 3.12\n\n# Jobs to run\njobs:\n\n  # Build and deploy documentation site\n  deploy:\n    if: github.event_name != 'pull_request' &amp;&amp; github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n\n      # Checkout source form GitHub\n      - uses: actions/checkout@v4\n\n      # Install Python runtime and dependencies\n      - uses: actions/setup-python@v4\n        with:\n          python-version: ${{ env.PYTHON_VERSION }}\n\n      # pip\n      - run: |\n          pip install -r requirements.txt\n\n      # deploy\n      - run: |\n          mkdocs gh-deploy --force\n</code></pre> compose.yaml<pre><code>name: app\n\n    db:\n        image: postgres:17\n        environment:\n            POSTGRES_DB: ${POSTGRES_DB:-projeto} # (1)!\n            POSTGRES_USER: ${POSTGRES_USER:-projeto}\n            POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-projeto}\n        ports:\n            - 5432:5432 #(2)!\n</code></pre> <ol> <li> <p>Caso a vari\u00e1vel de ambiente <code>POSTGRES_DB</code> n\u00e3o exista ou seja nula - n\u00e3o seja definida no arquivo <code>.env</code> - o valor padr\u00e3o ser\u00e1 <code>projeto</code>. Vide documenta\u00e7\u00e3o.</p> </li> <li> <p>Aqui \u00e9 feito um t\u00fanel da porta 5432 do container do banco de dados para a porta 5432 do host (no caso localhost). Em um ambiente de produ\u00e7\u00e3o, essa porta n\u00e3o deve ser exposta, pois ningu\u00e9m de fora do compose deveria acessar o banco de dados diretamente.</p> </li> </ol>"},{"location":"#exemplo-de-video","title":"Exemplo de v\u00eddeo","text":"<p>Lorem ipsum dolor sit amet</p>"},{"location":"#referencias","title":"Refer\u00eancias","text":"<p>Material for MkDocs</p>"},{"location":"exercises/01_data/data/","title":"Prepara\u00e7\u00e3o e An\u00e1lise de Dados para Redes Neurais","text":"In\u00a0[7]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nN = 100\nparams = {\n    0: {\"mean\": [2, 3],  \"std\": [0.8, 2.5]},\n    1: {\"mean\": [5, 6],  \"std\": [1.2, 1.9]},\n    2: {\"mean\": [8, 1],  \"std\": [0.9, 0.9]},\n    3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]},\n}\n\nXs, ys = [], []\nfor c, p in params.items():\n    mean = np.array(p[\"mean\"])\n    std = np.array(p[\"std\"])\n    Xc = np.random.randn(N, 2) * std + mean\n    Xs.append(Xc)\n    ys.append(np.full(N, c))\n\nX = np.vstack(Xs)\ny = np.hstack(ys)\n\nplt.figure(figsize=(7,5))\nfor c in params.keys():\n    plt.scatter(X[y==c,0], X[y==c,1], s=12, label=f\"Classe {c}\", alpha=0.8)\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.title(\"Exerc\u00edcio 1 \u2014 Distribui\u00e7\u00e3o 2D (4 classes)\")\nplt.legend()\nplt.show()\n</pre>  import numpy as np import matplotlib.pyplot as plt  np.random.seed(42)  N = 100 params = {     0: {\"mean\": [2, 3],  \"std\": [0.8, 2.5]},     1: {\"mean\": [5, 6],  \"std\": [1.2, 1.9]},     2: {\"mean\": [8, 1],  \"std\": [0.9, 0.9]},     3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]}, }  Xs, ys = [], [] for c, p in params.items():     mean = np.array(p[\"mean\"])     std = np.array(p[\"std\"])     Xc = np.random.randn(N, 2) * std + mean     Xs.append(Xc)     ys.append(np.full(N, c))  X = np.vstack(Xs) y = np.hstack(ys)  plt.figure(figsize=(7,5)) for c in params.keys():     plt.scatter(X[y==c,0], X[y==c,1], s=12, label=f\"Classe {c}\", alpha=0.8) plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.title(\"Exerc\u00edcio 1 \u2014 Distribui\u00e7\u00e3o 2D (4 classes)\") plt.legend() plt.show()  <p>As quatro classes se distribuem em regi\u00f5es distintas do plano, cada uma concentrada em torno de um centro espec\u00edfico. Embora o eixo x1 contribua fortemente para a separa\u00e7\u00e3o, a variabilidade em x2 cria dispers\u00f5es diferentes entre os grupos. Esse cen\u00e1rio exige a combina\u00e7\u00e3o de m\u00faltiplas fronteiras de decis\u00e3o ou o uso de modelos n\u00e3o lineares para capturar de forma adequada a estrutura dos dados.</p> In\u00a0[8]: Copied! <pre># Superf\u00edcies de decis\u00e3o com MLP\nfrom sklearn.neural_network import MLPClassifier\n\nclf = MLPClassifier(hidden_layer_sizes=(16,), activation=\"tanh\", max_iter=2000, random_state=42)\nclf.fit(X, y)\n\nxx, yy = np.meshgrid(\n    np.linspace(X[:,0].min()-1, X[:,0].max()+1, 300),\n    np.linspace(X[:,1].min()-1, X[:,1].max()+1, 300),\n)\nZZ = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\nplt.figure(figsize=(7,5))\nplt.contourf(xx, yy, ZZ, alpha=0.25, levels=[-0.5,0.5,1.5,2.5,3.5])\nfor c in params.keys():\n    plt.scatter(X[y==c,0], X[y==c,1], s=10, label=f\"Classe {c}\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.title(\"Exerc\u00edcio 1 \u2014 Superf\u00edcies de decis\u00e3o (MLP tanh)\")\nplt.legend()\nplt.show()\n</pre>  # Superf\u00edcies de decis\u00e3o com MLP from sklearn.neural_network import MLPClassifier  clf = MLPClassifier(hidden_layer_sizes=(16,), activation=\"tanh\", max_iter=2000, random_state=42) clf.fit(X, y)  xx, yy = np.meshgrid(     np.linspace(X[:,0].min()-1, X[:,0].max()+1, 300),     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 300), ) ZZ = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)  plt.figure(figsize=(7,5)) plt.contourf(xx, yy, ZZ, alpha=0.25, levels=[-0.5,0.5,1.5,2.5,3.5]) for c in params.keys():     plt.scatter(X[y==c,0], X[y==c,1], s=10, label=f\"Classe {c}\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.title(\"Exerc\u00edcio 1 \u2014 Superf\u00edcies de decis\u00e3o (MLP tanh)\") plt.legend() plt.show()  In\u00a0[9]: Copied! <pre>from sklearn.decomposition import PCA\n\nnp.random.seed(7)\n\nmuA = np.array([0, 0, 0, 0, 0])\nSigmaA = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\nmuB = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigmaB = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\nXA = np.random.multivariate_normal(muA, SigmaA, size=500)\nXB = np.random.multivariate_normal(muB, SigmaB, size=500)\nX5 = np.vstack([XA, XB])\ny5 = np.hstack([np.zeros(500), np.ones(500)])\n\npca = PCA(n_components=2, random_state=42)\nX2 = pca.fit_transform(X5)\n\nplt.figure(figsize=(7,5))\nplt.scatter(X2[y5==0,0], X2[y5==0,1], s=10, label=\"Classe A\")\nplt.scatter(X2[y5==1,0], X2[y5==1,1], s=10, label=\"Classe B\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.title(\"Exerc\u00edcio 2 \u2014 PCA (5D \u2192 2D)\")\nplt.legend()\nplt.show()\n</pre>  from sklearn.decomposition import PCA  np.random.seed(7)  muA = np.array([0, 0, 0, 0, 0]) SigmaA = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  muB = np.array([1.5, 1.5, 1.5, 1.5, 1.5]) SigmaB = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  XA = np.random.multivariate_normal(muA, SigmaA, size=500) XB = np.random.multivariate_normal(muB, SigmaB, size=500) X5 = np.vstack([XA, XB]) y5 = np.hstack([np.zeros(500), np.ones(500)])  pca = PCA(n_components=2, random_state=42) X2 = pca.fit_transform(X5)  plt.figure(figsize=(7,5)) plt.scatter(X2[y5==0,0], X2[y5==0,1], s=10, label=\"Classe A\") plt.scatter(X2[y5==1,0], X2[y5==1,1], s=10, label=\"Classe B\") plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\") plt.title(\"Exerc\u00edcio 2 \u2014 PCA (5D \u2192 2D)\") plt.legend() plt.show()  <p>A proje\u00e7\u00e3o via PCA mostra que as classes A e B t\u00eam centros deslocados, mas ainda apresentam forte sobreposi\u00e7\u00e3o devido \u00e0 vari\u00e2ncia dentro de cada grupo. Essa configura\u00e7\u00e3o torna a separa\u00e7\u00e3o linear pouco eficaz, j\u00e1 que n\u00e3o existe um hiperplano simples que separe bem as duas classes. Modelos mais expressivos, que incorporam n\u00e3o-linearidades, s\u00e3o mais adequados para capturar as fronteiras complexas observadas no espa\u00e7o reduzido.</p> <p>O dataset Spaceship Titanic foi proposto em uma competi\u00e7\u00e3o do Kaggle e tem como objetivo prever a vari\u00e1vel Transported, que indica se um passageiro foi levado para outra dimens\u00e3o durante a viagem.</p> <p>Os dados incluem atributos num\u00e9ricos, como <code>Age</code> e os gastos a bordo (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>), e atributos categ\u00f3ricos, como <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code> e <code>Cabin</code>. Antes do treinamento de uma rede neural, \u00e9 necess\u00e1rio inspecionar a base para identificar tipos de vari\u00e1veis, valores ausentes e definir as transforma\u00e7\u00f5es adequadas.</p> In\u00a0[10]: Copied! <pre>import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nplt.rcParams[\"figure.figsize\"] = (8,3.5)\n\ncsv_path = \"data/train.csv\"\nassert os.path.exists(csv_path), f\"Arquivo n\u00e3o encontrado: {csv_path}\"\n\ndf = pd.read_csv(csv_path)\n\ntarget_col = \"Transported\"\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\ncat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Cabin\"]\n\n# Colunas de gastos com forte assimetria (usar log1p)\nspend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n</pre> import os, numpy as np, pandas as pd, matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer plt.rcParams[\"figure.figsize\"] = (8,3.5)  csv_path = \"data/train.csv\" assert os.path.exists(csv_path), f\"Arquivo n\u00e3o encontrado: {csv_path}\"  df = pd.read_csv(csv_path)  target_col = \"Transported\" num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] cat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Cabin\"]  # Colunas de gastos com forte assimetria (usar log1p) spend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] In\u00a0[11]: Copied! <pre>print(\"Tipos detectados e relat\u00f3rio de valores ausentes:\")\ndisplay(df[num_cols + cat_cols + [target_col]].dtypes)\n\nna_count = df[num_cols + cat_cols + [target_col]].isna().sum().sort_values(ascending=False)\nna_pct = (na_count / len(df)).round(3)\nmissing_report = pd.DataFrame({\"missing\": na_count, \"pct\": na_pct})\ndisplay(missing_report[missing_report[\"missing\"] &gt; 0])\n</pre> print(\"Tipos detectados e relat\u00f3rio de valores ausentes:\") display(df[num_cols + cat_cols + [target_col]].dtypes)  na_count = df[num_cols + cat_cols + [target_col]].isna().sum().sort_values(ascending=False) na_pct = (na_count / len(df)).round(3) missing_report = pd.DataFrame({\"missing\": na_count, \"pct\": na_pct}) display(missing_report[missing_report[\"missing\"] &gt; 0]) <pre>Tipos detectados e relat\u00f3rio de valores ausentes:\n</pre> <pre>Age             float64\nRoomService     float64\nFoodCourt       float64\nShoppingMall    float64\nSpa             float64\nVRDeck          float64\nHomePlanet       object\nCryoSleep        object\nDestination      object\nVIP              object\nCabin            object\nTransported        bool\ndtype: object</pre> missing pct CryoSleep 217 0.025 ShoppingMall 208 0.024 VIP 203 0.023 HomePlanet 201 0.023 Cabin 199 0.023 VRDeck 188 0.022 FoodCourt 183 0.021 Spa 183 0.021 Destination 182 0.021 RoomService 181 0.021 Age 179 0.021 <p>A inspe\u00e7\u00e3o revelou que os atributos de gastos (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>) e <code>Age</code> s\u00e3o num\u00e9ricos, enquanto <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code> e <code>Cabin</code> s\u00e3o categ\u00f3ricos.</p> <p>O relat\u00f3rio de valores ausentes mostrou taxas pr\u00f3ximas de 2% em quase todas as colunas. Isso indica a necessidade de imputa\u00e7\u00e3o sistem\u00e1tica: a mediana para atributos num\u00e9ricos, robusta a outliers, e o valor mais frequente para categ\u00f3ricos, preservando consist\u00eancia.</p> <p>Al\u00e9m disso, os atributos num\u00e9ricos n\u00e3o seguem todos a mesma distribui\u00e7\u00e3o: <code>Age</code> tem assimetria moderada, enquanto os gastos apresentam caudas longas e forte concentra\u00e7\u00e3o em zero. Essa diferen\u00e7a motiva estrat\u00e9gias espec\u00edficas de transforma\u00e7\u00e3o, ilustradas a seguir.</p> In\u00a0[12]: Copied! <pre>def plot_before_after(series, transformer, title_after):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3.5))\n    ax1.hist(series.dropna().values, bins=40)\n    ax1.set_title(f\"{series.name} \u2014 original\")\n    tr = transformer.fit_transform(series.to_frame())\n    ax2.hist(tr.ravel(), bins=40)\n    ax2.set_title(f\"{series.name} \u2014 {title_after}\")\n    plt.tight_layout(); plt.show()\n\nage_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\nplot_before_after(df[\"Age\"], age_pipe, \"padronizada (StandardScaler)\")\n</pre> def plot_before_after(series, transformer, title_after):     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3.5))     ax1.hist(series.dropna().values, bins=40)     ax1.set_title(f\"{series.name} \u2014 original\")     tr = transformer.fit_transform(series.to_frame())     ax2.hist(tr.ravel(), bins=40)     ax2.set_title(f\"{series.name} \u2014 {title_after}\")     plt.tight_layout(); plt.show()  age_pipe = Pipeline([     (\"imputer\", SimpleImputer(strategy=\"median\")),     (\"scaler\", StandardScaler()) ]) plot_before_after(df[\"Age\"], age_pipe, \"padronizada (StandardScaler)\")  In\u00a0[13]: Copied! <pre>food_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"log\", FunctionTransformer(np.log1p, validate=False)),\n    (\"scaler\", StandardScaler())\n])\nplot_before_after(df[\"FoodCourt\"], food_pipe, \"log1p + padronizada\")\n</pre> food_pipe = Pipeline([     (\"imputer\", SimpleImputer(strategy=\"median\")),     (\"log\", FunctionTransformer(np.log1p, validate=False)),     (\"scaler\", StandardScaler()) ]) plot_before_after(df[\"FoodCourt\"], food_pipe, \"log1p + padronizada\")  <p>Os gr\u00e1ficos confirmam o efeito das transforma\u00e7\u00f5es. Em <code>Age</code>, a padroniza\u00e7\u00e3o centralizou a distribui\u00e7\u00e3o em torno de zero e ajustou a escala para desvio padr\u00e3o um, tornando os valores mais adequados para fun\u00e7\u00f5es de ativa\u00e7\u00e3o como <code>tanh</code>.</p> <p>J\u00e1 em <code>FoodCourt</code>, a aplica\u00e7\u00e3o de <code>log1p</code> antes da padroniza\u00e7\u00e3o reduziu o impacto de caudas longas e outliers, resultando em uma distribui\u00e7\u00e3o mais equilibrada. O mesmo racioc\u00ednio pode ser aplicado \u00e0s demais vari\u00e1veis de gastos.</p> <p>Por fim, os atributos categ\u00f3ricos ser\u00e3o tratados com imputa\u00e7\u00e3o do valor mais frequente e convertidos em indicadores bin\u00e1rios via one-hot encoding. O dataset resultante torna-se mais homog\u00eaneo e apropriado para redes neurais, favorecendo a estabilidade do gradiente e o desempenho do modelo.</p>"},{"location":"exercises/01_data/data/#preparacao-e-analise-de-dados-para-redes-neurais","title":"Prepara\u00e7\u00e3o e An\u00e1lise de Dados para Redes Neurais\u00b6","text":""},{"location":"exercises/01_data/data/#objetivo","title":"Objetivo\u00b6","text":"<p>Explorar separabilidade de classes em 2D, projetar dados 5D para 2D com PCA e preparar o dataset Spaceship Titanic para redes neurais com ativa\u00e7\u00e3o <code>tanh</code>.</p>"},{"location":"exercises/01_data/data/#exercicio-1-dados-2d-4-classes","title":"Exerc\u00edcio 1 \u2014 Dados 2D (4 classes)\u00b6","text":""},{"location":"exercises/01_data/data/#exercicio-2-dados-5d-ab-pca2d","title":"Exerc\u00edcio 2 \u2014 Dados 5D (A/B) + PCA(2D)\u00b6","text":""},{"location":"exercises/01_data/data/#exercicio-3-spaceship-titanic-pre-processamento","title":"Exerc\u00edcio 3 \u2014 Spaceship Titanic: pr\u00e9-processamento\u00b6","text":""},{"location":"exercises/01_data/data/#conclusoes","title":"Conclus\u00f5es\u00b6","text":"<p>Os experimentos mostraram que, em dados sint\u00e9ticos 2D, a separa\u00e7\u00e3o linear n\u00e3o \u00e9 suficiente, exigindo fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares para capturar fronteiras mais complexas. Na proje\u00e7\u00e3o dos dados 5D em 2D, a sobreposi\u00e7\u00e3o causada por correla\u00e7\u00f5es entre atributos refor\u00e7a essa limita\u00e7\u00e3o e destaca a necessidade de modelos mais expressivos. J\u00e1 no caso do Spaceship Titanic, o pr\u00e9-processamento com imputa\u00e7\u00e3o, codifica\u00e7\u00e3o categ\u00f3rica e padroniza\u00e7\u00e3o num\u00e9rica foi fundamental para tornar o conjunto compat\u00edvel com redes neurais baseadas em <code>tanh</code>, garantindo maior estabilidade no treinamento e melhor capacidade de generaliza\u00e7\u00e3o.</p>"},{"location":"exercises/02_perceptron/perceptron/","title":"Entendendo Perceptrons e Suas Limita\u00e7\u00f5es","text":"In\u00a0[65]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n</pre> import numpy as np import matplotlib.pyplot as plt  np.random.seed(42) In\u00a0[66]: Copied! <pre># par\u00e2metros\nmean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]]\nmean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]\n\n# gerar amostras\nn_samples = 1000\nclass0 = np.random.multivariate_normal(mean0, cov0, n_samples)\nclass1 = np.random.multivariate_normal(mean1, cov1, n_samples)\n\n# juntar\nX = np.vstack((class0, class1))\ny = np.hstack((-1*np.ones(n_samples), +1*np.ones(n_samples)))  # r\u00f3tulos {-1, +1}\n\n# plot\nplt.figure(figsize=(6,6))\nplt.scatter(class0[:,0], class0[:,1], alpha=0.6, label=\"Classe 0\")\nplt.scatter(class1[:,0], class1[:,1], alpha=0.6, label=\"Classe 1\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend(); plt.title(\"Dados 2D \u2014 Classes 0 e 1\")\nplt.show()\n</pre> # par\u00e2metros mean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]] mean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]  # gerar amostras n_samples = 1000 class0 = np.random.multivariate_normal(mean0, cov0, n_samples) class1 = np.random.multivariate_normal(mean1, cov1, n_samples)  # juntar X = np.vstack((class0, class1)) y = np.hstack((-1*np.ones(n_samples), +1*np.ones(n_samples)))  # r\u00f3tulos {-1, +1}  # plot plt.figure(figsize=(6,6)) plt.scatter(class0[:,0], class0[:,1], alpha=0.6, label=\"Classe 0\") plt.scatter(class1[:,0], class1[:,1], alpha=0.6, label=\"Classe 1\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend(); plt.title(\"Dados 2D \u2014 Classes 0 e 1\") plt.show() In\u00a0[67]: Copied! <pre>class Perceptron:\n    def __init__(self, lr=0.01, max_epochs=100, shuffle=True, seed=42):\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.shuffle = shuffle\n        self.seed = seed\n        self.w = None\n        self.b = None\n        self.history_ = []  # acur\u00e1cia por \u00e9poca\n\n    @staticmethod\n    def _step(z):\n        return np.where(z &gt;= 0.0, 1, -1)\n\n    def predict(self, X):\n        return self._step(X @ self.w + self.b)\n\n    def fit(self, X, y):\n        rng = np.random.default_rng(self.seed)\n        n, d = X.shape\n        self.w = np.zeros(d)\n        self.b = 0.0\n        self.history_.clear()\n\n        for epoch in range(1, self.max_epochs + 1):\n            idx = np.arange(n)\n            if self.shuffle:\n                rng.shuffle(idx)\n            updates = 0\n\n            for i in idx:\n                xi, yi = X[i], y[i]\n                y_hat = self._step(self.w @ xi + self.b)\n                if y_hat != yi:\n                    self.w += self.lr * yi * xi\n                    self.b += self.lr * yi\n                    updates += 1\n\n            # acur\u00e1cia ao fim da \u00e9poca\n            acc = (self.predict(X) == y).mean()\n            self.history_.append(acc)\n\n            if updates == 0:  # convergiu\n                break\n\n        return self\n</pre> class Perceptron:     def __init__(self, lr=0.01, max_epochs=100, shuffle=True, seed=42):         self.lr = lr         self.max_epochs = max_epochs         self.shuffle = shuffle         self.seed = seed         self.w = None         self.b = None         self.history_ = []  # acur\u00e1cia por \u00e9poca      @staticmethod     def _step(z):         return np.where(z &gt;= 0.0, 1, -1)      def predict(self, X):         return self._step(X @ self.w + self.b)      def fit(self, X, y):         rng = np.random.default_rng(self.seed)         n, d = X.shape         self.w = np.zeros(d)         self.b = 0.0         self.history_.clear()          for epoch in range(1, self.max_epochs + 1):             idx = np.arange(n)             if self.shuffle:                 rng.shuffle(idx)             updates = 0              for i in idx:                 xi, yi = X[i], y[i]                 y_hat = self._step(self.w @ xi + self.b)                 if y_hat != yi:                     self.w += self.lr * yi * xi                     self.b += self.lr * yi                     updates += 1              # acur\u00e1cia ao fim da \u00e9poca             acc = (self.predict(X) == y).mean()             self.history_.append(acc)              if updates == 0:  # convergiu                 break          return self  In\u00a0[68]: Copied! <pre>per = Perceptron(lr=0.01, max_epochs=100, shuffle=True, seed=42).fit(X, y)\n\ny_pred = per.predict(X)\nacc = (y_pred == y).mean()\n\nprint(f\"Pesos (w): {per.w}\")\nprint(f\"Vi\u00e9s (b): {per.b:.4f}\")\nprint(f\"Acur\u00e1cia final no conjunto completo: {acc:.4f}\")\nprint(f\"\u00c9pocas executadas: {len(per.history_)}\")\n</pre> per = Perceptron(lr=0.01, max_epochs=100, shuffle=True, seed=42).fit(X, y)  y_pred = per.predict(X) acc = (y_pred == y).mean()  print(f\"Pesos (w): {per.w}\") print(f\"Vi\u00e9s (b): {per.b:.4f}\") print(f\"Acur\u00e1cia final no conjunto completo: {acc:.4f}\") print(f\"\u00c9pocas executadas: {len(per.history_)}\")  <pre>Pesos (w): [0.02627968 0.02874301]\nVi\u00e9s (b): -0.1800\nAcur\u00e1cia final no conjunto completo: 1.0000\n\u00c9pocas executadas: 3\n</pre> In\u00a0[69]: Copied! <pre>def plot_decision_boundary_2d(X, y, w, b, title):\n    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n    xs = np.linspace(x_min, x_max, 400)\n\n    fig, ax = plt.subplots(figsize=(6,6))\n    ax.scatter(X[y==-1,0], X[y==-1,1], s=10, alpha=0.7, label=\"Classe -1\")\n    ax.scatter(X[y==+1,0], X[y==+1,1], s=10, alpha=0.7, label=\"Classe +1\")\n\n    if abs(w[1]) &gt; 1e-12:\n        ys = -(w[0]/w[1]) * xs - b / w[1]\n        ax.plot(xs, ys, \"k--\", lw=2, label=\"Fronteira do perceptron\")\n\n    # destacar erros\n    y_hat = np.where(X @ w + b &gt;= 0, 1, -1)\n    err = (y_hat != y)\n    if err.any():\n        ax.scatter(X[err,0], X[err,1], s=30, facecolors='none', edgecolors='r', label=\"Erros\")\n\n    ax.set_title(title)\n    ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\"); ax.legend()\n    plt.show()\n\nplot_decision_boundary_2d(X, y, per.w, per.b, \"Perceptron \u2014 fronteira e erros (Ex. 1)\")\n</pre> def plot_decision_boundary_2d(X, y, w, b, title):     x_min, x_max = X[:,0].min()-1, X[:,0].max()+1     xs = np.linspace(x_min, x_max, 400)      fig, ax = plt.subplots(figsize=(6,6))     ax.scatter(X[y==-1,0], X[y==-1,1], s=10, alpha=0.7, label=\"Classe -1\")     ax.scatter(X[y==+1,0], X[y==+1,1], s=10, alpha=0.7, label=\"Classe +1\")      if abs(w[1]) &gt; 1e-12:         ys = -(w[0]/w[1]) * xs - b / w[1]         ax.plot(xs, ys, \"k--\", lw=2, label=\"Fronteira do perceptron\")      # destacar erros     y_hat = np.where(X @ w + b &gt;= 0, 1, -1)     err = (y_hat != y)     if err.any():         ax.scatter(X[err,0], X[err,1], s=30, facecolors='none', edgecolors='r', label=\"Erros\")      ax.set_title(title)     ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\"); ax.legend()     plt.show()  plot_decision_boundary_2d(X, y, per.w, per.b, \"Perceptron \u2014 fronteira e erros (Ex. 1)\")  In\u00a0[70]: Copied! <pre>from matplotlib.ticker import FormatStrFormatter, MaxNLocator\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(per.history_)+1), per.history_, marker=\"o\")\nax.set_xlabel(\"\u00c9poca\")\nax.set_ylabel(\"Acur\u00e1cia (treino)\")\nax.set_title(\"Converg\u00eancia do Perceptron (Ex. 1)\")\n\n# mostrar valores absolutos, sem offset, com 3 casas\nax.ticklabel_format(axis=\"y\", useOffset=False)\nax.yaxis.set_major_formatter(FormatStrFormatter(\"%.3f\"))\nax.set_ylim(0.90, 1.001)          # zoom na faixa alta (ajuste se quiser)\nax.yaxis.set_major_locator(MaxNLocator(nbins=6))\nax.grid(True)\nplt.show()\n</pre> from matplotlib.ticker import FormatStrFormatter, MaxNLocator  fig, ax = plt.subplots() ax.plot(np.arange(1, len(per.history_)+1), per.history_, marker=\"o\") ax.set_xlabel(\"\u00c9poca\") ax.set_ylabel(\"Acur\u00e1cia (treino)\") ax.set_title(\"Converg\u00eancia do Perceptron (Ex. 1)\")  # mostrar valores absolutos, sem offset, com 3 casas ax.ticklabel_format(axis=\"y\", useOffset=False) ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.3f\")) ax.set_ylim(0.90, 1.001)          # zoom na faixa alta (ajuste se quiser) ax.yaxis.set_major_locator(MaxNLocator(nbins=6)) ax.grid(True) plt.show()  In\u00a0[71]: Copied! <pre>err_hist = 1.0 - np.array(per.history_)\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(err_hist)+1), err_hist, marker=\"o\")\nax.set_xlabel(\"\u00c9poca\")\nax.set_ylabel(\"Erro (1 - acur\u00e1cia)\")\nax.set_title(\"Erro por \u00e9poca (Ex. 1)\")\nax.set_ylim(-0.01, 0.1)           # ajuste conforme seu caso\nax.grid(True)\nplt.show()\n</pre> err_hist = 1.0 - np.array(per.history_)  fig, ax = plt.subplots() ax.plot(np.arange(1, len(err_hist)+1), err_hist, marker=\"o\") ax.set_xlabel(\"\u00c9poca\") ax.set_ylabel(\"Erro (1 - acur\u00e1cia)\") ax.set_title(\"Erro por \u00e9poca (Ex. 1)\") ax.set_ylim(-0.01, 0.1)           # ajuste conforme seu caso ax.grid(True) plt.show()  <p>No primeiro cen\u00e1rio, as m\u00e9dias distantes e a baixa vari\u00e2ncia tornam as classes praticamente linearmente separ\u00e1veis, e o perceptron converge em poucas \u00e9pocas, atingindo 100% de acur\u00e1cia. A fronteira linear separa perfeitamente as duas nuvens, sem erros residuais, em linha com o teorema do perceptron (converg\u00eancia em n\u00famero finito de atualiza\u00e7\u00f5es para dados separ\u00e1veis).</p> <p>A aparente \u201csubida imediata\u201d para 1.0 vem do fato de que a acur\u00e1cia \u00e9 medida ao fim da \u00e9poca. Durante a passagem pelos dados, o perceptron analisa um ponto por vez e, quando erra, empurra levemente a reta (ajusta $w$ e $b$) para o lado correto. A soma desses pequenos ajustes na primeira \u00e9poca j\u00e1 posiciona a reta quase no lugar ideal, fazendo a acur\u00e1cia ao final ficar perto de 0,9995 (que aparece como 1,000 ao arredondar). Na \u00e9poca seguinte, como a reta j\u00e1 est\u00e1 bem colocada, quase n\u00e3o h\u00e1 novos erros; ao completar uma \u00e9poca sem corre\u00e7\u00f5es (zero updates), considera-se converg\u00eancia e o gr\u00e1fico de erro cai a 0 \u2014 aprendizado est\u00e1vel e eficiente.</p> In\u00a0[72]: Copied! <pre># par\u00e2metros\nmean0, cov0 = [3, 3], [[1.5, 0], [0, 1.5]]\nmean1, cov1 = [4, 4], [[1.5, 0], [0, 1.5]]\n\n# gerar amostras\nn_samples = 1000\nclass0 = np.random.multivariate_normal(mean0, cov0, n_samples)\nclass1 = np.random.multivariate_normal(mean1, cov1, n_samples)\n\n# juntar\nX = np.vstack((class0, class1))\ny = np.hstack((-1*np.ones(n_samples), +1*np.ones(n_samples)))  # r\u00f3tulos {-1, +1}\n\n# plot\nplt.figure(figsize=(6,6))\nplt.scatter(class0[:,0], class0[:,1], alpha=0.6, label=\"Classe 0\")\nplt.scatter(class1[:,0], class1[:,1], alpha=0.6, label=\"Classe 1\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend(); plt.title(\"Dados 2D \u2014 Classes 0 e 1\")\nplt.show()\n</pre> # par\u00e2metros mean0, cov0 = [3, 3], [[1.5, 0], [0, 1.5]] mean1, cov1 = [4, 4], [[1.5, 0], [0, 1.5]]  # gerar amostras n_samples = 1000 class0 = np.random.multivariate_normal(mean0, cov0, n_samples) class1 = np.random.multivariate_normal(mean1, cov1, n_samples)  # juntar X = np.vstack((class0, class1)) y = np.hstack((-1*np.ones(n_samples), +1*np.ones(n_samples)))  # r\u00f3tulos {-1, +1}  # plot plt.figure(figsize=(6,6)) plt.scatter(class0[:,0], class0[:,1], alpha=0.6, label=\"Classe 0\") plt.scatter(class1[:,0], class1[:,1], alpha=0.6, label=\"Classe 1\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend(); plt.title(\"Dados 2D \u2014 Classes 0 e 1\") plt.show() In\u00a0[73]: Copied! <pre>per = Perceptron(lr=0.01, max_epochs=100, shuffle=True, seed=42).fit(X, y)\n\ny_pred = per.predict(X)\nacc = (y_pred == y).mean()\n\nprint(f\"Pesos (w): {per.w}\")\nprint(f\"Vi\u00e9s (b): {per.b:.4f}\")\nprint(f\"Acur\u00e1cia final no conjunto completo: {acc:.4f}\")\nprint(f\"\u00c9pocas executadas: {len(per.history_)}\")\n</pre> per = Perceptron(lr=0.01, max_epochs=100, shuffle=True, seed=42).fit(X, y)  y_pred = per.predict(X) acc = (y_pred == y).mean()  print(f\"Pesos (w): {per.w}\") print(f\"Vi\u00e9s (b): {per.b:.4f}\") print(f\"Acur\u00e1cia final no conjunto completo: {acc:.4f}\") print(f\"\u00c9pocas executadas: {len(per.history_)}\") <pre>Pesos (w): [0.06092489 0.01368968]\nVi\u00e9s (b): -0.4600\nAcur\u00e1cia final no conjunto completo: 0.5120\n\u00c9pocas executadas: 100\n</pre> In\u00a0[74]: Copied! <pre>plot_decision_boundary_2d(X, y, per.w, per.b, \"Perceptron \u2014 fronteira e erros (Ex. 2)\")\n</pre> plot_decision_boundary_2d(X, y, per.w, per.b, \"Perceptron \u2014 fronteira e erros (Ex. 2)\") In\u00a0[78]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(np.arange(1, len(per.history_)+1), per.history_, marker=\"o\")\nax.set_xlabel(\"\u00c9poca\")\nax.set_ylabel(\"Acur\u00e1cia (treino)\")\nax.set_title(\"Converg\u00eancia do Perceptron (Ex. 2)\")\n\n# mostrar valores absolutos, sem offset, com 3 casas\nax.ticklabel_format(axis=\"y\", useOffset=False)\nax.yaxis.set_major_formatter(FormatStrFormatter(\"%.3f\"))\nax.yaxis.set_major_locator(MaxNLocator(nbins=6))\nax.grid(True)\nplt.show()\n</pre> fig, ax = plt.subplots() ax.plot(np.arange(1, len(per.history_)+1), per.history_, marker=\"o\") ax.set_xlabel(\"\u00c9poca\") ax.set_ylabel(\"Acur\u00e1cia (treino)\") ax.set_title(\"Converg\u00eancia do Perceptron (Ex. 2)\")  # mostrar valores absolutos, sem offset, com 3 casas ax.ticklabel_format(axis=\"y\", useOffset=False) ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.3f\")) ax.yaxis.set_major_locator(MaxNLocator(nbins=6)) ax.grid(True) plt.show() In\u00a0[79]: Copied! <pre>err_hist = 1.0 - np.array(per.history_)\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(err_hist)+1), err_hist, marker=\"o\")\nax.set_xlabel(\"\u00c9poca\")\nax.set_ylabel(\"Erro (1 - acur\u00e1cia)\")\nax.set_title(\"Erro por \u00e9poca (Ex. 2)\")\nax.grid(True)\nplt.show()\n</pre> err_hist = 1.0 - np.array(per.history_)  fig, ax = plt.subplots() ax.plot(np.arange(1, len(err_hist)+1), err_hist, marker=\"o\") ax.set_xlabel(\"\u00c9poca\") ax.set_ylabel(\"Erro (1 - acur\u00e1cia)\") ax.set_title(\"Erro por \u00e9poca (Ex. 2)\") ax.grid(True) plt.show()  <p>No segundo cen\u00e1rio, as m\u00e9dias mais pr\u00f3ximas e a vari\u00e2ncia maior geram forte sobreposi\u00e7\u00e3o entre as classes. Nesse regime, n\u00e3o existe uma reta que separe perfeitamente todos os pontos; logo, o perceptron cl\u00e1ssico (que s\u00f3 para quando n\u00e3o h\u00e1 mais erros em uma \u00e9poca) n\u00e3o converge. No nosso experimento, ele chegou ao limite de 100 \u00e9pocas com acur\u00e1cia em torno de 0,51, o que reflete a dificuldade intr\u00ednseca do problema.</p> <p>Os gr\u00e1ficos deixam isso claro: a fronteira linear cruza a regi\u00e3o onde as nuvens se misturam e concentra os erros ao seu redor; a acur\u00e1cia por \u00e9poca oscila sem tend\u00eancia a 1,0; e o erro n\u00e3o se aproxima de zero. Isso \u00e9 esperado quando h\u00e1 n\u00e3o separabilidade linear: as atualiza\u00e7\u00f5es do perceptron continuam corrigindo pontos de um lado e criando novos erros do outro, levando a flutua\u00e7\u00f5es em vez de estabiliza\u00e7\u00e3o.</p> <p>Comparado ao Exerc\u00edcio 1, este caso evidencia a limita\u00e7\u00e3o do perceptron: ele funciona muito bem quando os dados s\u00e3o separ\u00e1veis por uma reta, mas, com sobreposi\u00e7\u00e3o, tende a estagnar. Para melhorar, \u00e9 preciso mais expressividade (p. ex., MLP com n\u00e3o linearidades) ou engenharia de atributos/transforma\u00e7\u00f5es que tornem a separa\u00e7\u00e3o mais pr\u00f3xima de linear.</p>"},{"location":"exercises/02_perceptron/perceptron/#entendendo-perceptrons-e-suas-limitacoes","title":"Entendendo Perceptrons e Suas Limita\u00e7\u00f5es\u00b6","text":""},{"location":"exercises/02_perceptron/perceptron/#exercicio-1","title":"Exerc\u00edcio 1\u00b6","text":""},{"location":"exercises/02_perceptron/perceptron/#geracao-dos-dados","title":"Gera\u00e7\u00e3o dos dados\u00b6","text":"<ul> <li>Classe 0: m\u00e9dia = [1.5, 1.5], covari\u00e2ncia = [[0.5, 0], [0, 0.5]]</li> <li>Classe 1: m\u00e9dia = [5, 5], covari\u00e2ncia = [[0.5, 0], [0, 0.5]]</li> </ul> <p>Cada classe ter\u00e1 1000 amostras. Espera-se separabilidade quase linear, dado o distanciamento entre m\u00e9dias e a baixa vari\u00e2ncia.</p>"},{"location":"exercises/02_perceptron/perceptron/#exercicio-2","title":"Exerc\u00edcio 2\u00b6","text":""},{"location":"exercises/03_mlp/mlp/","title":"Entendendo Perceptrons Multicamadas (MLPs)","text":"In\u00a0[71]: Copied! <pre># Bibliotecas \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n</pre> # Bibliotecas  import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA <p>Gerar um dataset 2D com 1 cluster para a classe 0 e 2 clusters para a classe 1 (usando <code>make_classification</code> em subconjuntos), treinar um MLP do zero (NumPy apenas) com 1 camada oculta, loss bin\u00e1rio (BCE), e avaliar: perda de treino, acur\u00e1cia de teste e fronteira de decis\u00e3o.</p> In\u00a0[72]: Copied! <pre>np.random.seed(42)\n\nn0, n1 = 500, 500  # total=1000\n\n# Subconjunto s\u00f3 com classe 0 (1 cluster)\nX0, y0 = make_classification(\n    n_samples=n0, n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=1, n_classes=2, weights=[1.0, 0.0],\n    class_sep=1.5, flip_y=0.0, random_state=42\n)\n\n# Subconjunto s\u00f3 com classe 1 (2 clusters)\nX1, y1 = make_classification(\n    n_samples=n1, n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=2, n_classes=2, weights=[0.0, 1.0],\n    class_sep=1.5, flip_y=0.0, random_state=43\n)\n\n# Junta e embaralha\nX = np.vstack([X0, X1])\ny = np.hstack([y0, y1])  # r\u00f3tulos {0,1}\n\nperm = np.random.permutation(len(X))\nX, y = X[perm], y[perm]\n\n# (seguran\u00e7a) se por acaso seus r\u00f3tulos estiverem em {-1,+1}, converte para {0,1}\nif set(np.unique(y)) == {-1, 1}:\n    y = ((y + 1) // 2).astype(int)\n\nprint(X.shape, y.shape, np.bincount(y))\n</pre> np.random.seed(42)  n0, n1 = 500, 500  # total=1000  # Subconjunto s\u00f3 com classe 0 (1 cluster) X0, y0 = make_classification(     n_samples=n0, n_features=2, n_informative=2, n_redundant=0,     n_clusters_per_class=1, n_classes=2, weights=[1.0, 0.0],     class_sep=1.5, flip_y=0.0, random_state=42 )  # Subconjunto s\u00f3 com classe 1 (2 clusters) X1, y1 = make_classification(     n_samples=n1, n_features=2, n_informative=2, n_redundant=0,     n_clusters_per_class=2, n_classes=2, weights=[0.0, 1.0],     class_sep=1.5, flip_y=0.0, random_state=43 )  # Junta e embaralha X = np.vstack([X0, X1]) y = np.hstack([y0, y1])  # r\u00f3tulos {0,1}  perm = np.random.permutation(len(X)) X, y = X[perm], y[perm]  # (seguran\u00e7a) se por acaso seus r\u00f3tulos estiverem em {-1,+1}, converte para {0,1} if set(np.unique(y)) == {-1, 1}:     y = ((y + 1) // 2).astype(int)  print(X.shape, y.shape, np.bincount(y))  <pre>(1000, 2) (1000,) [500 500]\n</pre> In\u00a0[73]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Opcional (recomendado): padronizar usando estat\u00edsticas do treino\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-8\nX_train = (X_train - mu) / sigma\nX_test  = (X_test  - mu) / sigma\n\nprint(\"train:\", X_train.shape, \"test:\", X_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, stratify=y, random_state=42 )  # Opcional (recomendado): padronizar usando estat\u00edsticas do treino mu = X_train.mean(axis=0) sigma = X_train.std(axis=0) + 1e-8 X_train = (X_train - mu) / sigma X_test  = (X_test  - mu) / sigma  print(\"train:\", X_train.shape, \"test:\", X_test.shape)  <pre>train: (800, 2) test: (200, 2)\n</pre> In\u00a0[74]: Copied! <pre># --------- Ativa\u00e7\u00f5es ---------\ndef tanh(z):\n    return np.tanh(z)\n\ndef dtanh(a):\n    # derivada em termos da ativa\u00e7\u00e3o a = tanh(z)\n    return 1.0 - a**2\n\ndef relu(z):\n    return np.maximum(0.0, z)\n\ndef drelu(z):\n    return (z &gt; 0.0).astype(z.dtype)\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef softmax(z):\n    # est\u00e1vel numericamente\n    z = z - z.max(axis=1, keepdims=True)\n    e = np.exp(z)\n    return e / e.sum(axis=1, keepdims=True)\n\n# --------- Perdas ---------\ndef bce_loss(y_true, y_prob, eps=1e-9):\n    # y_true shape: (N,1) com {0,1}; y_prob shape: (N,1)\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return -(y_true*np.log(y_prob) + (1-y_true)*np.log(1-y_prob)).mean()\n\ndef ce_loss(y_true_onehot, y_prob, eps=1e-9):\n    # y_true_onehot shape: (N,C); y_prob shape: (N,C)\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return -(y_true_onehot * np.log(y_prob)).sum(axis=1).mean()\n\n# --------- Utilit\u00e1rios ---------\ndef one_hot(y, num_classes=None):\n    y = y.astype(int).ravel()\n    if num_classes is None:\n        num_classes = int(y.max()) + 1\n    out = np.zeros((y.shape[0], num_classes), dtype=float)\n    out[np.arange(y.shape[0]), y] = 1.0\n    return out\n\ndef xavier_limit(fan_in, fan_out):\n    return np.sqrt(6.0 / (fan_in + fan_out))\n\ndef he_limit(fan_in):\n    # He uniform\n    return np.sqrt(6.0 / fan_in)\n</pre> # --------- Ativa\u00e7\u00f5es --------- def tanh(z):     return np.tanh(z)  def dtanh(a):     # derivada em termos da ativa\u00e7\u00e3o a = tanh(z)     return 1.0 - a**2  def relu(z):     return np.maximum(0.0, z)  def drelu(z):     return (z &gt; 0.0).astype(z.dtype)  def sigmoid(z):     return 1.0 / (1.0 + np.exp(-z))  def softmax(z):     # est\u00e1vel numericamente     z = z - z.max(axis=1, keepdims=True)     e = np.exp(z)     return e / e.sum(axis=1, keepdims=True)  # --------- Perdas --------- def bce_loss(y_true, y_prob, eps=1e-9):     # y_true shape: (N,1) com {0,1}; y_prob shape: (N,1)     y_prob = np.clip(y_prob, eps, 1.0 - eps)     return -(y_true*np.log(y_prob) + (1-y_true)*np.log(1-y_prob)).mean()  def ce_loss(y_true_onehot, y_prob, eps=1e-9):     # y_true_onehot shape: (N,C); y_prob shape: (N,C)     y_prob = np.clip(y_prob, eps, 1.0 - eps)     return -(y_true_onehot * np.log(y_prob)).sum(axis=1).mean()  # --------- Utilit\u00e1rios --------- def one_hot(y, num_classes=None):     y = y.astype(int).ravel()     if num_classes is None:         num_classes = int(y.max()) + 1     out = np.zeros((y.shape[0], num_classes), dtype=float)     out[np.arange(y.shape[0]), y] = 1.0     return out  def xavier_limit(fan_in, fan_out):     return np.sqrt(6.0 / (fan_in + fan_out))  def he_limit(fan_in):     # He uniform     return np.sqrt(6.0 / fan_in)  In\u00a0[75]: Copied! <pre>class MLP:\n    \"\"\"\n    MLP gen\u00e9rico (NumPy puro).\n    - layer_sizes: lista com dimens\u00f5es [in, h1, ..., hK, out]\n    - activations: lista de strings para cada camada oculta + sa\u00edda (len = len(layer_sizes)-1)\n        op\u00e7\u00f5es: 'tanh', 'relu', 'sigmoid', 'softmax' (use 'sigmoid' p/ bin\u00e1rio e 'softmax' p/ multiclasse)\n    - loss: 'bce' (bin\u00e1rio; requer sa\u00edda 'sigmoid') ou 'ce' (multiclasse; requer sa\u00edda 'softmax')\n    - l2: regulariza\u00e7\u00e3o L2 (lambda), default 0.0\n    - lr: taxa de aprendizado\n    - seed: reprodutibilidade\n    \"\"\"\n    def __init__(self, layer_sizes, activations, loss, lr=0.05, l2=0.0, seed=42):\n        assert len(layer_sizes) &gt;= 2, \"Precisa de pelo menos entrada e sa\u00edda\"\n        assert len(activations) == len(layer_sizes) - 1, \"Uma ativa\u00e7\u00e3o por camada\"\n        self.sizes = list(layer_sizes)\n        self.acts = list(activations)\n        self.loss_name = loss\n        self.lr = lr\n        self.l2 = float(l2)\n\n        self.rng = np.random.default_rng(seed)\n        self.params = self._init_params()\n        self.loss_hist = []\n\n        # mapear nomes para fun\u00e7\u00f5es\n        self._act = {\n            'tanh': (tanh, dtanh),\n            'relu': (relu, None),     # derivative usa Z\n            'sigmoid': (sigmoid, None), # derivative calculada via BCE no topo; n\u00e3o usada nas ocultas\n            'softmax': (softmax, None)\n        }\n\n        if loss == 'bce':\n            assert self.acts[-1] == 'sigmoid', \"BCE requer sa\u00edda 'sigmoid'\"\n        elif loss == 'ce':\n            assert self.acts[-1] == 'softmax', \"CE requer sa\u00edda 'softmax'\"\n        else:\n            raise ValueError(\"loss deve ser 'bce' ou 'ce'\")\n\n    def _init_params(self):\n        params = []\n        for l in range(len(self.sizes) - 1):\n            fan_in, fan_out = self.sizes[l], self.sizes[l+1]\n            act = self.acts[l]\n            if act in ('tanh', 'sigmoid', 'softmax'):\n                lim = xavier_limit(fan_in, fan_out)\n            elif act == 'relu':\n                lim = he_limit(fan_in)\n            else:\n                lim = xavier_limit(fan_in, fan_out)\n\n            W = self.rng.uniform(-lim, lim, size=(fan_in, fan_out))\n            b = np.zeros((1, fan_out))\n            params.append({'W': W, 'b': b})\n        return params\n\n    def _forward(self, X):\n        \"\"\"\n        Retorna A_list, Z_list:\n        A_list[0] = X\n        para l&gt;=1: Z_list[l] = A_list[l-1] @ W_l + b_l; A_list[l] = act(Z_list[l])\n        \"\"\"\n        A_list = [X]\n        Z_list = [None]\n        for l, layer in enumerate(self.params, start=1):\n            W, b = layer['W'], layer['b']\n            Z = A_list[-1] @ W + b\n            act_name = self.acts[l-1]\n            if act_name == 'tanh':\n                A = tanh(Z)\n            elif act_name == 'relu':\n                A = relu(Z)\n            elif act_name == 'sigmoid':\n                A = sigmoid(Z)\n            elif act_name == 'softmax':\n                A = softmax(Z)\n            else:\n                raise ValueError(f\"Ativa\u00e7\u00e3o desconhecida: {act_name}\")\n            Z_list.append(Z); A_list.append(A)\n        return A_list, Z_list\n\n    def _compute_loss(self, y_true, Aout):\n        if self.loss_name == 'bce':\n            loss = bce_loss(y_true.reshape(-1,1), Aout)\n        else:  # 'ce'\n            if Aout.ndim == 1 or Aout.shape[1] == 1:\n                raise ValueError(\"CE requer probabilidades (N,C) e r\u00f3tulos one-hot (N,C)\")\n            loss = ce_loss(y_true, Aout)\n\n        # L2\n        if self.l2 &gt; 0.0:\n            l2_sum = sum((layer['W']**2).sum() for layer in self.params)\n            loss = loss + 0.5 * self.l2 * l2_sum / y_true.shape[0]\n        return float(loss)\n\n    def _backward(self, A_list, Z_list, y_true):\n        grads = [None] * len(self.params)\n        N = A_list[0].shape[0]\n\n        # dZ da camada de sa\u00edda\n        Aout = A_list[-1]\n        if self.loss_name == 'bce':\n            # BCE + sigmoid =&gt; dZ = (Aout - y)/N\n            y = y_true.reshape(-1,1)\n            dZ = (Aout - y) / N\n        else:  # 'ce' + softmax =&gt; dZ = (Aout - Y)/N\n            Y = y_true\n            dZ = (Aout - Y) / N\n\n        # camadas de tr\u00e1s para frente\n        for l in reversed(range(len(self.params))):\n            A_prev = A_list[l]\n            W = self.params[l]['W']\n\n            dW = A_prev.T @ dZ\n            db = dZ.sum(axis=0, keepdims=True)\n\n            # L2\n            if self.l2 &gt; 0.0:\n                dW = dW + self.l2 * W / N\n\n            grads[l] = {'dW': dW, 'db': db}\n\n            if l &gt; 0:  # propagar para tr\u00e1s se n\u00e3o for a primeira camada\n                dA_prev = dZ @ W.T\n                act_name = self.acts[l-1]  # ativa\u00e7\u00e3o da camada l\n                if act_name == 'tanh':\n                    dZ = dA_prev * dtanh(A_list[l])\n                elif act_name == 'relu':\n                    dZ = dA_prev * drelu(Z_list[l])\n                elif act_name in ('sigmoid', 'softmax'):\n                    # n\u00e3o usamos sigmoid/softmax em ocultas neste design; se usar, trate aqui:\n                    a = A_list[l]\n                    if act_name == 'sigmoid':\n                        dZ = dA_prev * a * (1 - a)\n                    else:\n                        raise ValueError(\"Softmax em camada oculta n\u00e3o suportado\")\n                else:\n                    raise ValueError(f\"Ativa\u00e7\u00e3o desconhecida: {act_name}\")\n        return grads\n\n    def _step(self, grads):\n        for layer, g in zip(self.params, grads):\n            layer['W'] -= self.lr * g['dW']\n            layer['b'] -= self.lr * g['db']\n\n    def fit(self, X, y, epochs=300, verbose=False):\n        \"\"\"\n        y:\n          - bin\u00e1rio/BCE: array (N,) ou (N,1) com {0,1}\n          - CE: one-hot (N,C)\n        \"\"\"\n        self.loss_hist.clear()\n        for ep in range(1, epochs+1):\n            A_list, Z_list = self._forward(X)\n            loss = self._compute_loss(y, A_list[-1])\n            self.loss_hist.append(loss)\n            grads = self._backward(A_list, Z_list, y)\n            self._step(grads)\n            if verbose and (ep % 50 == 0 or ep == 1):\n                print(f\"\u00e9poca {ep:03d} | loss={loss:.4f}\")\n        return self\n\n    def predict_proba(self, X):\n        A_list, _ = self._forward(X)\n        return A_list[-1]\n\n    def predict(self, X, thr=0.5):\n        proba = self.predict_proba(X)\n        # BCE bin\u00e1rio\n        if self.loss_name == 'bce':\n            return (proba &gt;= thr).astype(int).ravel()\n        # CE multiclasse\n        return np.argmax(proba, axis=1)\n</pre> class MLP:     \"\"\"     MLP gen\u00e9rico (NumPy puro).     - layer_sizes: lista com dimens\u00f5es [in, h1, ..., hK, out]     - activations: lista de strings para cada camada oculta + sa\u00edda (len = len(layer_sizes)-1)         op\u00e7\u00f5es: 'tanh', 'relu', 'sigmoid', 'softmax' (use 'sigmoid' p/ bin\u00e1rio e 'softmax' p/ multiclasse)     - loss: 'bce' (bin\u00e1rio; requer sa\u00edda 'sigmoid') ou 'ce' (multiclasse; requer sa\u00edda 'softmax')     - l2: regulariza\u00e7\u00e3o L2 (lambda), default 0.0     - lr: taxa de aprendizado     - seed: reprodutibilidade     \"\"\"     def __init__(self, layer_sizes, activations, loss, lr=0.05, l2=0.0, seed=42):         assert len(layer_sizes) &gt;= 2, \"Precisa de pelo menos entrada e sa\u00edda\"         assert len(activations) == len(layer_sizes) - 1, \"Uma ativa\u00e7\u00e3o por camada\"         self.sizes = list(layer_sizes)         self.acts = list(activations)         self.loss_name = loss         self.lr = lr         self.l2 = float(l2)          self.rng = np.random.default_rng(seed)         self.params = self._init_params()         self.loss_hist = []          # mapear nomes para fun\u00e7\u00f5es         self._act = {             'tanh': (tanh, dtanh),             'relu': (relu, None),     # derivative usa Z             'sigmoid': (sigmoid, None), # derivative calculada via BCE no topo; n\u00e3o usada nas ocultas             'softmax': (softmax, None)         }          if loss == 'bce':             assert self.acts[-1] == 'sigmoid', \"BCE requer sa\u00edda 'sigmoid'\"         elif loss == 'ce':             assert self.acts[-1] == 'softmax', \"CE requer sa\u00edda 'softmax'\"         else:             raise ValueError(\"loss deve ser 'bce' ou 'ce'\")      def _init_params(self):         params = []         for l in range(len(self.sizes) - 1):             fan_in, fan_out = self.sizes[l], self.sizes[l+1]             act = self.acts[l]             if act in ('tanh', 'sigmoid', 'softmax'):                 lim = xavier_limit(fan_in, fan_out)             elif act == 'relu':                 lim = he_limit(fan_in)             else:                 lim = xavier_limit(fan_in, fan_out)              W = self.rng.uniform(-lim, lim, size=(fan_in, fan_out))             b = np.zeros((1, fan_out))             params.append({'W': W, 'b': b})         return params      def _forward(self, X):         \"\"\"         Retorna A_list, Z_list:         A_list[0] = X         para l&gt;=1: Z_list[l] = A_list[l-1] @ W_l + b_l; A_list[l] = act(Z_list[l])         \"\"\"         A_list = [X]         Z_list = [None]         for l, layer in enumerate(self.params, start=1):             W, b = layer['W'], layer['b']             Z = A_list[-1] @ W + b             act_name = self.acts[l-1]             if act_name == 'tanh':                 A = tanh(Z)             elif act_name == 'relu':                 A = relu(Z)             elif act_name == 'sigmoid':                 A = sigmoid(Z)             elif act_name == 'softmax':                 A = softmax(Z)             else:                 raise ValueError(f\"Ativa\u00e7\u00e3o desconhecida: {act_name}\")             Z_list.append(Z); A_list.append(A)         return A_list, Z_list      def _compute_loss(self, y_true, Aout):         if self.loss_name == 'bce':             loss = bce_loss(y_true.reshape(-1,1), Aout)         else:  # 'ce'             if Aout.ndim == 1 or Aout.shape[1] == 1:                 raise ValueError(\"CE requer probabilidades (N,C) e r\u00f3tulos one-hot (N,C)\")             loss = ce_loss(y_true, Aout)          # L2         if self.l2 &gt; 0.0:             l2_sum = sum((layer['W']**2).sum() for layer in self.params)             loss = loss + 0.5 * self.l2 * l2_sum / y_true.shape[0]         return float(loss)      def _backward(self, A_list, Z_list, y_true):         grads = [None] * len(self.params)         N = A_list[0].shape[0]          # dZ da camada de sa\u00edda         Aout = A_list[-1]         if self.loss_name == 'bce':             # BCE + sigmoid =&gt; dZ = (Aout - y)/N             y = y_true.reshape(-1,1)             dZ = (Aout - y) / N         else:  # 'ce' + softmax =&gt; dZ = (Aout - Y)/N             Y = y_true             dZ = (Aout - Y) / N          # camadas de tr\u00e1s para frente         for l in reversed(range(len(self.params))):             A_prev = A_list[l]             W = self.params[l]['W']              dW = A_prev.T @ dZ             db = dZ.sum(axis=0, keepdims=True)              # L2             if self.l2 &gt; 0.0:                 dW = dW + self.l2 * W / N              grads[l] = {'dW': dW, 'db': db}              if l &gt; 0:  # propagar para tr\u00e1s se n\u00e3o for a primeira camada                 dA_prev = dZ @ W.T                 act_name = self.acts[l-1]  # ativa\u00e7\u00e3o da camada l                 if act_name == 'tanh':                     dZ = dA_prev * dtanh(A_list[l])                 elif act_name == 'relu':                     dZ = dA_prev * drelu(Z_list[l])                 elif act_name in ('sigmoid', 'softmax'):                     # n\u00e3o usamos sigmoid/softmax em ocultas neste design; se usar, trate aqui:                     a = A_list[l]                     if act_name == 'sigmoid':                         dZ = dA_prev * a * (1 - a)                     else:                         raise ValueError(\"Softmax em camada oculta n\u00e3o suportado\")                 else:                     raise ValueError(f\"Ativa\u00e7\u00e3o desconhecida: {act_name}\")         return grads      def _step(self, grads):         for layer, g in zip(self.params, grads):             layer['W'] -= self.lr * g['dW']             layer['b'] -= self.lr * g['db']      def fit(self, X, y, epochs=300, verbose=False):         \"\"\"         y:           - bin\u00e1rio/BCE: array (N,) ou (N,1) com {0,1}           - CE: one-hot (N,C)         \"\"\"         self.loss_hist.clear()         for ep in range(1, epochs+1):             A_list, Z_list = self._forward(X)             loss = self._compute_loss(y, A_list[-1])             self.loss_hist.append(loss)             grads = self._backward(A_list, Z_list, y)             self._step(grads)             if verbose and (ep % 50 == 0 or ep == 1):                 print(f\"\u00e9poca {ep:03d} | loss={loss:.4f}\")         return self      def predict_proba(self, X):         A_list, _ = self._forward(X)         return A_list[-1]      def predict(self, X, thr=0.5):         proba = self.predict_proba(X)         # BCE bin\u00e1rio         if self.loss_name == 'bce':             return (proba &gt;= thr).astype(int).ravel()         # CE multiclasse         return np.argmax(proba, axis=1)  In\u00a0[76]: Copied! <pre># Exemplo bin\u00e1rio: 2 -&gt; 8 -&gt; 1, tanh + sigmoid, BCE\nmlp_bin = MLP(layer_sizes=[2, 8, 1],\n              activations=['tanh', 'sigmoid'],\n              loss='bce',\n              lr=0.05, l2=0.0, seed=42)\n\nmlp_bin.fit(X_train, y_train, epochs=300, verbose=True)\ny_pred = mlp_bin.predict(X_test)\nacc = (y_pred == y_test).mean()\nprint(f\"Acur\u00e1cia (bin\u00e1rio): {acc:.3f}\")\n</pre> # Exemplo bin\u00e1rio: 2 -&gt; 8 -&gt; 1, tanh + sigmoid, BCE mlp_bin = MLP(layer_sizes=[2, 8, 1],               activations=['tanh', 'sigmoid'],               loss='bce',               lr=0.05, l2=0.0, seed=42)  mlp_bin.fit(X_train, y_train, epochs=300, verbose=True) y_pred = mlp_bin.predict(X_test) acc = (y_pred == y_test).mean() print(f\"Acur\u00e1cia (bin\u00e1rio): {acc:.3f}\") <pre>\u00e9poca 001 | loss=0.6169\n\u00e9poca 050 | loss=0.5573\n\u00e9poca 100 | loss=0.5391\n\u00e9poca 150 | loss=0.5321\n\u00e9poca 200 | loss=0.5283\n\u00e9poca 250 | loss=0.5257\n\u00e9poca 300 | loss=0.5235\nAcur\u00e1cia (bin\u00e1rio): 0.620\n</pre> In\u00a0[77]: Copied! <pre>def plot_decision_boundary_model(model, X, y, title=\"Fronteira de decis\u00e3o\"):\n    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                         np.linspace(y_min, y_max, 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    proba = model.predict_proba(grid)\n    if proba.ndim == 1 or proba.shape[1] == 1:\n        zz = proba.reshape(xx.shape)\n    else:\n        zz = proba.max(axis=1).reshape(xx.shape)\n\n    plt.figure(figsize=(6,5))\n    cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.4)\n    plt.colorbar(cs)\n    # pontos\n    if y.ndim == 2 and y.shape[1] &gt; 1:\n        y_plot = np.argmax(y, axis=1)\n    else:\n        y_plot = y.ravel()\n    for cls in np.unique(y_plot):\n        pts = X[y_plot==cls]\n        plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")\n    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n    plt.show()\n</pre> def plot_decision_boundary_model(model, X, y, title=\"Fronteira de decis\u00e3o\"):     x_min, x_max = X[:,0].min()-1, X[:,0].max()+1     y_min, y_max = X[:,1].min()-1, X[:,1].max()+1     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),                          np.linspace(y_min, y_max, 300))     grid = np.c_[xx.ravel(), yy.ravel()]     proba = model.predict_proba(grid)     if proba.ndim == 1 or proba.shape[1] == 1:         zz = proba.reshape(xx.shape)     else:         zz = proba.max(axis=1).reshape(xx.shape)      plt.figure(figsize=(6,5))     cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.4)     plt.colorbar(cs)     # pontos     if y.ndim == 2 and y.shape[1] &gt; 1:         y_plot = np.argmax(y, axis=1)     else:         y_plot = y.ravel()     for cls in np.unique(y_plot):         pts = X[y_plot==cls]         plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")     plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()     plt.show()  In\u00a0[78]: Copied! <pre>plot_decision_boundary_model(mlp_bin, X_train, y_train, title=\"Fronteira de decis\u00e3o (treino)\")\n</pre> plot_decision_boundary_model(mlp_bin, X_train, y_train, title=\"Fronteira de decis\u00e3o (treino)\") In\u00a0[79]: Copied! <pre>plt.figure(figsize=(6,4))\nplt.plot(range(1, len(mlp_bin.loss_hist)+1), mlp_bin.loss_hist, marker=\"o\", ms=3)\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss (treino)\")\nplt.title(\"MLP \u2014 curva de perda (BCE)\")\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(6,4)) plt.plot(range(1, len(mlp_bin.loss_hist)+1), mlp_bin.loss_hist, marker=\"o\", ms=3) plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss (treino)\") plt.title(\"MLP \u2014 curva de perda (BCE)\") plt.grid(True) plt.show()  In\u00a0[80]: Copied! <pre>def confusion_matrix_bin(y_true, y_pred):\n    tn = np.sum((y_true==0) &amp; (y_pred==0))\n    fp = np.sum((y_true==0) &amp; (y_pred==1))\n    fn = np.sum((y_true==1) &amp; (y_pred==0))\n    tp = np.sum((y_true==1) &amp; (y_pred==1))\n    return np.array([[tn, fp],\n                     [fn, tp]])\n\ny_pred_tr = mlp_bin.predict(X_train)\ny_pred_te = mlp_bin.predict(X_test)\n\nacc_tr = (y_pred_tr == y_train).mean()\nacc_te = (y_pred_te == y_test).mean()\ncm_te  = confusion_matrix_bin(y_test, y_pred_te)\n\nprint(f\"Acur\u00e1cia (treino): {acc_tr:.3f}\")\nprint(f\"Acur\u00e1cia (teste) : {acc_te:.3f}\")\nprint(\"Matriz de confus\u00e3o (teste) [[TN, FP],[FN, TP]]:\\n\", cm_te)\n</pre> def confusion_matrix_bin(y_true, y_pred):     tn = np.sum((y_true==0) &amp; (y_pred==0))     fp = np.sum((y_true==0) &amp; (y_pred==1))     fn = np.sum((y_true==1) &amp; (y_pred==0))     tp = np.sum((y_true==1) &amp; (y_pred==1))     return np.array([[tn, fp],                      [fn, tp]])  y_pred_tr = mlp_bin.predict(X_train) y_pred_te = mlp_bin.predict(X_test)  acc_tr = (y_pred_tr == y_train).mean() acc_te = (y_pred_te == y_test).mean() cm_te  = confusion_matrix_bin(y_test, y_pred_te)  print(f\"Acur\u00e1cia (treino): {acc_tr:.3f}\") print(f\"Acur\u00e1cia (teste) : {acc_te:.3f}\") print(\"Matriz de confus\u00e3o (teste) [[TN, FP],[FN, TP]]:\\n\", cm_te)  <pre>Acur\u00e1cia (treino): 0.676\nAcur\u00e1cia (teste) : 0.620\nMatriz de confus\u00e3o (teste) [[TN, FP],[FN, TP]]:\n [[74 26]\n [50 50]]\n</pre> In\u00a0[81]: Copied! <pre>def plot_boundary_with_errors(model, X, y, title=\"Fronteira (teste) + erros\"):\n    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                         np.linspace(y_min, y_max, 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    zz = model.predict_proba(grid).reshape(xx.shape)\n\n    plt.figure(figsize=(6,5))\n    cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.35)\n    plt.colorbar(cs)\n\n    y_hat = model.predict(X)\n    err = y_hat != y\n\n    for cls in np.unique(y):\n        pts = X[(y==cls) &amp; (~err)]\n        plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")\n\n    if err.any():\n        plt.scatter(X[err,0], X[err,1], s=40, facecolors='none', edgecolors='r', linewidths=1.2, label=\"erros\")\n\n    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n    plt.show()\n\nplot_boundary_with_errors(mlp_bin, X_test, y_test, title=\"Fronteira de decis\u00e3o (teste) + erros\")\n</pre> def plot_boundary_with_errors(model, X, y, title=\"Fronteira (teste) + erros\"):     x_min, x_max = X[:,0].min()-1, X[:,0].max()+1     y_min, y_max = X[:,1].min()-1, X[:,1].max()+1     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),                          np.linspace(y_min, y_max, 300))     grid = np.c_[xx.ravel(), yy.ravel()]     zz = model.predict_proba(grid).reshape(xx.shape)      plt.figure(figsize=(6,5))     cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.35)     plt.colorbar(cs)      y_hat = model.predict(X)     err = y_hat != y      for cls in np.unique(y):         pts = X[(y==cls) &amp; (~err)]         plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")      if err.any():         plt.scatter(X[err,0], X[err,1], s=40, facecolors='none', edgecolors='r', linewidths=1.2, label=\"erros\")      plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()     plt.show()  plot_boundary_with_errors(mlp_bin, X_test, y_test, title=\"Fronteira de decis\u00e3o (teste) + erros\")  <p>O MLP de 1 camada oculta aprendeu uma fronteira n\u00e3o linear que separa razoavelmente o \u00fanico cluster da classe 0 dos dois clusters da classe 1. A perda caiu de ~0.62 para ~0.52 e a acur\u00e1cia de teste ficou ~0.62, compat\u00edvel com a sobreposi\u00e7\u00e3o vis\u00edvel entre as classes: h\u00e1 regi\u00f5es onde ambos os r\u00f3tulos s\u00e3o plaus\u00edveis, e uma fronteira suave (tanh + sigmoid) n\u00e3o resolve todas. Ainda assim, o modelo captura a estrutura multimodal, melhor que um classificador linear. Ganhos adicionais viriam de mais capacidade (mais neur\u00f4nios/camadas), regulariza\u00e7\u00e3o e ajuste de taxa de aprendizado/\u00e9pocas.</p> In\u00a0[82]: Copied! <pre>rng = np.random.default_rng(42)\n\n# 1500 = 500 por classe\nn0 = n1 = n2 = 500\nF = 4\n\n# Classe 0: 2 clusters\nX0, y0 = make_classification(\n    n_samples=n0, n_features=F, n_informative=F, n_redundant=0,\n    n_classes=2, n_clusters_per_class=2, weights=[1.0, 0.0],\n    class_sep=1.6, flip_y=0.0, random_state=10\n)\ny0[:] = 0\n\n# Classe 1: 3 clusters\nX1, y1 = make_classification(\n    n_samples=n1, n_features=F, n_informative=F, n_redundant=0,\n    n_classes=2, n_clusters_per_class=3, weights=[0.0, 1.0],\n    class_sep=1.5, flip_y=0.0, random_state=11\n)\ny1[:] = 1\n\n# Classe 2: 4 clusters\nX2, y2 = make_classification(\n    n_samples=n2, n_features=F, n_informative=F, n_redundant=0,\n    n_classes=2, n_clusters_per_class=4, weights=[0.0, 1.0],\n    class_sep=1.4, flip_y=0.0, random_state=12\n)\ny2[:] = 2\n\nX = np.vstack([X0, X1, X2])\ny = np.concatenate([y0, y1, y2]).astype(int)\n\nperm = rng.permutation(len(X))\nX, y = X[perm], y[perm]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# padroniza\u00e7\u00e3o por estat\u00edstica do treino\nmu, sig = X_train.mean(axis=0), X_train.std(axis=0) + 1e-8\nX_train = (X_train - mu) / sig\nX_test  = (X_test  - mu) / sig\n</pre> rng = np.random.default_rng(42)  # 1500 = 500 por classe n0 = n1 = n2 = 500 F = 4  # Classe 0: 2 clusters X0, y0 = make_classification(     n_samples=n0, n_features=F, n_informative=F, n_redundant=0,     n_classes=2, n_clusters_per_class=2, weights=[1.0, 0.0],     class_sep=1.6, flip_y=0.0, random_state=10 ) y0[:] = 0  # Classe 1: 3 clusters X1, y1 = make_classification(     n_samples=n1, n_features=F, n_informative=F, n_redundant=0,     n_classes=2, n_clusters_per_class=3, weights=[0.0, 1.0],     class_sep=1.5, flip_y=0.0, random_state=11 ) y1[:] = 1  # Classe 2: 4 clusters X2, y2 = make_classification(     n_samples=n2, n_features=F, n_informative=F, n_redundant=0,     n_classes=2, n_clusters_per_class=4, weights=[0.0, 1.0],     class_sep=1.4, flip_y=0.0, random_state=12 ) y2[:] = 2  X = np.vstack([X0, X1, X2]) y = np.concatenate([y0, y1, y2]).astype(int)  perm = rng.permutation(len(X)) X, y = X[perm], y[perm]  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, stratify=y, random_state=42 )  # padroniza\u00e7\u00e3o por estat\u00edstica do treino mu, sig = X_train.mean(axis=0), X_train.std(axis=0) + 1e-8 X_train = (X_train - mu) / sig X_test  = (X_test  - mu) / sig  In\u00a0[83]: Copied! <pre># Sa\u00edda com 3 neur\u00f4nios, tanh nas ocultas, softmax na sa\u00edda, CE\nmlp_mc = MLP(layer_sizes=[X_train.shape[1], 16, 3],\n             activations=['tanh', 'softmax'],\n             loss='ce', lr=0.05, l2=0.0, seed=42)\n\n# r\u00f3tulos one-hot para CE\ndef one_hot(y, C=None):\n    C = int(y.max())+1 if C is None else C\n    Y = np.zeros((y.size, C)); Y[np.arange(y.size), y] = 1.0\n    return Y\n\nY_train = one_hot(y_train, 3)\nY_test  = one_hot(y_test, 3)\n\nmlp_mc.fit(X_train, Y_train, epochs=400, verbose=True)\n\ny_pred = mlp_mc.predict(X_test)          # argmax\nacc = (y_pred == y_test).mean()\nprint(f\"Acur\u00e1cia (teste, 3 classes): {acc:.3f}\")\n</pre> # Sa\u00edda com 3 neur\u00f4nios, tanh nas ocultas, softmax na sa\u00edda, CE mlp_mc = MLP(layer_sizes=[X_train.shape[1], 16, 3],              activations=['tanh', 'softmax'],              loss='ce', lr=0.05, l2=0.0, seed=42)  # r\u00f3tulos one-hot para CE def one_hot(y, C=None):     C = int(y.max())+1 if C is None else C     Y = np.zeros((y.size, C)); Y[np.arange(y.size), y] = 1.0     return Y  Y_train = one_hot(y_train, 3) Y_test  = one_hot(y_test, 3)  mlp_mc.fit(X_train, Y_train, epochs=400, verbose=True)  y_pred = mlp_mc.predict(X_test)          # argmax acc = (y_pred == y_test).mean() print(f\"Acur\u00e1cia (teste, 3 classes): {acc:.3f}\")  <pre>\u00e9poca 001 | loss=1.3406\n\u00e9poca 050 | loss=0.9813\n\u00e9poca 100 | loss=0.8792\n\u00e9poca 150 | loss=0.8220\n\u00e9poca 200 | loss=0.7852\n\u00e9poca 250 | loss=0.7567\n\u00e9poca 300 | loss=0.7311\n\u00e9poca 350 | loss=0.7065\n\u00e9poca 400 | loss=0.6825\nAcur\u00e1cia (teste, 3 classes): 0.723\n</pre> In\u00a0[84]: Copied! <pre>plt.plot(mlp_mc.loss_hist); plt.xlabel(\"\u00e9poca\"); plt.ylabel(\"loss (CE)\")\nplt.title(\"Curva de treino \u2014 CE\"); plt.show()\n\nZ = PCA(n_components=2, random_state=0).fit_transform(X_test)\ny_hat = mlp_mc.predict(X_test)\nfor c in np.unique(y_test):\n    pts = Z[y_test==c]\n    plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\")\nplt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\")\nplt.legend(); plt.title(\"Teste (PCA 2D): r\u00f3tulo real vs. predito\"); plt.show()\n</pre> plt.plot(mlp_mc.loss_hist); plt.xlabel(\"\u00e9poca\"); plt.ylabel(\"loss (CE)\") plt.title(\"Curva de treino \u2014 CE\"); plt.show()  Z = PCA(n_components=2, random_state=0).fit_transform(X_test) y_hat = mlp_mc.predict(X_test) for c in np.unique(y_test):     pts = Z[y_test==c]     plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\") plt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\") plt.legend(); plt.title(\"Teste (PCA 2D): r\u00f3tulo real vs. predito\"); plt.show()  <p>O modelo com uma \u00fanica camada oculta conseguiu reduzir a perda de 1.34 \u2192 0.68 ao longo de 400 \u00e9pocas, mostrando aprendizado consistente. A curva de treino apresenta decaimento suave, mas ainda relativamente lento, indicando que a capacidade de representa\u00e7\u00e3o \u00e9 limitada.</p> <p>No teste, a acur\u00e1cia atingiu 72,3% em um problema de 3 classes. O gr\u00e1fico de PCA revela sobreposi\u00e7\u00e3o consider\u00e1vel entre os r\u00f3tulos verdadeiros e preditos, evidenciando dificuldade em separar regi\u00f5es mais confusas do espa\u00e7o. Esse resultado reflete a limita\u00e7\u00e3o de um MLP raso: apesar de capturar padr\u00f5es n\u00e3o lineares, a fronteira de decis\u00e3o ainda n\u00e3o \u00e9 suficientemente expressiva para classes com forte sobreposi\u00e7\u00e3o.</p> In\u00a0[85]: Copied! <pre># 4 -&gt; 32 -&gt; 16 -&gt; 3 (tanh nas ocultas, softmax na sa\u00edda)\nmlp_deep = MLP(layer_sizes=[X_train.shape[1], 32, 16, 3],\n               activations=['tanh', 'tanh', 'softmax'],\n               loss='ce', lr=0.05, l2=1e-4, seed=42)\n\nY_train = one_hot(y_train, 3)\nY_test  = one_hot(y_test, 3)\n\nmlp_deep.fit(X_train, Y_train, epochs=500, verbose=True)\ny_pred = mlp_deep.predict(X_test)\nacc = (y_pred == y_test).mean()\nprint(f\"Acur\u00e1cia (teste, 2 ocultas): {acc:.3f}\")\n</pre> # 4 -&gt; 32 -&gt; 16 -&gt; 3 (tanh nas ocultas, softmax na sa\u00edda) mlp_deep = MLP(layer_sizes=[X_train.shape[1], 32, 16, 3],                activations=['tanh', 'tanh', 'softmax'],                loss='ce', lr=0.05, l2=1e-4, seed=42)  Y_train = one_hot(y_train, 3) Y_test  = one_hot(y_test, 3)  mlp_deep.fit(X_train, Y_train, epochs=500, verbose=True) y_pred = mlp_deep.predict(X_test) acc = (y_pred == y_test).mean() print(f\"Acur\u00e1cia (teste, 2 ocultas): {acc:.3f}\")  <pre>\u00e9poca 001 | loss=1.2982\n\u00e9poca 050 | loss=0.9095\n\u00e9poca 100 | loss=0.7993\n\u00e9poca 150 | loss=0.7358\n\u00e9poca 200 | loss=0.6819\n</pre> <pre>\u00e9poca 250 | loss=0.6312\n\u00e9poca 300 | loss=0.5859\n\u00e9poca 350 | loss=0.5481\n\u00e9poca 400 | loss=0.5174\n\u00e9poca 450 | loss=0.4928\n\u00e9poca 500 | loss=0.4728\nAcur\u00e1cia (teste, 2 ocultas): 0.807\n</pre> In\u00a0[86]: Copied! <pre>plt.plot(mlp_deep.loss_hist); plt.xlabel(\"\u00e9poca\"); plt.ylabel(\"loss (CE)\")\nplt.title(\"Curva de treino \u2014 MLP profundo\"); plt.show()\n\nZ = PCA(n_components=2, random_state=0).fit_transform(X_test)\ny_hat = mlp_mc.predict(X_test)\nfor c in np.unique(y_test):\n    pts = Z[y_test==c]\n    plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\")\nplt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\")\nplt.legend(); plt.title(\"Teste (PCA 2D): r\u00f3tulo real vs. predito\"); plt.show()\n</pre> plt.plot(mlp_deep.loss_hist); plt.xlabel(\"\u00e9poca\"); plt.ylabel(\"loss (CE)\") plt.title(\"Curva de treino \u2014 MLP profundo\"); plt.show()  Z = PCA(n_components=2, random_state=0).fit_transform(X_test) y_hat = mlp_mc.predict(X_test) for c in np.unique(y_test):     pts = Z[y_test==c]     plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\") plt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\") plt.legend(); plt.title(\"Teste (PCA 2D): r\u00f3tulo real vs. predito\"); plt.show()  <p>Com duas camadas ocultas, a perda caiu de 1.29 \u2192 0.47 em 500 \u00e9pocas, num ritmo mais acelerado e cont\u00ednuo que no caso anterior. A curva de treino mostra melhora clara na capacidade de ajuste.</p> <p>No teste, a acur\u00e1cia subiu para 80,7%, superando o modelo raso. A proje\u00e7\u00e3o em PCA tamb\u00e9m evidencia maior alinhamento entre r\u00f3tulos reais e predi\u00e7\u00f5es, embora ainda haja sobreposi\u00e7\u00e3o entre classes vizinhas. Esse resultado confirma o ganho de expressividade ao aumentar a profundidade: o MLP profundo consegue capturar fronteiras de decis\u00e3o mais complexas e se adapta melhor ao problema.</p>"},{"location":"exercises/03_mlp/mlp/#entendendo-perceptrons-multicamadas-mlps","title":"Entendendo Perceptrons Multicamadas (MLPs)\u00b6","text":""},{"location":"exercises/03_mlp/mlp/#exercicio-1","title":"Exerc\u00edcio 1\u00b6","text":"<p>Dados do problema.</p> <p>Entrada:</p> <p>$$x=[0.5,\\,-0.2]$$</p> <p>Alvo:</p> <p>$$y=1.0$$</p> <p>Camada oculta (2 neur\u00f4nios, tanh):</p> <p>$$ W^{(1)}=\\begin{bmatrix}0.3&amp;-0.1\\\\ 0.2&amp;0.4\\end{bmatrix},\\quad b^{(1)}=\\begin{bmatrix}0.1\\\\-0.2\\end{bmatrix} $$</p> <p>Sa\u00edda (1 neur\u00f4nio, tanh):</p> <p>$$ W^{(2)}=\\begin{bmatrix}0.5&amp;-0.3\\end{bmatrix},\\quad b^{(2)}=0.2 $$</p>"},{"location":"exercises/03_mlp/mlp/#1-forward-pass","title":"1) Forward pass\u00b6","text":"<p>Pr\u00e9-ativa\u00e7\u00f5es na oculta</p> <p>$$ z^{(1)}=W^{(1)}x+b^{(1)} $$</p> <p>$$ \\begin{aligned} z^{(1)}_1&amp;=0.3\\cdot0.5+(-0.1)\\cdot(-0.2)+0.1=0.27 \\\\ z^{(1)}_2&amp;=0.2\\cdot0.5+0.4\\cdot(-0.2)-0.2=-0.18 \\end{aligned} \\Longrightarrow\\quad z^{(1)}=\\begin{bmatrix}0.270000\\\\-0.180000\\end{bmatrix} $$</p> <p>Ativa\u00e7\u00f5es na oculta</p> <p>$$ a^{(1)}=\\tanh(z^{(1)}) $$</p> <p>$$ a^{(1)}=\\begin{bmatrix}\\tanh(0.27)\\\\ \\tanh(-0.18)\\end{bmatrix} =\\begin{bmatrix}0.263625\\\\-0.178081\\end{bmatrix} $$</p> <p>Pr\u00e9-ativa\u00e7\u00e3o na sa\u00edda</p> <p>$$ z^{(2)}=W^{(2)}a^{(1)}+b^{(2)} $$</p> <p>$$ z^{(2)}=0.5\\cdot0.263625+(-0.3)\\cdot(-0.178081)+0.2 =0.385237 $$</p> <p>Sa\u00edda</p> <p>$$ \\hat y=\\tanh(z^{(2)})=\\tanh(0.385237)=\\mathbf{0.367247} $$</p>"},{"location":"exercises/03_mlp/mlp/#2-loss","title":"2) Loss\u00b6","text":"<p>$$ L=(y-\\hat y)^2=(1-0.367247)^2=\\mathbf{0.400377} $$</p>"},{"location":"exercises/03_mlp/mlp/#3-backward-pass","title":"3) Backward pass\u00b6","text":"<p>Derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda:</p> <p>$$ \\frac{\\partial L}{\\partial \\hat y}=2(\\hat y-y)=2(0.367247-1)=-1.265507 $$</p> <p>Derivada da tanh:</p> <p>$$ \\frac{d}{dz}\\tanh(z)=1-\\tanh^2(z)  $$</p> <p>Logo:</p> <p>$$ \\frac{\\partial L}{\\partial z^{(2)}}=\\frac{\\partial L}{\\partial \\hat y}\\,(1-\\hat y^2) =-1.265507\\cdot(1-0.367247^2)=\\mathbf{-1.094828} $$</p> <p>Gradientes da camada de sa\u00edda</p> <p>$$ \\frac{\\partial L}{\\partial W^{(2)}}=\\frac{\\partial L}{\\partial z^{(2)}}\\,a^{(1)} =\\begin{bmatrix}-0.288624 &amp; 0.194968\\end{bmatrix},\\quad \\frac{\\partial L}{\\partial b^{(2)}}=\\mathbf{-1.094828} $$</p> <p>Propaga\u00e7\u00e3o para a oculta</p> <p>$$ \\frac{\\partial L}{\\partial a^{(1)}}=\\frac{\\partial L}{\\partial z^{(2)}}\\,W^{(2)} =\\begin{bmatrix}-0.547414\\\\ 0.328448\\end{bmatrix},\\qquad 1-(a^{(1)})^2=\\begin{bmatrix}0.930502\\\\ 0.968287\\end{bmatrix} $$</p> <p>$$ \\frac{\\partial L}{\\partial z^{(1)}}=\\frac{\\partial L}{\\partial a^{(1)}}\\odot\\bigl(1-(a^{(1)})^2\\bigr) =\\begin{bmatrix}-0.509370\\\\ 0.318032\\end{bmatrix} $$</p> <p>Gradientes da camada oculta</p> <p>$$ \\frac{\\partial L}{\\partial W^{(1)}} =\\begin{bmatrix} -0.254685 &amp; 0.101874\\\\ \\phantom{-}0.159016 &amp; -0.063606 \\end{bmatrix},\\quad \\frac{\\partial L}{\\partial b^{(1)}}=\\begin{bmatrix}-0.509370\\\\ 0.318032\\end{bmatrix} $$</p>"},{"location":"exercises/03_mlp/mlp/#4-atualizacao-de-parametros","title":"4) Atualiza\u00e7\u00e3o de par\u00e2metros\u00b6","text":"<p>$$\\eta=\\mathbf{0.1}$$ $$ \\begin{aligned} \\\\ W^{(2)}_{\\text{novo}}&amp;=W^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial W^{(2)}}  =\\begin{bmatrix}0.528862 &amp; -0.319497\\end{bmatrix} \\\\ b^{(2)}_{\\text{novo}}&amp;=b^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(2)}} =\\mathbf{0.309483} \\\\ W^{(1)}_{\\text{novo}}&amp;=W^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial W^{(1)}} \\\\ &amp;=\\begin{bmatrix} 0.325468 &amp; -0.110187\\\\ 0.184098 &amp; \\phantom{-}0.406361 \\end{bmatrix}\\\\[2pt] b^{(1)}_{\\text{novo}}&amp;=b^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(1)}} =\\begin{bmatrix}\\phantom{-}0.150937\\\\ -0.231803\\end{bmatrix} \\end{aligned} $$</p>"},{"location":"exercises/03_mlp/mlp/#exercicio-2-mlp-from-scratch-classificacao-binaria-em-2d","title":"Exerc\u00edcio 2 \u2014 MLP from scratch (classifica\u00e7\u00e3o bin\u00e1ria em 2D)\u00b6","text":""},{"location":"exercises/03_mlp/mlp/#exercicio-3-mlp-multiclasse-3-classes-4-features","title":"Exerc\u00edcio 3 \u2014 MLP multiclasse (3 classes, 4 features)\u00b6","text":"<p>Gerar 1500 amostras, 3 classes, 4 features; 2 clusters para a classe 0, 3 para a classe 1 e 4 para a classe 2 (combinando subconjuntos). Treinar a mesma MLP do Ex. 2 (c\u00f3digo id\u00eantico), trocando apenas sa\u00edda e fun\u00e7\u00e3o de perda.</p>"},{"location":"exercises/03_mlp/mlp/#exercicio-4-mlp-mais-profundo-2-camadas-ocultas","title":"Exerc\u00edcio 4 \u2014 MLP mais profundo (\u22652 camadas ocultas)\u00b6","text":""},{"location":"template/projeto/main/","title":"Projeto","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"template/roteiro1/main/","title":"Roteiro 1","text":""},{"location":"template/roteiro1/main/#objetivo","title":"Objetivo","text":"<p>Aqui vai o objetivo macro do roteiro. Por que estamos fazendo o que estamos fazendo?</p>"},{"location":"template/roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"template/roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Instalando o MAAS:</p> sudo snap install maas --channel=3.5/Stable <p></p> <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"template/roteiro1/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"template/roteiro1/main/#app","title":"App","text":""},{"location":"template/roteiro1/main/#tarefa-1_1","title":"Tarefa 1","text":""},{"location":"template/roteiro1/main/#tarefa-2_1","title":"Tarefa 2","text":"<p>Exemplo de diagrama</p> <pre><code>architecture-beta\n    group api(cloud)[API]\n\n    service db(database)[Database] in api\n    service disk1(disk)[Storage] in api\n    service disk2(disk)[Storage] in api\n    service server(server)[Server] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n    disk2:T -- B:db</code></pre> <p>Mermaid</p>"},{"location":"template/roteiro1/main/#questionario-projeto-ou-plano","title":"Question\u00e1rio, Projeto ou Plano","text":"<p>Esse se\u00e7\u00e3o deve ser preenchida apenas se houver demanda do roteiro.</p>"},{"location":"template/roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Quais as dificuldades encontradas? O que foi mais f\u00e1cil? O que foi mais dif\u00edcil?</p>"},{"location":"template/roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O que foi poss\u00edvel concluir com a realiza\u00e7\u00e3o do roteiro?</p>"},{"location":"template/roteiro2/main/","title":"Roteiro 2","text":""},{"location":"template/roteiro2/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"template/roteiro2/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"template/roteiro3/main/","title":"Roteiro 3","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> </p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"template/roteiro4/limit.def/","title":"Limit.def","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom io import StringIO\n</pre> import matplotlib.pyplot as plt import numpy as np from io import StringIO In\u00a0[\u00a0]: Copied! <pre>eq = lambda x: np.exp(x)\n</pre> eq = lambda x: np.exp(x) In\u00a0[\u00a0]: Copied! <pre>x = np.linspace(-.2, 2.1)\n</pre> x = np.linspace(-.2, 2.1) In\u00a0[\u00a0]: Copied! <pre>plt.rcParams[\"figure.figsize\"] = (15, 5)\n</pre> plt.rcParams[\"figure.figsize\"] = (15, 5) In\u00a0[\u00a0]: Copied! <pre>xa = 1.5\nya = 7\nk = 0.3\nka = xa - k\nak = xa + k\n</pre> xa = 1.5 ya = 7 k = 0.3 ka = xa - k ak = xa + k In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(1, 3)\nfor i in range(3):\n  ax[i].axhline(0, color='gray') # x = 0\n  ax[i].axvline(0, color='gray') # y = 0\n  ax[i].spines['top'].set_visible(False)\n  ax[i].spines['right'].set_visible(False)\n  ax[i].spines['bottom'].set_visible(False)\n  ax[i].spines['left'].set_visible(False)\n  ax[i].plot(x, eq(x), '-r', lw=4)\n  ax[i].set_xlim(min(x), max(x))\n  ax[i].set_xticks([])\n  ax[i].set_yticks([])\n  ax[i].plot([ka, ka], [0, eq(ka)], 'g:')\n  ax[i].plot([0, ka], [eq(ka), eq(ka)], 'g:')\n  ax[i].plot([ak, ak], [0, eq(ak)], 'g:')\n  ax[i].plot([0, ak], [eq(ak), eq(ak)], 'g:')\n  ax[i].text(xa, -0.5, 'a', horizontalalignment='center', fontsize=15)\n  ax[i].text(ka, -0.5, '$a-\\delta$', horizontalalignment='center', fontsize=15)\n  ax[i].text(ak, -0.5, '$a+\\delta$', horizontalalignment='center', fontsize=15)\n  ax[i].text(0, eq(ka), '$L-\\epsilon$', horizontalalignment='right', verticalalignment='center', fontsize=15)\n  ax[i].text(0, eq(ak), '$L+\\epsilon$', horizontalalignment='right', verticalalignment='center', fontsize=15)\n</pre> fig, ax = plt.subplots(1, 3) for i in range(3):   ax[i].axhline(0, color='gray') # x = 0   ax[i].axvline(0, color='gray') # y = 0   ax[i].spines['top'].set_visible(False)   ax[i].spines['right'].set_visible(False)   ax[i].spines['bottom'].set_visible(False)   ax[i].spines['left'].set_visible(False)   ax[i].plot(x, eq(x), '-r', lw=4)   ax[i].set_xlim(min(x), max(x))   ax[i].set_xticks([])   ax[i].set_yticks([])   ax[i].plot([ka, ka], [0, eq(ka)], 'g:')   ax[i].plot([0, ka], [eq(ka), eq(ka)], 'g:')   ax[i].plot([ak, ak], [0, eq(ak)], 'g:')   ax[i].plot([0, ak], [eq(ak), eq(ak)], 'g:')   ax[i].text(xa, -0.5, 'a', horizontalalignment='center', fontsize=15)   ax[i].text(ka, -0.5, '$a-\\delta$', horizontalalignment='center', fontsize=15)   ax[i].text(ak, -0.5, '$a+\\delta$', horizontalalignment='center', fontsize=15)   ax[i].text(0, eq(ka), '$L-\\epsilon$', horizontalalignment='right', verticalalignment='center', fontsize=15)   ax[i].text(0, eq(ak), '$L+\\epsilon$', horizontalalignment='right', verticalalignment='center', fontsize=15) In\u00a0[\u00a0]: Copied! <pre>ax[0].plot([xa, xa], [0, eq(xa)], 'b:')\nax[0].plot([0, xa], [eq(xa), eq(xa)], 'b:')\nax[0].plot(xa, eq(xa), 'ro', ms=15)\nax[0].text(0, eq(xa), 'L=f(a)', horizontalalignment='right', verticalalignment='center', fontsize=15)\nax[1].plot([xa, xa], [0, eq(xa)], 'b:')\nax[1].plot([0, xa], [eq(xa), eq(xa)], 'm:')\nax[1].plot(xa, eq(xa), marker='o', ms=15, mec='red', color='white')\nax[1].text(0, eq(xa), 'L', horizontalalignment='right', verticalalignment='center', fontsize=15)\nax[2].plot(xa, eq(xa), marker='o', ms=15, mec='white', color='white')\nax[2].plot([xa, xa], [0, ya], 'b:')\nax[2].plot([0, xa], [ya, ya], 'b:')\nax[2].plot([0, xa], [eq(xa), eq(xa)], 'm:')\nax[2].plot(xa, eq(xa), marker='o', ms=15, mec='red', color='white')\nax[2].plot(xa, ya, 'ro', ms=15)\nax[2].text(0, ya, 'f(a)', horizontalalignment='right', verticalalignment='center', fontsize=15)\nax[2].text(0, eq(xa), 'L', horizontalalignment='right', verticalalignment='center', fontsize=15)\n</pre> ax[0].plot([xa, xa], [0, eq(xa)], 'b:') ax[0].plot([0, xa], [eq(xa), eq(xa)], 'b:') ax[0].plot(xa, eq(xa), 'ro', ms=15) ax[0].text(0, eq(xa), 'L=f(a)', horizontalalignment='right', verticalalignment='center', fontsize=15) ax[1].plot([xa, xa], [0, eq(xa)], 'b:') ax[1].plot([0, xa], [eq(xa), eq(xa)], 'm:') ax[1].plot(xa, eq(xa), marker='o', ms=15, mec='red', color='white') ax[1].text(0, eq(xa), 'L', horizontalalignment='right', verticalalignment='center', fontsize=15) ax[2].plot(xa, eq(xa), marker='o', ms=15, mec='white', color='white') ax[2].plot([xa, xa], [0, ya], 'b:') ax[2].plot([0, xa], [ya, ya], 'b:') ax[2].plot([0, xa], [eq(xa), eq(xa)], 'm:') ax[2].plot(xa, eq(xa), marker='o', ms=15, mec='red', color='white') ax[2].plot(xa, ya, 'ro', ms=15) ax[2].text(0, ya, 'f(a)', horizontalalignment='right', verticalalignment='center', fontsize=15) ax[2].text(0, eq(xa), 'L', horizontalalignment='right', verticalalignment='center', fontsize=15) In\u00a0[\u00a0]: Copied! <pre>fig.tight_layout()\n</pre> fig.tight_layout() In\u00a0[\u00a0]: Copied! <pre>buffer = StringIO()\nplt.savefig(buffer, format=\"svg\", transparent=True)\nprint(buffer.getvalue())\n</pre> buffer = StringIO() plt.savefig(buffer, format=\"svg\", transparent=True) print(buffer.getvalue())"},{"location":"template/roteiro4/main/","title":"Roteiro 4","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> <p></p> <p></p> <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"},{"location":"template/roteiro4/smc/","title":"Smc","text":"In\u00a0[\u00a0]: Copied! <pre>from datetime import datetime\nimport matplotlib.pyplot as plt\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nfrom io import StringIO\n</pre> from datetime import datetime import matplotlib.pyplot as plt import yfinance as yf import numpy as np import pandas as pd from io import StringIO In\u00a0[\u00a0]: Copied! <pre>num_days = 250\nnum_simulations = 200\norder_poly = 1\n</pre> num_days = 250 num_simulations = 200 order_poly = 1 In\u00a0[\u00a0]: Copied! <pre>ticker = '^BVSP'\n</pre> ticker = '^BVSP' In\u00a0[\u00a0]: Copied! <pre>info = yf.Ticker(ticker)\ndata = info.history(period='2y')\n</pre> info = yf.Ticker(ticker) data = info.history(period='2y') In\u00a0[\u00a0]: Copied! <pre>close = data['Close']\ndaily_return = close.pct_change()\n</pre> close = data['Close'] daily_return = close.pct_change() In\u00a0[\u00a0]: Copied! <pre>x = np.linspace(1, len(close), len(close))\nf = np.poly1d(np.polyfit(x, close, order_poly))\nxs = np.linspace(max(x), max(x) + num_days, num_days)\n</pre> x = np.linspace(1, len(close), len(close)) f = np.poly1d(np.polyfit(x, close, order_poly)) xs = np.linspace(max(x), max(x) + num_days, num_days) In\u00a0[\u00a0]: Copied! <pre>sigma = daily_return.std()\nmu = daily_return.mean()\n</pre> sigma = daily_return.std() mu = daily_return.mean() In\u00a0[\u00a0]: Copied! <pre>simulated_prices = np.zeros((num_days, num_simulations))\n</pre> simulated_prices = np.zeros((num_days, num_simulations)) In\u00a0[\u00a0]: Copied! <pre>for i in range(num_simulations):\n    simulated_prices[0][i] = close[-1]\n    for j in range(1, num_days):\n        daily_return = np.random.normal(mu, sigma)\n        simulated_prices[j][i] = simulated_prices[j-1][i] * (1 + daily_return)\n</pre> for i in range(num_simulations):     simulated_prices[0][i] = close[-1]     for j in range(1, num_days):         daily_return = np.random.normal(mu, sigma)         simulated_prices[j][i] = simulated_prices[j-1][i] * (1 + daily_return) In\u00a0[\u00a0]: Copied! <pre>simulated_means = np.mean(simulated_prices, axis=1)\nsimulated_stds = np.std(simulated_prices, axis=1)\n</pre> simulated_means = np.mean(simulated_prices, axis=1) simulated_stds = np.std(simulated_prices, axis=1) In\u00a0[\u00a0]: Copied! <pre>fig, ax = plt.subplots(1, 1, figsize=(12, 8))\nax.plot(\n    x, close,\n    x, f(x), 'r:',\n    xs, simulated_prices,\n    xs, simulated_means,\n    xs, simulated_means + 1*simulated_stds, 'w:',\n    xs, simulated_means - 1*simulated_stds, 'w:',\n    xs, simulated_means + 2*simulated_stds, 'k:',\n    xs, simulated_means - 2*simulated_stds, 'k:',\n    xs, f(xs), 'g',\n)\nax.set_xlim(min(x), max(xs))\n</pre> fig, ax = plt.subplots(1, 1, figsize=(12, 8)) ax.plot(     x, close,     x, f(x), 'r:',     xs, simulated_prices,     xs, simulated_means,     xs, simulated_means + 1*simulated_stds, 'w:',     xs, simulated_means - 1*simulated_stds, 'w:',     xs, simulated_means + 2*simulated_stds, 'k:',     xs, simulated_means - 2*simulated_stds, 'k:',     xs, f(xs), 'g', ) ax.set_xlim(min(x), max(xs)) In\u00a0[\u00a0]: Copied! <pre>buffer = StringIO()\nplt.savefig(buffer, format=\"svg\")\nprint(buffer.getvalue())\n</pre> buffer = StringIO() plt.savefig(buffer, format=\"svg\") print(buffer.getvalue())"},{"location":"template/thisdocumentation/main/","title":"This documentation","text":""},{"location":"template/thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"template/thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"template/thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"template/thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}