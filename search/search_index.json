{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Relat\u00f3rios de Atividades \u2014 Redes Neurais (2025.1)","text":"\u00daltima atualiza\u00e7\u00e3o <p>16/11/2025</p>"},{"location":"#aluno","title":"Aluno","text":"<ul> <li>Gabriel Mendon\u00e7a de Mello Hermida</li> </ul>"},{"location":"#atividades-notebooks","title":"Atividades (notebooks)","text":"<ul> <li> <p> Atividade 1 \u2014 Pr\u00e9-processamento (Spaceship Titanic) Notebook: <code>exercises/01_data/data.ipynb</code> O que foi feito: descri\u00e7\u00e3o do objetivo (Transported), identifica\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas e categ\u00f3ricas, auditoria de aus\u00eancias (~2% em v\u00e1rias colunas). Pr\u00e9-processamento: imputa\u00e7\u00e3o (mediana p/ num\u00e9ricos; mais frequente p/ categ\u00f3ricos), one-hot encoding, padroniza\u00e7\u00e3o; para gastos (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>) aplicou-se log1p + padroniza\u00e7\u00e3o para reduzir caudas longas. Evid\u00eancias: histogramas \u201cantes/depois\u201d de <code>Age</code> (padronizada) e <code>FoodCourt</code> (log1p + padronizada), mostrando centraliza\u00e7\u00e3o e escala adequadas \u00e0 <code>tanh</code>.</p> </li> <li> <p> Atividade 2 \u2014 Perceptron from scratch Notebook: <code>exercises/02_perceptron/perceptron.ipynb</code> Cen\u00e1rio A (separ\u00e1vel): m\u00e9dias bem separadas e baixa vari\u00e2ncia \u2192 converg\u00eancia em poucas \u00e9pocas e acur\u00e1cia 100%; fronteira linear sem erros residuais. Cen\u00e1rio B (n\u00e3o separ\u00e1vel): m\u00e9dias pr\u00f3ximas e maior vari\u00e2ncia \u2192 sobreposi\u00e7\u00e3o, oscila\u00e7\u00f5es de acur\u00e1cia (~50\u201370%) e n\u00e3o converg\u00eancia, evidenciando a limita\u00e7\u00e3o do perceptron para fronteiras n\u00e3o lineares.</p> </li> <li> <p> Atividade 3 \u2014 MLP from scratch Notebook: <code>exercises/03_mlp/mlp.ipynb</code> Bin\u00e1rio (2D, 1 oculta): MLP raso (tanh \u2192 sigmoid, BCE). Perda estabiliza ~0.52; acur\u00e1cia de teste ~62%; fronteira separa parcialmente com erros em zonas de sobreposi\u00e7\u00e3o.   Multiclasse (3 classes):   \u2022 MLP raso: perda final ~0.68; acur\u00e1cia de teste 72,3%.   \u2022 MLP profundo (2 ocultas): perda final ~0.47; acur\u00e1cia de teste 80,7%.   Conclus\u00e3o: aumentar profundidade melhora a capacidade de representa\u00e7\u00e3o e o desempenho.</p> </li> <li> <p> Atividade 4 \u2014 Variational Autoencoders (VAE) Notebook: <code>exercises/04_vae/vae.ipynb</code> Implementa\u00e7\u00e3o: VAE para MNIST com encoder-decoder (652k par\u00e2metros), reparameterization trick para amostragem diferenci\u00e1vel.   T\u00e9cnicas: Beta annealing (\u03b2: 0\u21921 em 10 \u00e9pocas), acelera\u00e7\u00e3o GPU com PyTorch.   Resultados: Loss total 102.64 (BCE: 76.81, KL: 25.82). Visualiza\u00e7\u00f5es de reconstru\u00e7\u00e3o, gera\u00e7\u00e3o e espa\u00e7o latente via t-SNE.</p> </li> </ul>"},{"location":"#projetos","title":"Projetos","text":"<ul> <li> Projeto 3 \u2014 Modelos Generativos (Stable Diffusion + ControlNet) Notebook: <code>projects/generative/generative.ipynb</code> Pipeline: Text-to-image com SD 1.5 + ControlNet Canny para controle espacial.   Exemplos: 5 designs de produtos (cadeiras, smartwatch, rel\u00f3gio, garrafa) com varia\u00e7\u00e3o de par\u00e2metros.   Documenta\u00e7\u00e3o: Diagramas de arquitetura, an\u00e1lise de hiperpar\u00e2metros e otimiza\u00e7\u00f5es de mem\u00f3ria.</li> </ul>"},{"location":"exercises/01_data/data/","title":"Prepara\u00e7\u00e3o e An\u00e1lise de Dados para Redes Neurais","text":"In\u00a0[7]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nN = 100\nparams = {\n    0: {\"mean\": [2, 3],  \"std\": [0.8, 2.5]},\n    1: {\"mean\": [5, 6],  \"std\": [1.2, 1.9]},\n    2: {\"mean\": [8, 1],  \"std\": [0.9, 0.9]},\n    3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]},\n}\n\nXs, ys = [], []\nfor c, p in params.items():\n    mean = np.array(p[\"mean\"])\n    std = np.array(p[\"std\"])\n    Xc = np.random.randn(N, 2) * std + mean\n    Xs.append(Xc)\n    ys.append(np.full(N, c))\n\nX = np.vstack(Xs)\ny = np.hstack(ys)\n\nplt.figure(figsize=(7,5))\nfor c in params.keys():\n    plt.scatter(X[y==c,0], X[y==c,1], s=12, label=f\"Classe {c}\", alpha=0.8)\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.title(\"Exerc\u00edcio 1 \u2014 Distribui\u00e7\u00e3o 2D (4 classes)\")\nplt.legend()\nplt.show()\n</pre>  import numpy as np import matplotlib.pyplot as plt  np.random.seed(42)  N = 100 params = {     0: {\"mean\": [2, 3],  \"std\": [0.8, 2.5]},     1: {\"mean\": [5, 6],  \"std\": [1.2, 1.9]},     2: {\"mean\": [8, 1],  \"std\": [0.9, 0.9]},     3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]}, }  Xs, ys = [], [] for c, p in params.items():     mean = np.array(p[\"mean\"])     std = np.array(p[\"std\"])     Xc = np.random.randn(N, 2) * std + mean     Xs.append(Xc)     ys.append(np.full(N, c))  X = np.vstack(Xs) y = np.hstack(ys)  plt.figure(figsize=(7,5)) for c in params.keys():     plt.scatter(X[y==c,0], X[y==c,1], s=12, label=f\"Classe {c}\", alpha=0.8) plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.title(\"Exerc\u00edcio 1 \u2014 Distribui\u00e7\u00e3o 2D (4 classes)\") plt.legend() plt.show()  <p>As quatro classes se distribuem em regi\u00f5es distintas do plano, cada uma concentrada em torno de um centro espec\u00edfico. Embora o eixo x1 contribua fortemente para a separa\u00e7\u00e3o, a variabilidade em x2 cria dispers\u00f5es diferentes entre os grupos. Esse cen\u00e1rio exige a combina\u00e7\u00e3o de m\u00faltiplas fronteiras de decis\u00e3o ou o uso de modelos n\u00e3o lineares para capturar de forma adequada a estrutura dos dados.</p> In\u00a0[8]: Copied! <pre># Superf\u00edcies de decis\u00e3o com MLP\nfrom sklearn.neural_network import MLPClassifier\n\nclf = MLPClassifier(hidden_layer_sizes=(16,), activation=\"tanh\", max_iter=2000, random_state=42)\nclf.fit(X, y)\n\nxx, yy = np.meshgrid(\n    np.linspace(X[:,0].min()-1, X[:,0].max()+1, 300),\n    np.linspace(X[:,1].min()-1, X[:,1].max()+1, 300),\n)\nZZ = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\nplt.figure(figsize=(7,5))\nplt.contourf(xx, yy, ZZ, alpha=0.25, levels=[-0.5,0.5,1.5,2.5,3.5])\nfor c in params.keys():\n    plt.scatter(X[y==c,0], X[y==c,1], s=10, label=f\"Classe {c}\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.title(\"Exerc\u00edcio 1 \u2014 Superf\u00edcies de decis\u00e3o (MLP tanh)\")\nplt.legend()\nplt.show()\n</pre>  # Superf\u00edcies de decis\u00e3o com MLP from sklearn.neural_network import MLPClassifier  clf = MLPClassifier(hidden_layer_sizes=(16,), activation=\"tanh\", max_iter=2000, random_state=42) clf.fit(X, y)  xx, yy = np.meshgrid(     np.linspace(X[:,0].min()-1, X[:,0].max()+1, 300),     np.linspace(X[:,1].min()-1, X[:,1].max()+1, 300), ) ZZ = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)  plt.figure(figsize=(7,5)) plt.contourf(xx, yy, ZZ, alpha=0.25, levels=[-0.5,0.5,1.5,2.5,3.5]) for c in params.keys():     plt.scatter(X[y==c,0], X[y==c,1], s=10, label=f\"Classe {c}\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.title(\"Exerc\u00edcio 1 \u2014 Superf\u00edcies de decis\u00e3o (MLP tanh)\") plt.legend() plt.show()  In\u00a0[9]: Copied! <pre>from sklearn.decomposition import PCA\n\nnp.random.seed(7)\n\nmuA = np.array([0, 0, 0, 0, 0])\nSigmaA = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0]\n])\n\nmuB = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\nSigmaB = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5]\n])\n\nXA = np.random.multivariate_normal(muA, SigmaA, size=500)\nXB = np.random.multivariate_normal(muB, SigmaB, size=500)\nX5 = np.vstack([XA, XB])\ny5 = np.hstack([np.zeros(500), np.ones(500)])\n\npca = PCA(n_components=2, random_state=42)\nX2 = pca.fit_transform(X5)\n\nplt.figure(figsize=(7,5))\nplt.scatter(X2[y5==0,0], X2[y5==0,1], s=10, label=\"Classe A\")\nplt.scatter(X2[y5==1,0], X2[y5==1,1], s=10, label=\"Classe B\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.title(\"Exerc\u00edcio 2 \u2014 PCA (5D \u2192 2D)\")\nplt.legend()\nplt.show()\n</pre>  from sklearn.decomposition import PCA  np.random.seed(7)  muA = np.array([0, 0, 0, 0, 0]) SigmaA = np.array([     [1.0, 0.8, 0.1, 0.0, 0.0],     [0.8, 1.0, 0.3, 0.0, 0.0],     [0.1, 0.3, 1.0, 0.5, 0.0],     [0.0, 0.0, 0.5, 1.0, 0.2],     [0.0, 0.0, 0.0, 0.2, 1.0] ])  muB = np.array([1.5, 1.5, 1.5, 1.5, 1.5]) SigmaB = np.array([     [1.5, -0.7, 0.2, 0.0, 0.0],     [-0.7, 1.5, 0.4, 0.0, 0.0],     [0.2, 0.4, 1.5, 0.6, 0.0],     [0.0, 0.0, 0.6, 1.5, 0.3],     [0.0, 0.0, 0.0, 0.3, 1.5] ])  XA = np.random.multivariate_normal(muA, SigmaA, size=500) XB = np.random.multivariate_normal(muB, SigmaB, size=500) X5 = np.vstack([XA, XB]) y5 = np.hstack([np.zeros(500), np.ones(500)])  pca = PCA(n_components=2, random_state=42) X2 = pca.fit_transform(X5)  plt.figure(figsize=(7,5)) plt.scatter(X2[y5==0,0], X2[y5==0,1], s=10, label=\"Classe A\") plt.scatter(X2[y5==1,0], X2[y5==1,1], s=10, label=\"Classe B\") plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\") plt.title(\"Exerc\u00edcio 2 \u2014 PCA (5D \u2192 2D)\") plt.legend() plt.show()  <p>A proje\u00e7\u00e3o via PCA mostra que as classes A e B t\u00eam centros deslocados, mas ainda apresentam forte sobreposi\u00e7\u00e3o devido \u00e0 vari\u00e2ncia dentro de cada grupo. Essa configura\u00e7\u00e3o torna a separa\u00e7\u00e3o linear pouco eficaz, j\u00e1 que n\u00e3o existe um hiperplano simples que separe bem as duas classes. Modelos mais expressivos, que incorporam n\u00e3o-linearidades, s\u00e3o mais adequados para capturar as fronteiras complexas observadas no espa\u00e7o reduzido.</p> <p>O dataset Spaceship Titanic foi proposto em uma competi\u00e7\u00e3o do Kaggle e tem como objetivo prever a vari\u00e1vel Transported, que indica se um passageiro foi levado para outra dimens\u00e3o durante a viagem.</p> <p>Os dados incluem atributos num\u00e9ricos, como <code>Age</code> e os gastos a bordo (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>), e atributos categ\u00f3ricos, como <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code> e <code>Cabin</code>. Antes do treinamento de uma rede neural, \u00e9 necess\u00e1rio inspecionar a base para identificar tipos de vari\u00e1veis, valores ausentes e definir as transforma\u00e7\u00f5es adequadas.</p> In\u00a0[10]: Copied! <pre>import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nplt.rcParams[\"figure.figsize\"] = (8,3.5)\n\ncsv_path = \"data/train.csv\"\nassert os.path.exists(csv_path), f\"Arquivo n\u00e3o encontrado: {csv_path}\"\n\ndf = pd.read_csv(csv_path)\n\ntarget_col = \"Transported\"\nnum_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\ncat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Cabin\"]\n\n# Colunas de gastos com forte assimetria (usar log1p)\nspend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n</pre> import os, numpy as np, pandas as pd, matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer plt.rcParams[\"figure.figsize\"] = (8,3.5)  csv_path = \"data/train.csv\" assert os.path.exists(csv_path), f\"Arquivo n\u00e3o encontrado: {csv_path}\"  df = pd.read_csv(csv_path)  target_col = \"Transported\" num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] cat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Cabin\"]  # Colunas de gastos com forte assimetria (usar log1p) spend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"] In\u00a0[11]: Copied! <pre>print(\"Tipos detectados e relat\u00f3rio de valores ausentes:\")\ndisplay(df[num_cols + cat_cols + [target_col]].dtypes)\n\nna_count = df[num_cols + cat_cols + [target_col]].isna().sum().sort_values(ascending=False)\nna_pct = (na_count / len(df)).round(3)\nmissing_report = pd.DataFrame({\"missing\": na_count, \"pct\": na_pct})\ndisplay(missing_report[missing_report[\"missing\"] &gt; 0])\n</pre> print(\"Tipos detectados e relat\u00f3rio de valores ausentes:\") display(df[num_cols + cat_cols + [target_col]].dtypes)  na_count = df[num_cols + cat_cols + [target_col]].isna().sum().sort_values(ascending=False) na_pct = (na_count / len(df)).round(3) missing_report = pd.DataFrame({\"missing\": na_count, \"pct\": na_pct}) display(missing_report[missing_report[\"missing\"] &gt; 0]) <pre>Tipos detectados e relat\u00f3rio de valores ausentes:\n</pre> <pre>Age             float64\nRoomService     float64\nFoodCourt       float64\nShoppingMall    float64\nSpa             float64\nVRDeck          float64\nHomePlanet       object\nCryoSleep        object\nDestination      object\nVIP              object\nCabin            object\nTransported        bool\ndtype: object</pre> missing pct CryoSleep 217 0.025 ShoppingMall 208 0.024 VIP 203 0.023 HomePlanet 201 0.023 Cabin 199 0.023 VRDeck 188 0.022 FoodCourt 183 0.021 Spa 183 0.021 Destination 182 0.021 RoomService 181 0.021 Age 179 0.021 <p>A inspe\u00e7\u00e3o revelou que os atributos de gastos (<code>RoomService</code>, <code>FoodCourt</code>, <code>ShoppingMall</code>, <code>Spa</code>, <code>VRDeck</code>) e <code>Age</code> s\u00e3o num\u00e9ricos, enquanto <code>HomePlanet</code>, <code>CryoSleep</code>, <code>Destination</code>, <code>VIP</code> e <code>Cabin</code> s\u00e3o categ\u00f3ricos.</p> <p>O relat\u00f3rio de valores ausentes mostrou taxas pr\u00f3ximas de 2% em quase todas as colunas. Isso indica a necessidade de imputa\u00e7\u00e3o sistem\u00e1tica: a mediana para atributos num\u00e9ricos, robusta a outliers, e o valor mais frequente para categ\u00f3ricos, preservando consist\u00eancia.</p> <p>Al\u00e9m disso, os atributos num\u00e9ricos n\u00e3o seguem todos a mesma distribui\u00e7\u00e3o: <code>Age</code> tem assimetria moderada, enquanto os gastos apresentam caudas longas e forte concentra\u00e7\u00e3o em zero. Essa diferen\u00e7a motiva estrat\u00e9gias espec\u00edficas de transforma\u00e7\u00e3o, ilustradas a seguir.</p> In\u00a0[12]: Copied! <pre>def plot_before_after(series, transformer, title_after):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3.5))\n    ax1.hist(series.dropna().values, bins=40)\n    ax1.set_title(f\"{series.name} \u2014 original\")\n    tr = transformer.fit_transform(series.to_frame())\n    ax2.hist(tr.ravel(), bins=40)\n    ax2.set_title(f\"{series.name} \u2014 {title_after}\")\n    plt.tight_layout(); plt.show()\n\nage_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\nplot_before_after(df[\"Age\"], age_pipe, \"padronizada (StandardScaler)\")\n</pre> def plot_before_after(series, transformer, title_after):     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3.5))     ax1.hist(series.dropna().values, bins=40)     ax1.set_title(f\"{series.name} \u2014 original\")     tr = transformer.fit_transform(series.to_frame())     ax2.hist(tr.ravel(), bins=40)     ax2.set_title(f\"{series.name} \u2014 {title_after}\")     plt.tight_layout(); plt.show()  age_pipe = Pipeline([     (\"imputer\", SimpleImputer(strategy=\"median\")),     (\"scaler\", StandardScaler()) ]) plot_before_after(df[\"Age\"], age_pipe, \"padronizada (StandardScaler)\")  In\u00a0[13]: Copied! <pre>food_pipe = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"log\", FunctionTransformer(np.log1p, validate=False)),\n    (\"scaler\", StandardScaler())\n])\nplot_before_after(df[\"FoodCourt\"], food_pipe, \"log1p + padronizada\")\n</pre> food_pipe = Pipeline([     (\"imputer\", SimpleImputer(strategy=\"median\")),     (\"log\", FunctionTransformer(np.log1p, validate=False)),     (\"scaler\", StandardScaler()) ]) plot_before_after(df[\"FoodCourt\"], food_pipe, \"log1p + padronizada\")  <p>Os gr\u00e1ficos confirmam o efeito das transforma\u00e7\u00f5es. Em <code>Age</code>, a padroniza\u00e7\u00e3o centralizou a distribui\u00e7\u00e3o em torno de zero e ajustou a escala para desvio padr\u00e3o um, tornando os valores mais adequados para fun\u00e7\u00f5es de ativa\u00e7\u00e3o como <code>tanh</code>.</p> <p>J\u00e1 em <code>FoodCourt</code>, a aplica\u00e7\u00e3o de <code>log1p</code> antes da padroniza\u00e7\u00e3o reduziu o impacto de caudas longas e outliers, resultando em uma distribui\u00e7\u00e3o mais equilibrada. O mesmo racioc\u00ednio pode ser aplicado \u00e0s demais vari\u00e1veis de gastos.</p> <p>Por fim, os atributos categ\u00f3ricos ser\u00e3o tratados com imputa\u00e7\u00e3o do valor mais frequente e convertidos em indicadores bin\u00e1rios via one-hot encoding. O dataset resultante torna-se mais homog\u00eaneo e apropriado para redes neurais, favorecendo a estabilidade do gradiente e o desempenho do modelo.</p>"},{"location":"exercises/01_data/data/#preparacao-e-analise-de-dados-para-redes-neurais","title":"Prepara\u00e7\u00e3o e An\u00e1lise de Dados para Redes Neurais\u00b6","text":""},{"location":"exercises/01_data/data/#objetivo","title":"Objetivo\u00b6","text":"<p>Explorar separabilidade de classes em 2D, projetar dados 5D para 2D com PCA e preparar o dataset Spaceship Titanic para redes neurais com ativa\u00e7\u00e3o <code>tanh</code>.</p>"},{"location":"exercises/01_data/data/#exercicio-1-dados-2d-4-classes","title":"Exerc\u00edcio 1 \u2014 Dados 2D (4 classes)\u00b6","text":""},{"location":"exercises/01_data/data/#exercicio-2-dados-5d-ab-pca2d","title":"Exerc\u00edcio 2 \u2014 Dados 5D (A/B) + PCA(2D)\u00b6","text":""},{"location":"exercises/01_data/data/#exercicio-3-spaceship-titanic-pre-processamento","title":"Exerc\u00edcio 3 \u2014 Spaceship Titanic: pr\u00e9-processamento\u00b6","text":""},{"location":"exercises/01_data/data/#conclusoes","title":"Conclus\u00f5es\u00b6","text":"<p>Os experimentos mostraram que, em dados sint\u00e9ticos 2D, a separa\u00e7\u00e3o linear n\u00e3o \u00e9 suficiente, exigindo fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares para capturar fronteiras mais complexas. Na proje\u00e7\u00e3o dos dados 5D em 2D, a sobreposi\u00e7\u00e3o causada por correla\u00e7\u00f5es entre atributos refor\u00e7a essa limita\u00e7\u00e3o e destaca a necessidade de modelos mais expressivos. J\u00e1 no caso do Spaceship Titanic, o pr\u00e9-processamento com imputa\u00e7\u00e3o, codifica\u00e7\u00e3o categ\u00f3rica e padroniza\u00e7\u00e3o num\u00e9rica foi fundamental para tornar o conjunto compat\u00edvel com redes neurais baseadas em <code>tanh</code>, garantindo maior estabilidade no treinamento e melhor capacidade de generaliza\u00e7\u00e3o.</p>"},{"location":"exercises/02_perceptron/perceptron/","title":"Entendendo Perceptrons e Suas Limita\u00e7\u00f5es","text":"In\u00a0[65]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n</pre> import numpy as np import matplotlib.pyplot as plt  np.random.seed(42) In\u00a0[66]: Copied! <pre># par\u00e2metros\nmean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]]\nmean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]\n\n# gerar amostras\nn_samples = 1000\nclass0 = np.random.multivariate_normal(mean0, cov0, n_samples)\nclass1 = np.random.multivariate_normal(mean1, cov1, n_samples)\n\n# juntar\nX = np.vstack((class0, class1))\ny = np.hstack((-1*np.ones(n_samples), +1*np.ones(n_samples)))  # r\u00f3tulos {-1, +1}\n\n# plot\nplt.figure(figsize=(6,6))\nplt.scatter(class0[:,0], class0[:,1], alpha=0.6, label=\"Classe 0\")\nplt.scatter(class1[:,0], class1[:,1], alpha=0.6, label=\"Classe 1\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend(); plt.title(\"Dados 2D \u2014 Classes 0 e 1\")\nplt.show()\n</pre> # par\u00e2metros mean0, cov0 = [1.5, 1.5], [[0.5, 0], [0, 0.5]] mean1, cov1 = [5, 5], [[0.5, 0], [0, 0.5]]  # gerar amostras n_samples = 1000 class0 = np.random.multivariate_normal(mean0, cov0, n_samples) class1 = np.random.multivariate_normal(mean1, cov1, n_samples)  # juntar X = np.vstack((class0, class1)) y = np.hstack((-1*np.ones(n_samples), +1*np.ones(n_samples)))  # r\u00f3tulos {-1, +1}  # plot plt.figure(figsize=(6,6)) plt.scatter(class0[:,0], class0[:,1], alpha=0.6, label=\"Classe 0\") plt.scatter(class1[:,0], class1[:,1], alpha=0.6, label=\"Classe 1\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend(); plt.title(\"Dados 2D \u2014 Classes 0 e 1\") plt.show() In\u00a0[67]: Copied! <pre>class Perceptron:\n    def __init__(self, lr=0.01, max_epochs=100, shuffle=True, seed=42):\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.shuffle = shuffle\n        self.seed = seed\n        self.w = None\n        self.b = None\n        self.history_ = []  # acur\u00e1cia por \u00e9poca\n\n    @staticmethod\n    def _step(z):\n        return np.where(z &gt;= 0.0, 1, -1)\n\n    def predict(self, X):\n        return self._step(X @ self.w + self.b)\n\n    def fit(self, X, y):\n        rng = np.random.default_rng(self.seed)\n        n, d = X.shape\n        self.w = np.zeros(d)\n        self.b = 0.0\n        self.history_.clear()\n\n        for epoch in range(1, self.max_epochs + 1):\n            idx = np.arange(n)\n            if self.shuffle:\n                rng.shuffle(idx)\n            updates = 0\n\n            for i in idx:\n                xi, yi = X[i], y[i]\n                y_hat = self._step(self.w @ xi + self.b)\n                if y_hat != yi:\n                    self.w += self.lr * yi * xi\n                    self.b += self.lr * yi\n                    updates += 1\n\n            # acur\u00e1cia ao fim da \u00e9poca\n            acc = (self.predict(X) == y).mean()\n            self.history_.append(acc)\n\n            if updates == 0:  # convergiu\n                break\n\n        return self\n</pre> class Perceptron:     def __init__(self, lr=0.01, max_epochs=100, shuffle=True, seed=42):         self.lr = lr         self.max_epochs = max_epochs         self.shuffle = shuffle         self.seed = seed         self.w = None         self.b = None         self.history_ = []  # acur\u00e1cia por \u00e9poca      @staticmethod     def _step(z):         return np.where(z &gt;= 0.0, 1, -1)      def predict(self, X):         return self._step(X @ self.w + self.b)      def fit(self, X, y):         rng = np.random.default_rng(self.seed)         n, d = X.shape         self.w = np.zeros(d)         self.b = 0.0         self.history_.clear()          for epoch in range(1, self.max_epochs + 1):             idx = np.arange(n)             if self.shuffle:                 rng.shuffle(idx)             updates = 0              for i in idx:                 xi, yi = X[i], y[i]                 y_hat = self._step(self.w @ xi + self.b)                 if y_hat != yi:                     self.w += self.lr * yi * xi                     self.b += self.lr * yi                     updates += 1              # acur\u00e1cia ao fim da \u00e9poca             acc = (self.predict(X) == y).mean()             self.history_.append(acc)              if updates == 0:  # convergiu                 break          return self  In\u00a0[68]: Copied! <pre>per = Perceptron(lr=0.01, max_epochs=100, shuffle=True, seed=42).fit(X, y)\n\ny_pred = per.predict(X)\nacc = (y_pred == y).mean()\n\nprint(f\"Pesos (w): {per.w}\")\nprint(f\"Vi\u00e9s (b): {per.b:.4f}\")\nprint(f\"Acur\u00e1cia final no conjunto completo: {acc:.4f}\")\nprint(f\"\u00c9pocas executadas: {len(per.history_)}\")\n</pre> per = Perceptron(lr=0.01, max_epochs=100, shuffle=True, seed=42).fit(X, y)  y_pred = per.predict(X) acc = (y_pred == y).mean()  print(f\"Pesos (w): {per.w}\") print(f\"Vi\u00e9s (b): {per.b:.4f}\") print(f\"Acur\u00e1cia final no conjunto completo: {acc:.4f}\") print(f\"\u00c9pocas executadas: {len(per.history_)}\")  <pre>Pesos (w): [0.02627968 0.02874301]\nVi\u00e9s (b): -0.1800\nAcur\u00e1cia final no conjunto completo: 1.0000\n\u00c9pocas executadas: 3\n</pre> In\u00a0[69]: Copied! <pre>def plot_decision_boundary_2d(X, y, w, b, title):\n    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n    xs = np.linspace(x_min, x_max, 400)\n\n    fig, ax = plt.subplots(figsize=(6,6))\n    ax.scatter(X[y==-1,0], X[y==-1,1], s=10, alpha=0.7, label=\"Classe -1\")\n    ax.scatter(X[y==+1,0], X[y==+1,1], s=10, alpha=0.7, label=\"Classe +1\")\n\n    if abs(w[1]) &gt; 1e-12:\n        ys = -(w[0]/w[1]) * xs - b / w[1]\n        ax.plot(xs, ys, \"k--\", lw=2, label=\"Fronteira do perceptron\")\n\n    # destacar erros\n    y_hat = np.where(X @ w + b &gt;= 0, 1, -1)\n    err = (y_hat != y)\n    if err.any():\n        ax.scatter(X[err,0], X[err,1], s=30, facecolors='none', edgecolors='r', label=\"Erros\")\n\n    ax.set_title(title)\n    ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\"); ax.legend()\n    plt.show()\n\nplot_decision_boundary_2d(X, y, per.w, per.b, \"Perceptron \u2014 fronteira e erros (Ex. 1)\")\n</pre> def plot_decision_boundary_2d(X, y, w, b, title):     x_min, x_max = X[:,0].min()-1, X[:,0].max()+1     xs = np.linspace(x_min, x_max, 400)      fig, ax = plt.subplots(figsize=(6,6))     ax.scatter(X[y==-1,0], X[y==-1,1], s=10, alpha=0.7, label=\"Classe -1\")     ax.scatter(X[y==+1,0], X[y==+1,1], s=10, alpha=0.7, label=\"Classe +1\")      if abs(w[1]) &gt; 1e-12:         ys = -(w[0]/w[1]) * xs - b / w[1]         ax.plot(xs, ys, \"k--\", lw=2, label=\"Fronteira do perceptron\")      # destacar erros     y_hat = np.where(X @ w + b &gt;= 0, 1, -1)     err = (y_hat != y)     if err.any():         ax.scatter(X[err,0], X[err,1], s=30, facecolors='none', edgecolors='r', label=\"Erros\")      ax.set_title(title)     ax.set_xlabel(\"x1\"); ax.set_ylabel(\"x2\"); ax.legend()     plt.show()  plot_decision_boundary_2d(X, y, per.w, per.b, \"Perceptron \u2014 fronteira e erros (Ex. 1)\")  In\u00a0[70]: Copied! <pre>from matplotlib.ticker import FormatStrFormatter, MaxNLocator\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(per.history_)+1), per.history_, marker=\"o\")\nax.set_xlabel(\"\u00c9poca\")\nax.set_ylabel(\"Acur\u00e1cia (treino)\")\nax.set_title(\"Converg\u00eancia do Perceptron (Ex. 1)\")\n\n# mostrar valores absolutos, sem offset, com 3 casas\nax.ticklabel_format(axis=\"y\", useOffset=False)\nax.yaxis.set_major_formatter(FormatStrFormatter(\"%.3f\"))\nax.set_ylim(0.90, 1.001)          # zoom na faixa alta (ajuste se quiser)\nax.yaxis.set_major_locator(MaxNLocator(nbins=6))\nax.grid(True)\nplt.show()\n</pre> from matplotlib.ticker import FormatStrFormatter, MaxNLocator  fig, ax = plt.subplots() ax.plot(np.arange(1, len(per.history_)+1), per.history_, marker=\"o\") ax.set_xlabel(\"\u00c9poca\") ax.set_ylabel(\"Acur\u00e1cia (treino)\") ax.set_title(\"Converg\u00eancia do Perceptron (Ex. 1)\")  # mostrar valores absolutos, sem offset, com 3 casas ax.ticklabel_format(axis=\"y\", useOffset=False) ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.3f\")) ax.set_ylim(0.90, 1.001)          # zoom na faixa alta (ajuste se quiser) ax.yaxis.set_major_locator(MaxNLocator(nbins=6)) ax.grid(True) plt.show()  In\u00a0[71]: Copied! <pre>err_hist = 1.0 - np.array(per.history_)\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(err_hist)+1), err_hist, marker=\"o\")\nax.set_xlabel(\"\u00c9poca\")\nax.set_ylabel(\"Erro (1 - acur\u00e1cia)\")\nax.set_title(\"Erro por \u00e9poca (Ex. 1)\")\nax.set_ylim(-0.01, 0.1)           # ajuste conforme seu caso\nax.grid(True)\nplt.show()\n</pre> err_hist = 1.0 - np.array(per.history_)  fig, ax = plt.subplots() ax.plot(np.arange(1, len(err_hist)+1), err_hist, marker=\"o\") ax.set_xlabel(\"\u00c9poca\") ax.set_ylabel(\"Erro (1 - acur\u00e1cia)\") ax.set_title(\"Erro por \u00e9poca (Ex. 1)\") ax.set_ylim(-0.01, 0.1)           # ajuste conforme seu caso ax.grid(True) plt.show()  <p>No primeiro cen\u00e1rio, as m\u00e9dias distantes e a baixa vari\u00e2ncia tornam as classes praticamente linearmente separ\u00e1veis, e o perceptron converge em poucas \u00e9pocas, atingindo 100% de acur\u00e1cia. A fronteira linear separa perfeitamente as duas nuvens, sem erros residuais, em linha com o teorema do perceptron (converg\u00eancia em n\u00famero finito de atualiza\u00e7\u00f5es para dados separ\u00e1veis).</p> <p>A aparente \u201csubida imediata\u201d para 1.0 vem do fato de que a acur\u00e1cia \u00e9 medida ao fim da \u00e9poca. Durante a passagem pelos dados, o perceptron analisa um ponto por vez e, quando erra, empurra levemente a reta (ajusta $w$ e $b$) para o lado correto. A soma desses pequenos ajustes na primeira \u00e9poca j\u00e1 posiciona a reta quase no lugar ideal, fazendo a acur\u00e1cia ao final ficar perto de 0,9995 (que aparece como 1,000 ao arredondar). Na \u00e9poca seguinte, como a reta j\u00e1 est\u00e1 bem colocada, quase n\u00e3o h\u00e1 novos erros; ao completar uma \u00e9poca sem corre\u00e7\u00f5es (zero updates), considera-se converg\u00eancia e o gr\u00e1fico de erro cai a 0 \u2014 aprendizado est\u00e1vel e eficiente.</p> In\u00a0[72]: Copied! <pre># par\u00e2metros\nmean0, cov0 = [3, 3], [[1.5, 0], [0, 1.5]]\nmean1, cov1 = [4, 4], [[1.5, 0], [0, 1.5]]\n\n# gerar amostras\nn_samples = 1000\nclass0 = np.random.multivariate_normal(mean0, cov0, n_samples)\nclass1 = np.random.multivariate_normal(mean1, cov1, n_samples)\n\n# juntar\nX = np.vstack((class0, class1))\ny = np.hstack((-1*np.ones(n_samples), +1*np.ones(n_samples)))  # r\u00f3tulos {-1, +1}\n\n# plot\nplt.figure(figsize=(6,6))\nplt.scatter(class0[:,0], class0[:,1], alpha=0.6, label=\"Classe 0\")\nplt.scatter(class1[:,0], class1[:,1], alpha=0.6, label=\"Classe 1\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\")\nplt.legend(); plt.title(\"Dados 2D \u2014 Classes 0 e 1\")\nplt.show()\n</pre> # par\u00e2metros mean0, cov0 = [3, 3], [[1.5, 0], [0, 1.5]] mean1, cov1 = [4, 4], [[1.5, 0], [0, 1.5]]  # gerar amostras n_samples = 1000 class0 = np.random.multivariate_normal(mean0, cov0, n_samples) class1 = np.random.multivariate_normal(mean1, cov1, n_samples)  # juntar X = np.vstack((class0, class1)) y = np.hstack((-1*np.ones(n_samples), +1*np.ones(n_samples)))  # r\u00f3tulos {-1, +1}  # plot plt.figure(figsize=(6,6)) plt.scatter(class0[:,0], class0[:,1], alpha=0.6, label=\"Classe 0\") plt.scatter(class1[:,0], class1[:,1], alpha=0.6, label=\"Classe 1\") plt.xlabel(\"x1\"); plt.ylabel(\"x2\") plt.legend(); plt.title(\"Dados 2D \u2014 Classes 0 e 1\") plt.show() In\u00a0[73]: Copied! <pre>per = Perceptron(lr=0.01, max_epochs=100, shuffle=True, seed=42).fit(X, y)\n\ny_pred = per.predict(X)\nacc = (y_pred == y).mean()\n\nprint(f\"Pesos (w): {per.w}\")\nprint(f\"Vi\u00e9s (b): {per.b:.4f}\")\nprint(f\"Acur\u00e1cia final no conjunto completo: {acc:.4f}\")\nprint(f\"\u00c9pocas executadas: {len(per.history_)}\")\n</pre> per = Perceptron(lr=0.01, max_epochs=100, shuffle=True, seed=42).fit(X, y)  y_pred = per.predict(X) acc = (y_pred == y).mean()  print(f\"Pesos (w): {per.w}\") print(f\"Vi\u00e9s (b): {per.b:.4f}\") print(f\"Acur\u00e1cia final no conjunto completo: {acc:.4f}\") print(f\"\u00c9pocas executadas: {len(per.history_)}\") <pre>Pesos (w): [0.06092489 0.01368968]\nVi\u00e9s (b): -0.4600\nAcur\u00e1cia final no conjunto completo: 0.5120\n\u00c9pocas executadas: 100\n</pre> In\u00a0[74]: Copied! <pre>plot_decision_boundary_2d(X, y, per.w, per.b, \"Perceptron \u2014 fronteira e erros (Ex. 2)\")\n</pre> plot_decision_boundary_2d(X, y, per.w, per.b, \"Perceptron \u2014 fronteira e erros (Ex. 2)\") In\u00a0[78]: Copied! <pre>fig, ax = plt.subplots()\nax.plot(np.arange(1, len(per.history_)+1), per.history_, marker=\"o\")\nax.set_xlabel(\"\u00c9poca\")\nax.set_ylabel(\"Acur\u00e1cia (treino)\")\nax.set_title(\"Converg\u00eancia do Perceptron (Ex. 2)\")\n\n# mostrar valores absolutos, sem offset, com 3 casas\nax.ticklabel_format(axis=\"y\", useOffset=False)\nax.yaxis.set_major_formatter(FormatStrFormatter(\"%.3f\"))\nax.yaxis.set_major_locator(MaxNLocator(nbins=6))\nax.grid(True)\nplt.show()\n</pre> fig, ax = plt.subplots() ax.plot(np.arange(1, len(per.history_)+1), per.history_, marker=\"o\") ax.set_xlabel(\"\u00c9poca\") ax.set_ylabel(\"Acur\u00e1cia (treino)\") ax.set_title(\"Converg\u00eancia do Perceptron (Ex. 2)\")  # mostrar valores absolutos, sem offset, com 3 casas ax.ticklabel_format(axis=\"y\", useOffset=False) ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.3f\")) ax.yaxis.set_major_locator(MaxNLocator(nbins=6)) ax.grid(True) plt.show() In\u00a0[79]: Copied! <pre>err_hist = 1.0 - np.array(per.history_)\n\nfig, ax = plt.subplots()\nax.plot(np.arange(1, len(err_hist)+1), err_hist, marker=\"o\")\nax.set_xlabel(\"\u00c9poca\")\nax.set_ylabel(\"Erro (1 - acur\u00e1cia)\")\nax.set_title(\"Erro por \u00e9poca (Ex. 2)\")\nax.grid(True)\nplt.show()\n</pre> err_hist = 1.0 - np.array(per.history_)  fig, ax = plt.subplots() ax.plot(np.arange(1, len(err_hist)+1), err_hist, marker=\"o\") ax.set_xlabel(\"\u00c9poca\") ax.set_ylabel(\"Erro (1 - acur\u00e1cia)\") ax.set_title(\"Erro por \u00e9poca (Ex. 2)\") ax.grid(True) plt.show()  <p>No segundo cen\u00e1rio, as m\u00e9dias mais pr\u00f3ximas e a vari\u00e2ncia maior geram forte sobreposi\u00e7\u00e3o entre as classes. Nesse regime, n\u00e3o existe uma reta que separe perfeitamente todos os pontos; logo, o perceptron cl\u00e1ssico (que s\u00f3 para quando n\u00e3o h\u00e1 mais erros em uma \u00e9poca) n\u00e3o converge. No nosso experimento, ele chegou ao limite de 100 \u00e9pocas com acur\u00e1cia em torno de 0,51, o que reflete a dificuldade intr\u00ednseca do problema.</p> <p>Os gr\u00e1ficos deixam isso claro: a fronteira linear cruza a regi\u00e3o onde as nuvens se misturam e concentra os erros ao seu redor; a acur\u00e1cia por \u00e9poca oscila sem tend\u00eancia a 1,0; e o erro n\u00e3o se aproxima de zero. Isso \u00e9 esperado quando h\u00e1 n\u00e3o separabilidade linear: as atualiza\u00e7\u00f5es do perceptron continuam corrigindo pontos de um lado e criando novos erros do outro, levando a flutua\u00e7\u00f5es em vez de estabiliza\u00e7\u00e3o.</p> <p>Comparado ao Exerc\u00edcio 1, este caso evidencia a limita\u00e7\u00e3o do perceptron: ele funciona muito bem quando os dados s\u00e3o separ\u00e1veis por uma reta, mas, com sobreposi\u00e7\u00e3o, tende a estagnar. Para melhorar, \u00e9 preciso mais expressividade (p. ex., MLP com n\u00e3o linearidades) ou engenharia de atributos/transforma\u00e7\u00f5es que tornem a separa\u00e7\u00e3o mais pr\u00f3xima de linear.</p>"},{"location":"exercises/02_perceptron/perceptron/#entendendo-perceptrons-e-suas-limitacoes","title":"Entendendo Perceptrons e Suas Limita\u00e7\u00f5es\u00b6","text":""},{"location":"exercises/02_perceptron/perceptron/#exercicio-1","title":"Exerc\u00edcio 1\u00b6","text":""},{"location":"exercises/02_perceptron/perceptron/#geracao-dos-dados","title":"Gera\u00e7\u00e3o dos dados\u00b6","text":"<ul> <li>Classe 0: m\u00e9dia = [1.5, 1.5], covari\u00e2ncia = [[0.5, 0], [0, 0.5]]</li> <li>Classe 1: m\u00e9dia = [5, 5], covari\u00e2ncia = [[0.5, 0], [0, 0.5]]</li> </ul> <p>Cada classe ter\u00e1 1000 amostras. Espera-se separabilidade quase linear, dado o distanciamento entre m\u00e9dias e a baixa vari\u00e2ncia.</p>"},{"location":"exercises/02_perceptron/perceptron/#exercicio-2","title":"Exerc\u00edcio 2\u00b6","text":""},{"location":"exercises/03_mlp/mlp/","title":"Entendendo Perceptrons Multicamadas (MLPs)","text":"In\u00a0[71]: Copied! <pre># Bibliotecas \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n</pre> # Bibliotecas  import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.decomposition import PCA <p>Gerar um dataset 2D com 1 cluster para a classe 0 e 2 clusters para a classe 1 (usando <code>make_classification</code> em subconjuntos), treinar um MLP do zero (NumPy apenas) com 1 camada oculta, loss bin\u00e1rio (BCE), e avaliar: perda de treino, acur\u00e1cia de teste e fronteira de decis\u00e3o.</p> In\u00a0[72]: Copied! <pre>np.random.seed(42)\n\nn0, n1 = 500, 500  # total=1000\n\n# Subconjunto s\u00f3 com classe 0 (1 cluster)\nX0, y0 = make_classification(\n    n_samples=n0, n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=1, n_classes=2, weights=[1.0, 0.0],\n    class_sep=1.5, flip_y=0.0, random_state=42\n)\n\n# Subconjunto s\u00f3 com classe 1 (2 clusters)\nX1, y1 = make_classification(\n    n_samples=n1, n_features=2, n_informative=2, n_redundant=0,\n    n_clusters_per_class=2, n_classes=2, weights=[0.0, 1.0],\n    class_sep=1.5, flip_y=0.0, random_state=43\n)\n\n# Junta e embaralha\nX = np.vstack([X0, X1])\ny = np.hstack([y0, y1])  # r\u00f3tulos {0,1}\n\nperm = np.random.permutation(len(X))\nX, y = X[perm], y[perm]\n\n# (seguran\u00e7a) se por acaso seus r\u00f3tulos estiverem em {-1,+1}, converte para {0,1}\nif set(np.unique(y)) == {-1, 1}:\n    y = ((y + 1) // 2).astype(int)\n\nprint(X.shape, y.shape, np.bincount(y))\n</pre> np.random.seed(42)  n0, n1 = 500, 500  # total=1000  # Subconjunto s\u00f3 com classe 0 (1 cluster) X0, y0 = make_classification(     n_samples=n0, n_features=2, n_informative=2, n_redundant=0,     n_clusters_per_class=1, n_classes=2, weights=[1.0, 0.0],     class_sep=1.5, flip_y=0.0, random_state=42 )  # Subconjunto s\u00f3 com classe 1 (2 clusters) X1, y1 = make_classification(     n_samples=n1, n_features=2, n_informative=2, n_redundant=0,     n_clusters_per_class=2, n_classes=2, weights=[0.0, 1.0],     class_sep=1.5, flip_y=0.0, random_state=43 )  # Junta e embaralha X = np.vstack([X0, X1]) y = np.hstack([y0, y1])  # r\u00f3tulos {0,1}  perm = np.random.permutation(len(X)) X, y = X[perm], y[perm]  # (seguran\u00e7a) se por acaso seus r\u00f3tulos estiverem em {-1,+1}, converte para {0,1} if set(np.unique(y)) == {-1, 1}:     y = ((y + 1) // 2).astype(int)  print(X.shape, y.shape, np.bincount(y))  <pre>(1000, 2) (1000,) [500 500]\n</pre> In\u00a0[73]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Opcional (recomendado): padronizar usando estat\u00edsticas do treino\nmu = X_train.mean(axis=0)\nsigma = X_train.std(axis=0) + 1e-8\nX_train = (X_train - mu) / sigma\nX_test  = (X_test  - mu) / sigma\n\nprint(\"train:\", X_train.shape, \"test:\", X_test.shape)\n</pre> X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, stratify=y, random_state=42 )  # Opcional (recomendado): padronizar usando estat\u00edsticas do treino mu = X_train.mean(axis=0) sigma = X_train.std(axis=0) + 1e-8 X_train = (X_train - mu) / sigma X_test  = (X_test  - mu) / sigma  print(\"train:\", X_train.shape, \"test:\", X_test.shape)  <pre>train: (800, 2) test: (200, 2)\n</pre> In\u00a0[74]: Copied! <pre># --------- Ativa\u00e7\u00f5es ---------\ndef tanh(z):\n    return np.tanh(z)\n\ndef dtanh(a):\n    # derivada em termos da ativa\u00e7\u00e3o a = tanh(z)\n    return 1.0 - a**2\n\ndef relu(z):\n    return np.maximum(0.0, z)\n\ndef drelu(z):\n    return (z &gt; 0.0).astype(z.dtype)\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef softmax(z):\n    # est\u00e1vel numericamente\n    z = z - z.max(axis=1, keepdims=True)\n    e = np.exp(z)\n    return e / e.sum(axis=1, keepdims=True)\n\n# --------- Perdas ---------\ndef bce_loss(y_true, y_prob, eps=1e-9):\n    # y_true shape: (N,1) com {0,1}; y_prob shape: (N,1)\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return -(y_true*np.log(y_prob) + (1-y_true)*np.log(1-y_prob)).mean()\n\ndef ce_loss(y_true_onehot, y_prob, eps=1e-9):\n    # y_true_onehot shape: (N,C); y_prob shape: (N,C)\n    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n    return -(y_true_onehot * np.log(y_prob)).sum(axis=1).mean()\n\n# --------- Utilit\u00e1rios ---------\ndef one_hot(y, num_classes=None):\n    y = y.astype(int).ravel()\n    if num_classes is None:\n        num_classes = int(y.max()) + 1\n    out = np.zeros((y.shape[0], num_classes), dtype=float)\n    out[np.arange(y.shape[0]), y] = 1.0\n    return out\n\ndef xavier_limit(fan_in, fan_out):\n    return np.sqrt(6.0 / (fan_in + fan_out))\n\ndef he_limit(fan_in):\n    # He uniform\n    return np.sqrt(6.0 / fan_in)\n</pre> # --------- Ativa\u00e7\u00f5es --------- def tanh(z):     return np.tanh(z)  def dtanh(a):     # derivada em termos da ativa\u00e7\u00e3o a = tanh(z)     return 1.0 - a**2  def relu(z):     return np.maximum(0.0, z)  def drelu(z):     return (z &gt; 0.0).astype(z.dtype)  def sigmoid(z):     return 1.0 / (1.0 + np.exp(-z))  def softmax(z):     # est\u00e1vel numericamente     z = z - z.max(axis=1, keepdims=True)     e = np.exp(z)     return e / e.sum(axis=1, keepdims=True)  # --------- Perdas --------- def bce_loss(y_true, y_prob, eps=1e-9):     # y_true shape: (N,1) com {0,1}; y_prob shape: (N,1)     y_prob = np.clip(y_prob, eps, 1.0 - eps)     return -(y_true*np.log(y_prob) + (1-y_true)*np.log(1-y_prob)).mean()  def ce_loss(y_true_onehot, y_prob, eps=1e-9):     # y_true_onehot shape: (N,C); y_prob shape: (N,C)     y_prob = np.clip(y_prob, eps, 1.0 - eps)     return -(y_true_onehot * np.log(y_prob)).sum(axis=1).mean()  # --------- Utilit\u00e1rios --------- def one_hot(y, num_classes=None):     y = y.astype(int).ravel()     if num_classes is None:         num_classes = int(y.max()) + 1     out = np.zeros((y.shape[0], num_classes), dtype=float)     out[np.arange(y.shape[0]), y] = 1.0     return out  def xavier_limit(fan_in, fan_out):     return np.sqrt(6.0 / (fan_in + fan_out))  def he_limit(fan_in):     # He uniform     return np.sqrt(6.0 / fan_in)  In\u00a0[75]: Copied! <pre>class MLP:\n    \"\"\"\n    MLP gen\u00e9rico (NumPy puro).\n    - layer_sizes: lista com dimens\u00f5es [in, h1, ..., hK, out]\n    - activations: lista de strings para cada camada oculta + sa\u00edda (len = len(layer_sizes)-1)\n        op\u00e7\u00f5es: 'tanh', 'relu', 'sigmoid', 'softmax' (use 'sigmoid' p/ bin\u00e1rio e 'softmax' p/ multiclasse)\n    - loss: 'bce' (bin\u00e1rio; requer sa\u00edda 'sigmoid') ou 'ce' (multiclasse; requer sa\u00edda 'softmax')\n    - l2: regulariza\u00e7\u00e3o L2 (lambda), default 0.0\n    - lr: taxa de aprendizado\n    - seed: reprodutibilidade\n    \"\"\"\n    def __init__(self, layer_sizes, activations, loss, lr=0.05, l2=0.0, seed=42):\n        assert len(layer_sizes) &gt;= 2, \"Precisa de pelo menos entrada e sa\u00edda\"\n        assert len(activations) == len(layer_sizes) - 1, \"Uma ativa\u00e7\u00e3o por camada\"\n        self.sizes = list(layer_sizes)\n        self.acts = list(activations)\n        self.loss_name = loss\n        self.lr = lr\n        self.l2 = float(l2)\n\n        self.rng = np.random.default_rng(seed)\n        self.params = self._init_params()\n        self.loss_hist = []\n\n        # mapear nomes para fun\u00e7\u00f5es\n        self._act = {\n            'tanh': (tanh, dtanh),\n            'relu': (relu, None),     # derivative usa Z\n            'sigmoid': (sigmoid, None), # derivative calculada via BCE no topo; n\u00e3o usada nas ocultas\n            'softmax': (softmax, None)\n        }\n\n        if loss == 'bce':\n            assert self.acts[-1] == 'sigmoid', \"BCE requer sa\u00edda 'sigmoid'\"\n        elif loss == 'ce':\n            assert self.acts[-1] == 'softmax', \"CE requer sa\u00edda 'softmax'\"\n        else:\n            raise ValueError(\"loss deve ser 'bce' ou 'ce'\")\n\n    def _init_params(self):\n        params = []\n        for l in range(len(self.sizes) - 1):\n            fan_in, fan_out = self.sizes[l], self.sizes[l+1]\n            act = self.acts[l]\n            if act in ('tanh', 'sigmoid', 'softmax'):\n                lim = xavier_limit(fan_in, fan_out)\n            elif act == 'relu':\n                lim = he_limit(fan_in)\n            else:\n                lim = xavier_limit(fan_in, fan_out)\n\n            W = self.rng.uniform(-lim, lim, size=(fan_in, fan_out))\n            b = np.zeros((1, fan_out))\n            params.append({'W': W, 'b': b})\n        return params\n\n    def _forward(self, X):\n        \"\"\"\n        Retorna A_list, Z_list:\n        A_list[0] = X\n        para l&gt;=1: Z_list[l] = A_list[l-1] @ W_l + b_l; A_list[l] = act(Z_list[l])\n        \"\"\"\n        A_list = [X]\n        Z_list = [None]\n        for l, layer in enumerate(self.params, start=1):\n            W, b = layer['W'], layer['b']\n            Z = A_list[-1] @ W + b\n            act_name = self.acts[l-1]\n            if act_name == 'tanh':\n                A = tanh(Z)\n            elif act_name == 'relu':\n                A = relu(Z)\n            elif act_name == 'sigmoid':\n                A = sigmoid(Z)\n            elif act_name == 'softmax':\n                A = softmax(Z)\n            else:\n                raise ValueError(f\"Ativa\u00e7\u00e3o desconhecida: {act_name}\")\n            Z_list.append(Z); A_list.append(A)\n        return A_list, Z_list\n\n    def _compute_loss(self, y_true, Aout):\n        if self.loss_name == 'bce':\n            loss = bce_loss(y_true.reshape(-1,1), Aout)\n        else:  # 'ce'\n            if Aout.ndim == 1 or Aout.shape[1] == 1:\n                raise ValueError(\"CE requer probabilidades (N,C) e r\u00f3tulos one-hot (N,C)\")\n            loss = ce_loss(y_true, Aout)\n\n        # L2\n        if self.l2 &gt; 0.0:\n            l2_sum = sum((layer['W']**2).sum() for layer in self.params)\n            loss = loss + 0.5 * self.l2 * l2_sum / y_true.shape[0]\n        return float(loss)\n\n    def _backward(self, A_list, Z_list, y_true):\n        grads = [None] * len(self.params)\n        N = A_list[0].shape[0]\n\n        # dZ da camada de sa\u00edda\n        Aout = A_list[-1]\n        if self.loss_name == 'bce':\n            # BCE + sigmoid =&gt; dZ = (Aout - y)/N\n            y = y_true.reshape(-1,1)\n            dZ = (Aout - y) / N\n        else:  # 'ce' + softmax =&gt; dZ = (Aout - Y)/N\n            Y = y_true\n            dZ = (Aout - Y) / N\n\n        # camadas de tr\u00e1s para frente\n        for l in reversed(range(len(self.params))):\n            A_prev = A_list[l]\n            W = self.params[l]['W']\n\n            dW = A_prev.T @ dZ\n            db = dZ.sum(axis=0, keepdims=True)\n\n            # L2\n            if self.l2 &gt; 0.0:\n                dW = dW + self.l2 * W / N\n\n            grads[l] = {'dW': dW, 'db': db}\n\n            if l &gt; 0:  # propagar para tr\u00e1s se n\u00e3o for a primeira camada\n                dA_prev = dZ @ W.T\n                act_name = self.acts[l-1]  # ativa\u00e7\u00e3o da camada l\n                if act_name == 'tanh':\n                    dZ = dA_prev * dtanh(A_list[l])\n                elif act_name == 'relu':\n                    dZ = dA_prev * drelu(Z_list[l])\n                elif act_name in ('sigmoid', 'softmax'):\n                    # n\u00e3o usamos sigmoid/softmax em ocultas neste design; se usar, trate aqui:\n                    a = A_list[l]\n                    if act_name == 'sigmoid':\n                        dZ = dA_prev * a * (1 - a)\n                    else:\n                        raise ValueError(\"Softmax em camada oculta n\u00e3o suportado\")\n                else:\n                    raise ValueError(f\"Ativa\u00e7\u00e3o desconhecida: {act_name}\")\n        return grads\n\n    def _step(self, grads):\n        for layer, g in zip(self.params, grads):\n            layer['W'] -= self.lr * g['dW']\n            layer['b'] -= self.lr * g['db']\n\n    def fit(self, X, y, epochs=300, verbose=False):\n        \"\"\"\n        y:\n          - bin\u00e1rio/BCE: array (N,) ou (N,1) com {0,1}\n          - CE: one-hot (N,C)\n        \"\"\"\n        self.loss_hist.clear()\n        for ep in range(1, epochs+1):\n            A_list, Z_list = self._forward(X)\n            loss = self._compute_loss(y, A_list[-1])\n            self.loss_hist.append(loss)\n            grads = self._backward(A_list, Z_list, y)\n            self._step(grads)\n            if verbose and (ep % 50 == 0 or ep == 1):\n                print(f\"\u00e9poca {ep:03d} | loss={loss:.4f}\")\n        return self\n\n    def predict_proba(self, X):\n        A_list, _ = self._forward(X)\n        return A_list[-1]\n\n    def predict(self, X, thr=0.5):\n        proba = self.predict_proba(X)\n        # BCE bin\u00e1rio\n        if self.loss_name == 'bce':\n            return (proba &gt;= thr).astype(int).ravel()\n        # CE multiclasse\n        return np.argmax(proba, axis=1)\n</pre> class MLP:     \"\"\"     MLP gen\u00e9rico (NumPy puro).     - layer_sizes: lista com dimens\u00f5es [in, h1, ..., hK, out]     - activations: lista de strings para cada camada oculta + sa\u00edda (len = len(layer_sizes)-1)         op\u00e7\u00f5es: 'tanh', 'relu', 'sigmoid', 'softmax' (use 'sigmoid' p/ bin\u00e1rio e 'softmax' p/ multiclasse)     - loss: 'bce' (bin\u00e1rio; requer sa\u00edda 'sigmoid') ou 'ce' (multiclasse; requer sa\u00edda 'softmax')     - l2: regulariza\u00e7\u00e3o L2 (lambda), default 0.0     - lr: taxa de aprendizado     - seed: reprodutibilidade     \"\"\"     def __init__(self, layer_sizes, activations, loss, lr=0.05, l2=0.0, seed=42):         assert len(layer_sizes) &gt;= 2, \"Precisa de pelo menos entrada e sa\u00edda\"         assert len(activations) == len(layer_sizes) - 1, \"Uma ativa\u00e7\u00e3o por camada\"         self.sizes = list(layer_sizes)         self.acts = list(activations)         self.loss_name = loss         self.lr = lr         self.l2 = float(l2)          self.rng = np.random.default_rng(seed)         self.params = self._init_params()         self.loss_hist = []          # mapear nomes para fun\u00e7\u00f5es         self._act = {             'tanh': (tanh, dtanh),             'relu': (relu, None),     # derivative usa Z             'sigmoid': (sigmoid, None), # derivative calculada via BCE no topo; n\u00e3o usada nas ocultas             'softmax': (softmax, None)         }          if loss == 'bce':             assert self.acts[-1] == 'sigmoid', \"BCE requer sa\u00edda 'sigmoid'\"         elif loss == 'ce':             assert self.acts[-1] == 'softmax', \"CE requer sa\u00edda 'softmax'\"         else:             raise ValueError(\"loss deve ser 'bce' ou 'ce'\")      def _init_params(self):         params = []         for l in range(len(self.sizes) - 1):             fan_in, fan_out = self.sizes[l], self.sizes[l+1]             act = self.acts[l]             if act in ('tanh', 'sigmoid', 'softmax'):                 lim = xavier_limit(fan_in, fan_out)             elif act == 'relu':                 lim = he_limit(fan_in)             else:                 lim = xavier_limit(fan_in, fan_out)              W = self.rng.uniform(-lim, lim, size=(fan_in, fan_out))             b = np.zeros((1, fan_out))             params.append({'W': W, 'b': b})         return params      def _forward(self, X):         \"\"\"         Retorna A_list, Z_list:         A_list[0] = X         para l&gt;=1: Z_list[l] = A_list[l-1] @ W_l + b_l; A_list[l] = act(Z_list[l])         \"\"\"         A_list = [X]         Z_list = [None]         for l, layer in enumerate(self.params, start=1):             W, b = layer['W'], layer['b']             Z = A_list[-1] @ W + b             act_name = self.acts[l-1]             if act_name == 'tanh':                 A = tanh(Z)             elif act_name == 'relu':                 A = relu(Z)             elif act_name == 'sigmoid':                 A = sigmoid(Z)             elif act_name == 'softmax':                 A = softmax(Z)             else:                 raise ValueError(f\"Ativa\u00e7\u00e3o desconhecida: {act_name}\")             Z_list.append(Z); A_list.append(A)         return A_list, Z_list      def _compute_loss(self, y_true, Aout):         if self.loss_name == 'bce':             loss = bce_loss(y_true.reshape(-1,1), Aout)         else:  # 'ce'             if Aout.ndim == 1 or Aout.shape[1] == 1:                 raise ValueError(\"CE requer probabilidades (N,C) e r\u00f3tulos one-hot (N,C)\")             loss = ce_loss(y_true, Aout)          # L2         if self.l2 &gt; 0.0:             l2_sum = sum((layer['W']**2).sum() for layer in self.params)             loss = loss + 0.5 * self.l2 * l2_sum / y_true.shape[0]         return float(loss)      def _backward(self, A_list, Z_list, y_true):         grads = [None] * len(self.params)         N = A_list[0].shape[0]          # dZ da camada de sa\u00edda         Aout = A_list[-1]         if self.loss_name == 'bce':             # BCE + sigmoid =&gt; dZ = (Aout - y)/N             y = y_true.reshape(-1,1)             dZ = (Aout - y) / N         else:  # 'ce' + softmax =&gt; dZ = (Aout - Y)/N             Y = y_true             dZ = (Aout - Y) / N          # camadas de tr\u00e1s para frente         for l in reversed(range(len(self.params))):             A_prev = A_list[l]             W = self.params[l]['W']              dW = A_prev.T @ dZ             db = dZ.sum(axis=0, keepdims=True)              # L2             if self.l2 &gt; 0.0:                 dW = dW + self.l2 * W / N              grads[l] = {'dW': dW, 'db': db}              if l &gt; 0:  # propagar para tr\u00e1s se n\u00e3o for a primeira camada                 dA_prev = dZ @ W.T                 act_name = self.acts[l-1]  # ativa\u00e7\u00e3o da camada l                 if act_name == 'tanh':                     dZ = dA_prev * dtanh(A_list[l])                 elif act_name == 'relu':                     dZ = dA_prev * drelu(Z_list[l])                 elif act_name in ('sigmoid', 'softmax'):                     # n\u00e3o usamos sigmoid/softmax em ocultas neste design; se usar, trate aqui:                     a = A_list[l]                     if act_name == 'sigmoid':                         dZ = dA_prev * a * (1 - a)                     else:                         raise ValueError(\"Softmax em camada oculta n\u00e3o suportado\")                 else:                     raise ValueError(f\"Ativa\u00e7\u00e3o desconhecida: {act_name}\")         return grads      def _step(self, grads):         for layer, g in zip(self.params, grads):             layer['W'] -= self.lr * g['dW']             layer['b'] -= self.lr * g['db']      def fit(self, X, y, epochs=300, verbose=False):         \"\"\"         y:           - bin\u00e1rio/BCE: array (N,) ou (N,1) com {0,1}           - CE: one-hot (N,C)         \"\"\"         self.loss_hist.clear()         for ep in range(1, epochs+1):             A_list, Z_list = self._forward(X)             loss = self._compute_loss(y, A_list[-1])             self.loss_hist.append(loss)             grads = self._backward(A_list, Z_list, y)             self._step(grads)             if verbose and (ep % 50 == 0 or ep == 1):                 print(f\"\u00e9poca {ep:03d} | loss={loss:.4f}\")         return self      def predict_proba(self, X):         A_list, _ = self._forward(X)         return A_list[-1]      def predict(self, X, thr=0.5):         proba = self.predict_proba(X)         # BCE bin\u00e1rio         if self.loss_name == 'bce':             return (proba &gt;= thr).astype(int).ravel()         # CE multiclasse         return np.argmax(proba, axis=1)  In\u00a0[76]: Copied! <pre># Exemplo bin\u00e1rio: 2 -&gt; 8 -&gt; 1, tanh + sigmoid, BCE\nmlp_bin = MLP(layer_sizes=[2, 8, 1],\n              activations=['tanh', 'sigmoid'],\n              loss='bce',\n              lr=0.05, l2=0.0, seed=42)\n\nmlp_bin.fit(X_train, y_train, epochs=300, verbose=True)\ny_pred = mlp_bin.predict(X_test)\nacc = (y_pred == y_test).mean()\nprint(f\"Acur\u00e1cia (bin\u00e1rio): {acc:.3f}\")\n</pre> # Exemplo bin\u00e1rio: 2 -&gt; 8 -&gt; 1, tanh + sigmoid, BCE mlp_bin = MLP(layer_sizes=[2, 8, 1],               activations=['tanh', 'sigmoid'],               loss='bce',               lr=0.05, l2=0.0, seed=42)  mlp_bin.fit(X_train, y_train, epochs=300, verbose=True) y_pred = mlp_bin.predict(X_test) acc = (y_pred == y_test).mean() print(f\"Acur\u00e1cia (bin\u00e1rio): {acc:.3f}\") <pre>\u00e9poca 001 | loss=0.6169\n\u00e9poca 050 | loss=0.5573\n\u00e9poca 100 | loss=0.5391\n\u00e9poca 150 | loss=0.5321\n\u00e9poca 200 | loss=0.5283\n\u00e9poca 250 | loss=0.5257\n\u00e9poca 300 | loss=0.5235\nAcur\u00e1cia (bin\u00e1rio): 0.620\n</pre> In\u00a0[77]: Copied! <pre>def plot_decision_boundary_model(model, X, y, title=\"Fronteira de decis\u00e3o\"):\n    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                         np.linspace(y_min, y_max, 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    proba = model.predict_proba(grid)\n    if proba.ndim == 1 or proba.shape[1] == 1:\n        zz = proba.reshape(xx.shape)\n    else:\n        zz = proba.max(axis=1).reshape(xx.shape)\n\n    plt.figure(figsize=(6,5))\n    cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.4)\n    plt.colorbar(cs)\n    # pontos\n    if y.ndim == 2 and y.shape[1] &gt; 1:\n        y_plot = np.argmax(y, axis=1)\n    else:\n        y_plot = y.ravel()\n    for cls in np.unique(y_plot):\n        pts = X[y_plot==cls]\n        plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")\n    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n    plt.show()\n</pre> def plot_decision_boundary_model(model, X, y, title=\"Fronteira de decis\u00e3o\"):     x_min, x_max = X[:,0].min()-1, X[:,0].max()+1     y_min, y_max = X[:,1].min()-1, X[:,1].max()+1     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),                          np.linspace(y_min, y_max, 300))     grid = np.c_[xx.ravel(), yy.ravel()]     proba = model.predict_proba(grid)     if proba.ndim == 1 or proba.shape[1] == 1:         zz = proba.reshape(xx.shape)     else:         zz = proba.max(axis=1).reshape(xx.shape)      plt.figure(figsize=(6,5))     cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.4)     plt.colorbar(cs)     # pontos     if y.ndim == 2 and y.shape[1] &gt; 1:         y_plot = np.argmax(y, axis=1)     else:         y_plot = y.ravel()     for cls in np.unique(y_plot):         pts = X[y_plot==cls]         plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")     plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()     plt.show()  In\u00a0[78]: Copied! <pre>plot_decision_boundary_model(mlp_bin, X_train, y_train, title=\"Fronteira de decis\u00e3o (treino)\")\n</pre> plot_decision_boundary_model(mlp_bin, X_train, y_train, title=\"Fronteira de decis\u00e3o (treino)\") In\u00a0[79]: Copied! <pre>plt.figure(figsize=(6,4))\nplt.plot(range(1, len(mlp_bin.loss_hist)+1), mlp_bin.loss_hist, marker=\"o\", ms=3)\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss (treino)\")\nplt.title(\"MLP \u2014 curva de perda (BCE)\")\nplt.grid(True)\nplt.show()\n</pre> plt.figure(figsize=(6,4)) plt.plot(range(1, len(mlp_bin.loss_hist)+1), mlp_bin.loss_hist, marker=\"o\", ms=3) plt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Loss (treino)\") plt.title(\"MLP \u2014 curva de perda (BCE)\") plt.grid(True) plt.show()  In\u00a0[80]: Copied! <pre>def confusion_matrix_bin(y_true, y_pred):\n    tn = np.sum((y_true==0) &amp; (y_pred==0))\n    fp = np.sum((y_true==0) &amp; (y_pred==1))\n    fn = np.sum((y_true==1) &amp; (y_pred==0))\n    tp = np.sum((y_true==1) &amp; (y_pred==1))\n    return np.array([[tn, fp],\n                     [fn, tp]])\n\ny_pred_tr = mlp_bin.predict(X_train)\ny_pred_te = mlp_bin.predict(X_test)\n\nacc_tr = (y_pred_tr == y_train).mean()\nacc_te = (y_pred_te == y_test).mean()\ncm_te  = confusion_matrix_bin(y_test, y_pred_te)\n\nprint(f\"Acur\u00e1cia (treino): {acc_tr:.3f}\")\nprint(f\"Acur\u00e1cia (teste) : {acc_te:.3f}\")\nprint(\"Matriz de confus\u00e3o (teste) [[TN, FP],[FN, TP]]:\\n\", cm_te)\n</pre> def confusion_matrix_bin(y_true, y_pred):     tn = np.sum((y_true==0) &amp; (y_pred==0))     fp = np.sum((y_true==0) &amp; (y_pred==1))     fn = np.sum((y_true==1) &amp; (y_pred==0))     tp = np.sum((y_true==1) &amp; (y_pred==1))     return np.array([[tn, fp],                      [fn, tp]])  y_pred_tr = mlp_bin.predict(X_train) y_pred_te = mlp_bin.predict(X_test)  acc_tr = (y_pred_tr == y_train).mean() acc_te = (y_pred_te == y_test).mean() cm_te  = confusion_matrix_bin(y_test, y_pred_te)  print(f\"Acur\u00e1cia (treino): {acc_tr:.3f}\") print(f\"Acur\u00e1cia (teste) : {acc_te:.3f}\") print(\"Matriz de confus\u00e3o (teste) [[TN, FP],[FN, TP]]:\\n\", cm_te)  <pre>Acur\u00e1cia (treino): 0.676\nAcur\u00e1cia (teste) : 0.620\nMatriz de confus\u00e3o (teste) [[TN, FP],[FN, TP]]:\n [[74 26]\n [50 50]]\n</pre> In\u00a0[81]: Copied! <pre>def plot_boundary_with_errors(model, X, y, title=\"Fronteira (teste) + erros\"):\n    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n                         np.linspace(y_min, y_max, 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    zz = model.predict_proba(grid).reshape(xx.shape)\n\n    plt.figure(figsize=(6,5))\n    cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.35)\n    plt.colorbar(cs)\n\n    y_hat = model.predict(X)\n    err = y_hat != y\n\n    for cls in np.unique(y):\n        pts = X[(y==cls) &amp; (~err)]\n        plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")\n\n    if err.any():\n        plt.scatter(X[err,0], X[err,1], s=40, facecolors='none', edgecolors='r', linewidths=1.2, label=\"erros\")\n\n    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n    plt.show()\n\nplot_boundary_with_errors(mlp_bin, X_test, y_test, title=\"Fronteira de decis\u00e3o (teste) + erros\")\n</pre> def plot_boundary_with_errors(model, X, y, title=\"Fronteira (teste) + erros\"):     x_min, x_max = X[:,0].min()-1, X[:,0].max()+1     y_min, y_max = X[:,1].min()-1, X[:,1].max()+1     xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),                          np.linspace(y_min, y_max, 300))     grid = np.c_[xx.ravel(), yy.ravel()]     zz = model.predict_proba(grid).reshape(xx.shape)      plt.figure(figsize=(6,5))     cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.35)     plt.colorbar(cs)      y_hat = model.predict(X)     err = y_hat != y      for cls in np.unique(y):         pts = X[(y==cls) &amp; (~err)]         plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")      if err.any():         plt.scatter(X[err,0], X[err,1], s=40, facecolors='none', edgecolors='r', linewidths=1.2, label=\"erros\")      plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()     plt.show()  plot_boundary_with_errors(mlp_bin, X_test, y_test, title=\"Fronteira de decis\u00e3o (teste) + erros\")  <p>O MLP de 1 camada oculta aprendeu uma fronteira n\u00e3o linear que separa razoavelmente o \u00fanico cluster da classe 0 dos dois clusters da classe 1. A perda caiu de ~0.62 para ~0.52 e a acur\u00e1cia de teste ficou ~0.62, compat\u00edvel com a sobreposi\u00e7\u00e3o vis\u00edvel entre as classes: h\u00e1 regi\u00f5es onde ambos os r\u00f3tulos s\u00e3o plaus\u00edveis, e uma fronteira suave (tanh + sigmoid) n\u00e3o resolve todas. Ainda assim, o modelo captura a estrutura multimodal, melhor que um classificador linear. Ganhos adicionais viriam de mais capacidade (mais neur\u00f4nios/camadas), regulariza\u00e7\u00e3o e ajuste de taxa de aprendizado/\u00e9pocas.</p> In\u00a0[82]: Copied! <pre>rng = np.random.default_rng(42)\n\n# 1500 = 500 por classe\nn0 = n1 = n2 = 500\nF = 4\n\n# Classe 0: 2 clusters\nX0, y0 = make_classification(\n    n_samples=n0, n_features=F, n_informative=F, n_redundant=0,\n    n_classes=2, n_clusters_per_class=2, weights=[1.0, 0.0],\n    class_sep=1.6, flip_y=0.0, random_state=10\n)\ny0[:] = 0\n\n# Classe 1: 3 clusters\nX1, y1 = make_classification(\n    n_samples=n1, n_features=F, n_informative=F, n_redundant=0,\n    n_classes=2, n_clusters_per_class=3, weights=[0.0, 1.0],\n    class_sep=1.5, flip_y=0.0, random_state=11\n)\ny1[:] = 1\n\n# Classe 2: 4 clusters\nX2, y2 = make_classification(\n    n_samples=n2, n_features=F, n_informative=F, n_redundant=0,\n    n_classes=2, n_clusters_per_class=4, weights=[0.0, 1.0],\n    class_sep=1.4, flip_y=0.0, random_state=12\n)\ny2[:] = 2\n\nX = np.vstack([X0, X1, X2])\ny = np.concatenate([y0, y1, y2]).astype(int)\n\nperm = rng.permutation(len(X))\nX, y = X[perm], y[perm]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# padroniza\u00e7\u00e3o por estat\u00edstica do treino\nmu, sig = X_train.mean(axis=0), X_train.std(axis=0) + 1e-8\nX_train = (X_train - mu) / sig\nX_test  = (X_test  - mu) / sig\n</pre> rng = np.random.default_rng(42)  # 1500 = 500 por classe n0 = n1 = n2 = 500 F = 4  # Classe 0: 2 clusters X0, y0 = make_classification(     n_samples=n0, n_features=F, n_informative=F, n_redundant=0,     n_classes=2, n_clusters_per_class=2, weights=[1.0, 0.0],     class_sep=1.6, flip_y=0.0, random_state=10 ) y0[:] = 0  # Classe 1: 3 clusters X1, y1 = make_classification(     n_samples=n1, n_features=F, n_informative=F, n_redundant=0,     n_classes=2, n_clusters_per_class=3, weights=[0.0, 1.0],     class_sep=1.5, flip_y=0.0, random_state=11 ) y1[:] = 1  # Classe 2: 4 clusters X2, y2 = make_classification(     n_samples=n2, n_features=F, n_informative=F, n_redundant=0,     n_classes=2, n_clusters_per_class=4, weights=[0.0, 1.0],     class_sep=1.4, flip_y=0.0, random_state=12 ) y2[:] = 2  X = np.vstack([X0, X1, X2]) y = np.concatenate([y0, y1, y2]).astype(int)  perm = rng.permutation(len(X)) X, y = X[perm], y[perm]  X_train, X_test, y_train, y_test = train_test_split(     X, y, test_size=0.2, stratify=y, random_state=42 )  # padroniza\u00e7\u00e3o por estat\u00edstica do treino mu, sig = X_train.mean(axis=0), X_train.std(axis=0) + 1e-8 X_train = (X_train - mu) / sig X_test  = (X_test  - mu) / sig  In\u00a0[83]: Copied! <pre># Sa\u00edda com 3 neur\u00f4nios, tanh nas ocultas, softmax na sa\u00edda, CE\nmlp_mc = MLP(layer_sizes=[X_train.shape[1], 16, 3],\n             activations=['tanh', 'softmax'],\n             loss='ce', lr=0.05, l2=0.0, seed=42)\n\n# r\u00f3tulos one-hot para CE\ndef one_hot(y, C=None):\n    C = int(y.max())+1 if C is None else C\n    Y = np.zeros((y.size, C)); Y[np.arange(y.size), y] = 1.0\n    return Y\n\nY_train = one_hot(y_train, 3)\nY_test  = one_hot(y_test, 3)\n\nmlp_mc.fit(X_train, Y_train, epochs=400, verbose=True)\n\ny_pred = mlp_mc.predict(X_test)          # argmax\nacc = (y_pred == y_test).mean()\nprint(f\"Acur\u00e1cia (teste, 3 classes): {acc:.3f}\")\n</pre> # Sa\u00edda com 3 neur\u00f4nios, tanh nas ocultas, softmax na sa\u00edda, CE mlp_mc = MLP(layer_sizes=[X_train.shape[1], 16, 3],              activations=['tanh', 'softmax'],              loss='ce', lr=0.05, l2=0.0, seed=42)  # r\u00f3tulos one-hot para CE def one_hot(y, C=None):     C = int(y.max())+1 if C is None else C     Y = np.zeros((y.size, C)); Y[np.arange(y.size), y] = 1.0     return Y  Y_train = one_hot(y_train, 3) Y_test  = one_hot(y_test, 3)  mlp_mc.fit(X_train, Y_train, epochs=400, verbose=True)  y_pred = mlp_mc.predict(X_test)          # argmax acc = (y_pred == y_test).mean() print(f\"Acur\u00e1cia (teste, 3 classes): {acc:.3f}\")  <pre>\u00e9poca 001 | loss=1.3406\n\u00e9poca 050 | loss=0.9813\n\u00e9poca 100 | loss=0.8792\n\u00e9poca 150 | loss=0.8220\n\u00e9poca 200 | loss=0.7852\n\u00e9poca 250 | loss=0.7567\n\u00e9poca 300 | loss=0.7311\n\u00e9poca 350 | loss=0.7065\n\u00e9poca 400 | loss=0.6825\nAcur\u00e1cia (teste, 3 classes): 0.723\n</pre> In\u00a0[84]: Copied! <pre>plt.plot(mlp_mc.loss_hist); plt.xlabel(\"\u00e9poca\"); plt.ylabel(\"loss (CE)\")\nplt.title(\"Curva de treino \u2014 CE\"); plt.show()\n\nZ = PCA(n_components=2, random_state=0).fit_transform(X_test)\ny_hat = mlp_mc.predict(X_test)\nfor c in np.unique(y_test):\n    pts = Z[y_test==c]\n    plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\")\nplt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\")\nplt.legend(); plt.title(\"Teste (PCA 2D): r\u00f3tulo real vs. predito\"); plt.show()\n</pre> plt.plot(mlp_mc.loss_hist); plt.xlabel(\"\u00e9poca\"); plt.ylabel(\"loss (CE)\") plt.title(\"Curva de treino \u2014 CE\"); plt.show()  Z = PCA(n_components=2, random_state=0).fit_transform(X_test) y_hat = mlp_mc.predict(X_test) for c in np.unique(y_test):     pts = Z[y_test==c]     plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\") plt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\") plt.legend(); plt.title(\"Teste (PCA 2D): r\u00f3tulo real vs. predito\"); plt.show()  <p>O modelo com uma \u00fanica camada oculta conseguiu reduzir a perda de 1.34 \u2192 0.68 ao longo de 400 \u00e9pocas, mostrando aprendizado consistente. A curva de treino apresenta decaimento suave, mas ainda relativamente lento, indicando que a capacidade de representa\u00e7\u00e3o \u00e9 limitada.</p> <p>No teste, a acur\u00e1cia atingiu 72,3% em um problema de 3 classes. O gr\u00e1fico de PCA revela sobreposi\u00e7\u00e3o consider\u00e1vel entre os r\u00f3tulos verdadeiros e preditos, evidenciando dificuldade em separar regi\u00f5es mais confusas do espa\u00e7o. Esse resultado reflete a limita\u00e7\u00e3o de um MLP raso: apesar de capturar padr\u00f5es n\u00e3o lineares, a fronteira de decis\u00e3o ainda n\u00e3o \u00e9 suficientemente expressiva para classes com forte sobreposi\u00e7\u00e3o.</p> In\u00a0[85]: Copied! <pre># 4 -&gt; 32 -&gt; 16 -&gt; 3 (tanh nas ocultas, softmax na sa\u00edda)\nmlp_deep = MLP(layer_sizes=[X_train.shape[1], 32, 16, 3],\n               activations=['tanh', 'tanh', 'softmax'],\n               loss='ce', lr=0.05, l2=1e-4, seed=42)\n\nY_train = one_hot(y_train, 3)\nY_test  = one_hot(y_test, 3)\n\nmlp_deep.fit(X_train, Y_train, epochs=500, verbose=True)\ny_pred = mlp_deep.predict(X_test)\nacc = (y_pred == y_test).mean()\nprint(f\"Acur\u00e1cia (teste, 2 ocultas): {acc:.3f}\")\n</pre> # 4 -&gt; 32 -&gt; 16 -&gt; 3 (tanh nas ocultas, softmax na sa\u00edda) mlp_deep = MLP(layer_sizes=[X_train.shape[1], 32, 16, 3],                activations=['tanh', 'tanh', 'softmax'],                loss='ce', lr=0.05, l2=1e-4, seed=42)  Y_train = one_hot(y_train, 3) Y_test  = one_hot(y_test, 3)  mlp_deep.fit(X_train, Y_train, epochs=500, verbose=True) y_pred = mlp_deep.predict(X_test) acc = (y_pred == y_test).mean() print(f\"Acur\u00e1cia (teste, 2 ocultas): {acc:.3f}\")  <pre>\u00e9poca 001 | loss=1.2982\n\u00e9poca 050 | loss=0.9095\n\u00e9poca 100 | loss=0.7993\n\u00e9poca 150 | loss=0.7358\n\u00e9poca 200 | loss=0.6819\n</pre> <pre>\u00e9poca 250 | loss=0.6312\n\u00e9poca 300 | loss=0.5859\n\u00e9poca 350 | loss=0.5481\n\u00e9poca 400 | loss=0.5174\n\u00e9poca 450 | loss=0.4928\n\u00e9poca 500 | loss=0.4728\nAcur\u00e1cia (teste, 2 ocultas): 0.807\n</pre> In\u00a0[86]: Copied! <pre>plt.plot(mlp_deep.loss_hist); plt.xlabel(\"\u00e9poca\"); plt.ylabel(\"loss (CE)\")\nplt.title(\"Curva de treino \u2014 MLP profundo\"); plt.show()\n\nZ = PCA(n_components=2, random_state=0).fit_transform(X_test)\ny_hat = mlp_mc.predict(X_test)\nfor c in np.unique(y_test):\n    pts = Z[y_test==c]\n    plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\")\nplt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\")\nplt.legend(); plt.title(\"Teste (PCA 2D): r\u00f3tulo real vs. predito\"); plt.show()\n</pre> plt.plot(mlp_deep.loss_hist); plt.xlabel(\"\u00e9poca\"); plt.ylabel(\"loss (CE)\") plt.title(\"Curva de treino \u2014 MLP profundo\"); plt.show()  Z = PCA(n_components=2, random_state=0).fit_transform(X_test) y_hat = mlp_mc.predict(X_test) for c in np.unique(y_test):     pts = Z[y_test==c]     plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\") plt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\") plt.legend(); plt.title(\"Teste (PCA 2D): r\u00f3tulo real vs. predito\"); plt.show()  <p>Com duas camadas ocultas, a perda caiu de 1.29 \u2192 0.47 em 500 \u00e9pocas, num ritmo mais acelerado e cont\u00ednuo que no caso anterior. A curva de treino mostra melhora clara na capacidade de ajuste.</p> <p>No teste, a acur\u00e1cia subiu para 80,7%, superando o modelo raso. A proje\u00e7\u00e3o em PCA tamb\u00e9m evidencia maior alinhamento entre r\u00f3tulos reais e predi\u00e7\u00f5es, embora ainda haja sobreposi\u00e7\u00e3o entre classes vizinhas. Esse resultado confirma o ganho de expressividade ao aumentar a profundidade: o MLP profundo consegue capturar fronteiras de decis\u00e3o mais complexas e se adapta melhor ao problema.</p>"},{"location":"exercises/03_mlp/mlp/#entendendo-perceptrons-multicamadas-mlps","title":"Entendendo Perceptrons Multicamadas (MLPs)\u00b6","text":""},{"location":"exercises/03_mlp/mlp/#exercicio-1","title":"Exerc\u00edcio 1\u00b6","text":"<p>Dados do problema.</p> <p>Entrada:</p> <p>$$x=[0.5,\\,-0.2]$$</p> <p>Alvo:</p> <p>$$y=1.0$$</p> <p>Camada oculta (2 neur\u00f4nios, tanh):</p> <p>$$ W^{(1)}=\\begin{bmatrix}0.3&amp;-0.1\\\\ 0.2&amp;0.4\\end{bmatrix},\\quad b^{(1)}=\\begin{bmatrix}0.1\\\\-0.2\\end{bmatrix} $$</p> <p>Sa\u00edda (1 neur\u00f4nio, tanh):</p> <p>$$ W^{(2)}=\\begin{bmatrix}0.5&amp;-0.3\\end{bmatrix},\\quad b^{(2)}=0.2 $$</p>"},{"location":"exercises/03_mlp/mlp/#1-forward-pass","title":"1) Forward pass\u00b6","text":"<p>Pr\u00e9-ativa\u00e7\u00f5es na oculta</p> <p>$$ z^{(1)}=W^{(1)}x+b^{(1)} $$</p> <p>$$ \\begin{aligned} z^{(1)}_1&amp;=0.3\\cdot0.5+(-0.1)\\cdot(-0.2)+0.1=0.27 \\\\ z^{(1)}_2&amp;=0.2\\cdot0.5+0.4\\cdot(-0.2)-0.2=-0.18 \\end{aligned} \\Longrightarrow\\quad z^{(1)}=\\begin{bmatrix}0.270000\\\\-0.180000\\end{bmatrix} $$</p> <p>Ativa\u00e7\u00f5es na oculta</p> <p>$$ a^{(1)}=\\tanh(z^{(1)}) $$</p> <p>$$ a^{(1)}=\\begin{bmatrix}\\tanh(0.27)\\\\ \\tanh(-0.18)\\end{bmatrix} =\\begin{bmatrix}0.263625\\\\-0.178081\\end{bmatrix} $$</p> <p>Pr\u00e9-ativa\u00e7\u00e3o na sa\u00edda</p> <p>$$ z^{(2)}=W^{(2)}a^{(1)}+b^{(2)} $$</p> <p>$$ z^{(2)}=0.5\\cdot0.263625+(-0.3)\\cdot(-0.178081)+0.2 =0.385237 $$</p> <p>Sa\u00edda</p> <p>$$ \\hat y=\\tanh(z^{(2)})=\\tanh(0.385237)=\\mathbf{0.367247} $$</p>"},{"location":"exercises/03_mlp/mlp/#2-loss","title":"2) Loss\u00b6","text":"<p>$$ L=(y-\\hat y)^2=(1-0.367247)^2=\\mathbf{0.400377} $$</p>"},{"location":"exercises/03_mlp/mlp/#3-backward-pass","title":"3) Backward pass\u00b6","text":"<p>Derivada da perda em rela\u00e7\u00e3o \u00e0 sa\u00edda:</p> <p>$$ \\frac{\\partial L}{\\partial \\hat y}=2(\\hat y-y)=2(0.367247-1)=-1.265507 $$</p> <p>Derivada da tanh:</p> <p>$$ \\frac{d}{dz}\\tanh(z)=1-\\tanh^2(z)  $$</p> <p>Logo:</p> <p>$$ \\frac{\\partial L}{\\partial z^{(2)}}=\\frac{\\partial L}{\\partial \\hat y}\\,(1-\\hat y^2) =-1.265507\\cdot(1-0.367247^2)=\\mathbf{-1.094828} $$</p> <p>Gradientes da camada de sa\u00edda</p> <p>$$ \\frac{\\partial L}{\\partial W^{(2)}}=\\frac{\\partial L}{\\partial z^{(2)}}\\,a^{(1)} =\\begin{bmatrix}-0.288624 &amp; 0.194968\\end{bmatrix},\\quad \\frac{\\partial L}{\\partial b^{(2)}}=\\mathbf{-1.094828} $$</p> <p>Propaga\u00e7\u00e3o para a oculta</p> <p>$$ \\frac{\\partial L}{\\partial a^{(1)}}=\\frac{\\partial L}{\\partial z^{(2)}}\\,W^{(2)} =\\begin{bmatrix}-0.547414\\\\ 0.328448\\end{bmatrix},\\qquad 1-(a^{(1)})^2=\\begin{bmatrix}0.930502\\\\ 0.968287\\end{bmatrix} $$</p> <p>$$ \\frac{\\partial L}{\\partial z^{(1)}}=\\frac{\\partial L}{\\partial a^{(1)}}\\odot\\bigl(1-(a^{(1)})^2\\bigr) =\\begin{bmatrix}-0.509370\\\\ 0.318032\\end{bmatrix} $$</p> <p>Gradientes da camada oculta</p> <p>$$ \\frac{\\partial L}{\\partial W^{(1)}} =\\begin{bmatrix} -0.254685 &amp; 0.101874\\\\ \\phantom{-}0.159016 &amp; -0.063606 \\end{bmatrix},\\quad \\frac{\\partial L}{\\partial b^{(1)}}=\\begin{bmatrix}-0.509370\\\\ 0.318032\\end{bmatrix} $$</p>"},{"location":"exercises/03_mlp/mlp/#4-atualizacao-de-parametros","title":"4) Atualiza\u00e7\u00e3o de par\u00e2metros\u00b6","text":"<p>$$\\eta=\\mathbf{0.1}$$ $$ \\begin{aligned} \\\\ W^{(2)}_{\\text{novo}}&amp;=W^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial W^{(2)}}  =\\begin{bmatrix}0.528862 &amp; -0.319497\\end{bmatrix} \\\\ b^{(2)}_{\\text{novo}}&amp;=b^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(2)}} =\\mathbf{0.309483} \\\\ W^{(1)}_{\\text{novo}}&amp;=W^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial W^{(1)}} \\\\ &amp;=\\begin{bmatrix} 0.325468 &amp; -0.110187\\\\ 0.184098 &amp; \\phantom{-}0.406361 \\end{bmatrix}\\\\[2pt] b^{(1)}_{\\text{novo}}&amp;=b^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(1)}} =\\begin{bmatrix}\\phantom{-}0.150937\\\\ -0.231803\\end{bmatrix} \\end{aligned} $$</p>"},{"location":"exercises/03_mlp/mlp/#exercicio-2-mlp-from-scratch-classificacao-binaria-em-2d","title":"Exerc\u00edcio 2 \u2014 MLP from scratch (classifica\u00e7\u00e3o bin\u00e1ria em 2D)\u00b6","text":""},{"location":"exercises/03_mlp/mlp/#exercicio-3-mlp-multiclasse-3-classes-4-features","title":"Exerc\u00edcio 3 \u2014 MLP multiclasse (3 classes, 4 features)\u00b6","text":"<p>Gerar 1500 amostras, 3 classes, 4 features; 2 clusters para a classe 0, 3 para a classe 1 e 4 para a classe 2 (combinando subconjuntos). Treinar a mesma MLP do Ex. 2 (c\u00f3digo id\u00eantico), trocando apenas sa\u00edda e fun\u00e7\u00e3o de perda.</p>"},{"location":"exercises/03_mlp/mlp/#exercicio-4-mlp-mais-profundo-2-camadas-ocultas","title":"Exerc\u00edcio 4 \u2014 MLP mais profundo (\u22652 camadas ocultas)\u00b6","text":""},{"location":"exercises/04_vae/vae/","title":"Variational Autoencoders (VAE)","text":"In\u00a0[7]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchvision import datasets, transforms\n\n# Configurar device (GPU se dispon\u00edvel)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Usando device: {device}\")\nif device.type == 'cuda':\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n# Seeds para reprodutibilidade\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\nplt.rcParams[\"figure.figsize\"] = (8, 6)\n</pre> import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.manifold import TSNE  import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader, TensorDataset from torchvision import datasets, transforms  # Configurar device (GPU se dispon\u00edvel) device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print(f\"Usando device: {device}\") if device.type == 'cuda':     print(f\"GPU: {torch.cuda.get_device_name(0)}\")  # Seeds para reprodutibilidade np.random.seed(42) torch.manual_seed(42) if torch.cuda.is_available():     torch.cuda.manual_seed(42)  plt.rcParams[\"figure.figsize\"] = (8, 6) <pre>Usando device: cuda\nGPU: NVIDIA GeForce RTX 3060 Laptop GPU\n</pre> <p>Vamos carregar o dataset MNIST, que cont\u00e9m 70.000 imagens de d\u00edgitos manuscritos (0-9) em escala de cinza (28\u00d728 pixels). Os dados ser\u00e3o normalizados para o intervalo [0, 1] e divididos em conjuntos de treino e valida\u00e7\u00e3o.</p> In\u00a0[8]: Copied! <pre># Carregar MNIST usando torchvision\nprint(\"Carregando MNIST...\")\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Converte para tensor e normaliza para [0, 1]\n])\n\n# Download e carregamento\ntrain_dataset = datasets.MNIST(root='~/.cache/mnist', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='~/.cache/mnist', train=False, download=True, transform=transform)\n\n# Combinar train e test\nX_train_torch = train_dataset.data.float() / 255.0  # [60000, 28, 28]\ny_train_torch = train_dataset.targets\nX_test_torch = test_dataset.data.float() / 255.0    # [10000, 28, 28]\ny_test_torch = test_dataset.targets\n\n# Flatten imagens: 28x28 -&gt; 784\nX_train_flat = X_train_torch.reshape(-1, 784)\nX_test_flat = X_test_torch.reshape(-1, 784)\n\nX_all = torch.cat([X_train_flat, X_test_flat], dim=0)\ny_all = torch.cat([y_train_torch, y_test_torch], dim=0)\n\n# Converter para numpy para split estratificado\nX_np = X_all.numpy()\ny_np = y_all.numpy()\n\n# Split treino/valida\u00e7\u00e3o (90/10)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_np, y_np, test_size=0.1, random_state=42, stratify=y_np\n)\n\nprint(f\"\\n\u2713 Treino: {X_train.shape}, Valida\u00e7\u00e3o: {X_val.shape}\")\nprint(f\"\u2713 Intervalo dos dados: [{X_train.min():.2f}, {X_train.max():.2f}]\")\nprint(f\"\u2713 Classes \u00fanicas: {sorted(np.unique(y_train).tolist())}\")\n</pre> # Carregar MNIST usando torchvision print(\"Carregando MNIST...\")  transform = transforms.Compose([     transforms.ToTensor(),  # Converte para tensor e normaliza para [0, 1] ])  # Download e carregamento train_dataset = datasets.MNIST(root='~/.cache/mnist', train=True, download=True, transform=transform) test_dataset = datasets.MNIST(root='~/.cache/mnist', train=False, download=True, transform=transform)  # Combinar train e test X_train_torch = train_dataset.data.float() / 255.0  # [60000, 28, 28] y_train_torch = train_dataset.targets X_test_torch = test_dataset.data.float() / 255.0    # [10000, 28, 28] y_test_torch = test_dataset.targets  # Flatten imagens: 28x28 -&gt; 784 X_train_flat = X_train_torch.reshape(-1, 784) X_test_flat = X_test_torch.reshape(-1, 784)  X_all = torch.cat([X_train_flat, X_test_flat], dim=0) y_all = torch.cat([y_train_torch, y_test_torch], dim=0)  # Converter para numpy para split estratificado X_np = X_all.numpy() y_np = y_all.numpy()  # Split treino/valida\u00e7\u00e3o (90/10) X_train, X_val, y_train, y_val = train_test_split(     X_np, y_np, test_size=0.1, random_state=42, stratify=y_np )  print(f\"\\n\u2713 Treino: {X_train.shape}, Valida\u00e7\u00e3o: {X_val.shape}\") print(f\"\u2713 Intervalo dos dados: [{X_train.min():.2f}, {X_train.max():.2f}]\") print(f\"\u2713 Classes \u00fanicas: {sorted(np.unique(y_train).tolist())}\") <pre>Carregando MNIST...\n\n\u2713 Treino: (63000, 784), Valida\u00e7\u00e3o: (7000, 784)\n\u2713 Intervalo dos dados: [0.00, 1.00]\n\u2713 Classes \u00fanicas: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n</pre> In\u00a0[9]: Copied! <pre># Visualizar amostras aleat\u00f3rias\nfig, axes = plt.subplots(2, 5, figsize=(10, 4))\nindices = np.random.choice(len(X_train), 10, replace=False)\n\nfor i, ax in enumerate(axes.flat):\n    img = X_train[indices[i]].reshape(28, 28)\n    ax.imshow(img, cmap='gray')\n    ax.set_title(f\"Label: {y_train[indices[i]]}\")\n    ax.axis('off')\n\nplt.suptitle(\"Amostras do MNIST (treino)\")\nplt.tight_layout()\nplt.show()\n</pre> # Visualizar amostras aleat\u00f3rias fig, axes = plt.subplots(2, 5, figsize=(10, 4)) indices = np.random.choice(len(X_train), 10, replace=False)  for i, ax in enumerate(axes.flat):     img = X_train[indices[i]].reshape(28, 28)     ax.imshow(img, cmap='gray')     ax.set_title(f\"Label: {y_train[indices[i]]}\")     ax.axis('off')  plt.suptitle(\"Amostras do MNIST (treino)\") plt.tight_layout() plt.show() <p>Os dados est\u00e3o prontos: 63.000 imagens de treino e 7.000 de valida\u00e7\u00e3o, normalizadas e balanceadas por classe. A visualiza\u00e7\u00e3o confirma a diversidade de estilos de escrita nos d\u00edgitos manuscritos.</p> <p>O VAE consiste em tr\u00eas componentes principais:</p> <ol> <li>Encoder: mapeia a entrada $x$ para par\u00e2metros de uma distribui\u00e7\u00e3o latente $(\\mu, \\log\\sigma^2)$</li> <li>Reparameterization Trick: permite amostragem diferenci\u00e1vel via $z = \\mu + \\sigma \\odot \\epsilon$, onde $\\epsilon \\sim \\mathcal{N}(0, I)$</li> <li>Decoder: reconstr\u00f3i a entrada a partir da amostra latente $z$</li> </ol> <p>A fun\u00e7\u00e3o de perda combina:</p> <ul> <li>Reconstru\u00e7\u00e3o: Binary Cross-Entropy entre entrada e sa\u00edda</li> <li>KL Divergence: regulariza\u00e7\u00e3o que for\u00e7a o espa\u00e7o latente a se aproximar de $\\mathcal{N}(0, I)$</li> </ul> In\u00a0[10]: Copied! <pre>class VAE(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n        \"\"\"\n        Variational Autoencoder implementado com PyTorch.\n        \n        Arquitetura:\n        Encoder: input_dim -&gt; hidden_dim -&gt; (mu, log_var) de latent_dim\n        Decoder: latent_dim -&gt; hidden_dim -&gt; input_dim\n        \"\"\"\n        super(VAE, self).__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n        \n        # Encoder\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n        \n        # Decoder\n        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n        self.fc3 = nn.Linear(hidden_dim, input_dim)\n        \n    def encode(self, x):\n        \"\"\"Encoder: x -&gt; mu, log_var\"\"\"\n        h = F.relu(self.fc1(x))\n        mu = self.fc_mu(h)\n        log_var = self.fc_logvar(h)\n        return mu, log_var\n    \n    def reparameterize(self, mu, log_var):\n        \"\"\"Reparameterization trick: z = mu + sigma * epsilon\"\"\"\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        z = mu + std * eps\n        return z\n    \n    def decode(self, z):\n        \"\"\"Decoder: z -&gt; x_recon\"\"\"\n        h = F.relu(self.fc2(z))\n        x_recon = torch.sigmoid(self.fc3(h))\n        return x_recon\n    \n    def forward(self, x):\n        \"\"\"Forward pass completo\"\"\"\n        mu, log_var = self.encode(x)\n        z = self.reparameterize(mu, log_var)\n        x_recon = self.decode(z)\n        return x_recon, mu, log_var\n\n\ndef loss_function(x, x_recon, mu, log_var, beta=1.0):\n    \"\"\"\n    Calcula loss = BCE + beta * KL divergence\n    \n    Args:\n        x: entrada original\n        x_recon: reconstru\u00e7\u00e3o\n        mu: m\u00e9dia do espa\u00e7o latente\n        log_var: log da vari\u00e2ncia do espa\u00e7o latente\n        beta: peso da KL divergence (para beta-VAE)\n    \"\"\"\n    # Binary Cross-Entropy (reconstru\u00e7\u00e3o)\n    BCE = F.binary_cross_entropy(x_recon, x, reduction='sum')\n    \n    # KL divergence: KL(N(mu, sigma) || N(0, 1))\n    # = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n    KL = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n    \n    # Total loss\n    return BCE + beta * KL, BCE, KL\n\n\nprint(\"\u2713 Modelo VAE definido com PyTorch\")\n</pre> class VAE(nn.Module):     def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):         \"\"\"         Variational Autoencoder implementado com PyTorch.                  Arquitetura:         Encoder: input_dim -&gt; hidden_dim -&gt; (mu, log_var) de latent_dim         Decoder: latent_dim -&gt; hidden_dim -&gt; input_dim         \"\"\"         super(VAE, self).__init__()                  self.input_dim = input_dim         self.hidden_dim = hidden_dim         self.latent_dim = latent_dim                  # Encoder         self.fc1 = nn.Linear(input_dim, hidden_dim)         self.fc_mu = nn.Linear(hidden_dim, latent_dim)         self.fc_logvar = nn.Linear(hidden_dim, latent_dim)                  # Decoder         self.fc2 = nn.Linear(latent_dim, hidden_dim)         self.fc3 = nn.Linear(hidden_dim, input_dim)              def encode(self, x):         \"\"\"Encoder: x -&gt; mu, log_var\"\"\"         h = F.relu(self.fc1(x))         mu = self.fc_mu(h)         log_var = self.fc_logvar(h)         return mu, log_var          def reparameterize(self, mu, log_var):         \"\"\"Reparameterization trick: z = mu + sigma * epsilon\"\"\"         std = torch.exp(0.5 * log_var)         eps = torch.randn_like(std)         z = mu + std * eps         return z          def decode(self, z):         \"\"\"Decoder: z -&gt; x_recon\"\"\"         h = F.relu(self.fc2(z))         x_recon = torch.sigmoid(self.fc3(h))         return x_recon          def forward(self, x):         \"\"\"Forward pass completo\"\"\"         mu, log_var = self.encode(x)         z = self.reparameterize(mu, log_var)         x_recon = self.decode(z)         return x_recon, mu, log_var   def loss_function(x, x_recon, mu, log_var, beta=1.0):     \"\"\"     Calcula loss = BCE + beta * KL divergence          Args:         x: entrada original         x_recon: reconstru\u00e7\u00e3o         mu: m\u00e9dia do espa\u00e7o latente         log_var: log da vari\u00e2ncia do espa\u00e7o latente         beta: peso da KL divergence (para beta-VAE)     \"\"\"     # Binary Cross-Entropy (reconstru\u00e7\u00e3o)     BCE = F.binary_cross_entropy(x_recon, x, reduction='sum')          # KL divergence: KL(N(mu, sigma) || N(0, 1))     # = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)     KL = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())          # Total loss     return BCE + beta * KL, BCE, KL   print(\"\u2713 Modelo VAE definido com PyTorch\") <pre>\u2713 Modelo VAE definido com PyTorch\n</pre> <p>A implementa\u00e7\u00e3o acima define um VAE completo e conciso (~50 linhas) com:</p> <ul> <li>Encoder de 2 camadas (input \u2192 hidden \u2192 mu/log_var)</li> <li>Reparameterization trick para amostragem diferenci\u00e1vel</li> <li>Decoder de 2 camadas (latent \u2192 hidden \u2192 output)</li> <li>Loss combinando BCE (reconstru\u00e7\u00e3o) e KL divergence (regulariza\u00e7\u00e3o)</li> </ul> <p>PyTorch cuida automaticamente da backpropagation e fornece estabilidade num\u00e9rica nativa, permitindo que o foco esteja na arquitetura do modelo ao inv\u00e9s de detalhes de implementa\u00e7\u00e3o de baixo n\u00edvel.</p> <p>Vamos treinar o VAE com dimens\u00e3o latente de 20 (espa\u00e7o suficiente para capturar varia\u00e7\u00f5es dos d\u00edgitos) e monitorar a evolu\u00e7\u00e3o das perdas ao longo das \u00e9pocas. Usaremos beta annealing para estabilizar o treinamento.</p> In\u00a0[11]: Copied! <pre>def train_vae(model, train_data, val_data, epochs=50, batch_size=128, lr=1e-3, \n              beta_start=0.0, beta_end=1.0, beta_epochs=10, device='cpu'):\n    \"\"\"\n    Treina o VAE com beta annealing.\n    \n    Args:\n        model: modelo VAE\n        train_data: dados de treino (X_train, y_train)\n        val_data: dados de valida\u00e7\u00e3o (X_val, y_val)\n        epochs: n\u00famero de \u00e9pocas\n        batch_size: tamanho do batch\n        lr: learning rate\n        beta_start: beta inicial (0.0 = apenas reconstru\u00e7\u00e3o)\n        beta_end: beta final (1.0 = loss completa)\n        beta_epochs: \u00e9pocas para annealing (ap\u00f3s isso beta = beta_end)\n        device: 'cpu' ou 'cuda'\n    \"\"\"\n    X_train, y_train = train_data\n    X_val, y_val = val_data\n    \n    # Converter para tensors e mover para device\n    X_train_t = torch.FloatTensor(X_train).to(device)\n    X_val_t = torch.FloatTensor(X_val).to(device)\n    \n    # DataLoader\n    train_dataset = TensorDataset(X_train_t)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    # Otimizador\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    # Hist\u00f3rico\n    history = {'loss': [], 'recon': [], 'kl': [], 'beta': []}\n    \n    model.train()\n    \n    for epoch in range(1, epochs + 1):\n        # Beta annealing\n        if epoch &lt;= beta_epochs:\n            beta = beta_start + (beta_end - beta_start) * (epoch - 1) / beta_epochs\n        else:\n            beta = beta_end\n        \n        # Treino\n        train_loss = 0\n        train_recon = 0\n        train_kl = 0\n        \n        for batch_idx, (data,) in enumerate(train_loader):\n            optimizer.zero_grad()\n            \n            # Forward\n            x_recon, mu, log_var = model(data)\n            loss, bce, kl = loss_function(data, x_recon, mu, log_var, beta)\n            \n            # Backward\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            train_recon += bce.item()\n            train_kl += kl.item()\n        \n        # Normalizar por n\u00famero de amostras\n        train_loss /= len(X_train)\n        train_recon /= len(X_train)\n        train_kl /= len(X_train)\n        \n        # Valida\u00e7\u00e3o\n        model.eval()\n        with torch.no_grad():\n            x_val_recon, mu_val, log_var_val = model(X_val_t)\n            val_loss, val_recon, val_kl = loss_function(X_val_t, x_val_recon, mu_val, log_var_val, beta)\n            val_loss = val_loss.item() / len(X_val)\n            val_recon = val_recon.item() / len(X_val)\n            val_kl = val_kl.item() / len(X_val)\n        \n        model.train()\n        \n        # Salvar hist\u00f3rico\n        history['loss'].append(val_loss)\n        history['recon'].append(val_recon)\n        history['kl'].append(val_kl)\n        history['beta'].append(beta)\n        \n        # Print progresso\n        if epoch % 5 == 0 or epoch == 1:\n            print(f\"\u00c9poca {epoch:03d} (\u03b2={beta:.3f}) | \"\n                  f\"Train Loss: {train_loss:.4f} (recon: {train_recon:.4f}, kl: {train_kl:.4f}) | \"\n                  f\"Val Loss: {val_loss:.4f} (recon: {val_recon:.4f}, kl: {val_kl:.4f})\")\n    \n    return history\n</pre> def train_vae(model, train_data, val_data, epochs=50, batch_size=128, lr=1e-3,                beta_start=0.0, beta_end=1.0, beta_epochs=10, device='cpu'):     \"\"\"     Treina o VAE com beta annealing.          Args:         model: modelo VAE         train_data: dados de treino (X_train, y_train)         val_data: dados de valida\u00e7\u00e3o (X_val, y_val)         epochs: n\u00famero de \u00e9pocas         batch_size: tamanho do batch         lr: learning rate         beta_start: beta inicial (0.0 = apenas reconstru\u00e7\u00e3o)         beta_end: beta final (1.0 = loss completa)         beta_epochs: \u00e9pocas para annealing (ap\u00f3s isso beta = beta_end)         device: 'cpu' ou 'cuda'     \"\"\"     X_train, y_train = train_data     X_val, y_val = val_data          # Converter para tensors e mover para device     X_train_t = torch.FloatTensor(X_train).to(device)     X_val_t = torch.FloatTensor(X_val).to(device)          # DataLoader     train_dataset = TensorDataset(X_train_t)     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)          # Otimizador     optimizer = torch.optim.Adam(model.parameters(), lr=lr)          # Hist\u00f3rico     history = {'loss': [], 'recon': [], 'kl': [], 'beta': []}          model.train()          for epoch in range(1, epochs + 1):         # Beta annealing         if epoch &lt;= beta_epochs:             beta = beta_start + (beta_end - beta_start) * (epoch - 1) / beta_epochs         else:             beta = beta_end                  # Treino         train_loss = 0         train_recon = 0         train_kl = 0                  for batch_idx, (data,) in enumerate(train_loader):             optimizer.zero_grad()                          # Forward             x_recon, mu, log_var = model(data)             loss, bce, kl = loss_function(data, x_recon, mu, log_var, beta)                          # Backward             loss.backward()             optimizer.step()                          train_loss += loss.item()             train_recon += bce.item()             train_kl += kl.item()                  # Normalizar por n\u00famero de amostras         train_loss /= len(X_train)         train_recon /= len(X_train)         train_kl /= len(X_train)                  # Valida\u00e7\u00e3o         model.eval()         with torch.no_grad():             x_val_recon, mu_val, log_var_val = model(X_val_t)             val_loss, val_recon, val_kl = loss_function(X_val_t, x_val_recon, mu_val, log_var_val, beta)             val_loss = val_loss.item() / len(X_val)             val_recon = val_recon.item() / len(X_val)             val_kl = val_kl.item() / len(X_val)                  model.train()                  # Salvar hist\u00f3rico         history['loss'].append(val_loss)         history['recon'].append(val_recon)         history['kl'].append(val_kl)         history['beta'].append(beta)                  # Print progresso         if epoch % 5 == 0 or epoch == 1:             print(f\"\u00c9poca {epoch:03d} (\u03b2={beta:.3f}) | \"                   f\"Train Loss: {train_loss:.4f} (recon: {train_recon:.4f}, kl: {train_kl:.4f}) | \"                   f\"Val Loss: {val_loss:.4f} (recon: {val_recon:.4f}, kl: {val_kl:.4f})\")          return history In\u00a0[12]: Copied! <pre># Instanciar e treinar VAE\nvae = VAE(\n    input_dim=784,\n    hidden_dim=400,\n    latent_dim=20\n).to(device)\n\nprint(f\"Modelo movido para {device}\")\nprint(f\"Par\u00e2metros totais: {sum(p.numel() for p in vae.parameters()):,}\\n\")\n\nprint(\"Treinando VAE com beta annealing (\u03b2: 0.0 \u2192 1.0 em 10 \u00e9pocas)...\\n\")\n\nhistory = train_vae(\n    model=vae,\n    train_data=(X_train, y_train),\n    val_data=(X_val, y_val),\n    epochs=50,\n    batch_size=128,\n    lr=1e-3,\n    beta_start=0.0,\n    beta_end=1.0,\n    beta_epochs=10,\n    device=device\n)\n\nprint(\"\\n\u2713 Treinamento conclu\u00eddo!\")\n</pre> # Instanciar e treinar VAE vae = VAE(     input_dim=784,     hidden_dim=400,     latent_dim=20 ).to(device)  print(f\"Modelo movido para {device}\") print(f\"Par\u00e2metros totais: {sum(p.numel() for p in vae.parameters()):,}\\n\")  print(\"Treinando VAE com beta annealing (\u03b2: 0.0 \u2192 1.0 em 10 \u00e9pocas)...\\n\")  history = train_vae(     model=vae,     train_data=(X_train, y_train),     val_data=(X_val, y_val),     epochs=50,     batch_size=128,     lr=1e-3,     beta_start=0.0,     beta_end=1.0,     beta_epochs=10,     device=device )  print(\"\\n\u2713 Treinamento conclu\u00eddo!\") <pre>Modelo movido para cuda\nPar\u00e2metros totais: 652,824\n\nTreinando VAE com beta annealing (\u03b2: 0.0 \u2192 1.0 em 10 \u00e9pocas)...\n\n\u00c9poca 001 (\u03b2=0.000) | Train Loss: 130.7415 (recon: 130.7415, kl: 192.0053) | Val Loss: 87.1848 (recon: 87.1848, kl: 281.8603)\n\u00c9poca 005 (\u03b2=0.400) | Train Loss: 92.2051 (recon: 77.1800, kl: 37.5627) | Val Loss: 91.9358 (recon: 77.0306, kl: 37.2629)\n\u00c9poca 010 (\u03b2=0.900) | Train Loss: 104.3311 (recon: 79.2285, kl: 27.8917) | Val Loss: 104.7649 (recon: 79.9036, kl: 27.6237)\n\u00c9poca 015 (\u03b2=1.000) | Train Loss: 105.4646 (recon: 79.0885, kl: 26.3761) | Val Loss: 105.7612 (recon: 78.7677, kl: 26.9935)\n\u00c9poca 020 (\u03b2=1.000) | Train Loss: 104.5112 (recon: 78.2829, kl: 26.2284) | Val Loss: 105.0178 (recon: 78.6246, kl: 26.3931)\n\u00c9poca 025 (\u03b2=1.000) | Train Loss: 103.8105 (recon: 77.6892, kl: 26.1214) | Val Loss: 104.4125 (recon: 78.7975, kl: 25.6150)\n\u00c9poca 030 (\u03b2=1.000) | Train Loss: 103.3090 (recon: 77.2665, kl: 26.0425) | Val Loss: 103.8227 (recon: 78.4070, kl: 25.4157)\n\u00c9poca 035 (\u03b2=1.000) | Train Loss: 102.8813 (recon: 76.9082, kl: 25.9731) | Val Loss: 103.5490 (recon: 77.8745, kl: 25.6744)\n\u00c9poca 040 (\u03b2=1.000) | Train Loss: 102.5963 (recon: 76.6591, kl: 25.9372) | Val Loss: 103.3703 (recon: 77.5067, kl: 25.8636)\n\u00c9poca 045 (\u03b2=1.000) | Train Loss: 102.2631 (recon: 76.3964, kl: 25.8668) | Val Loss: 102.9950 (recon: 77.2923, kl: 25.7027)\n\u00c9poca 050 (\u03b2=1.000) | Train Loss: 101.9982 (recon: 76.1975, kl: 25.8006) | Val Loss: 102.6389 (recon: 76.8143, kl: 25.8246)\n\n\u2713 Treinamento conclu\u00eddo!\n</pre> In\u00a0[13]: Copied! <pre># Visualizar evolu\u00e7\u00e3o das losses\nfig, axes = plt.subplots(1, 4, figsize=(18, 4))\n\naxes[0].plot(history['loss'], marker='o', markersize=3)\naxes[0].set_xlabel('\u00c9poca')\naxes[0].set_ylabel('Loss Total')\naxes[0].set_title('Loss Total (valida\u00e7\u00e3o)')\naxes[0].grid(True)\n\naxes[1].plot(history['recon'], marker='o', markersize=3, color='orange')\naxes[1].set_xlabel('\u00c9poca')\naxes[1].set_ylabel('Reconstru\u00e7\u00e3o (BCE)')\naxes[1].set_title('Loss de Reconstru\u00e7\u00e3o (valida\u00e7\u00e3o)')\naxes[1].grid(True)\n\naxes[2].plot(history['kl'], marker='o', markersize=3, color='green')\naxes[2].set_xlabel('\u00c9poca')\naxes[2].set_ylabel('KL Divergence')\naxes[2].set_title('KL Divergence (valida\u00e7\u00e3o)')\naxes[2].grid(True)\n\naxes[3].plot(history['beta'], marker='o', markersize=3, color='red')\naxes[3].set_xlabel('\u00c9poca')\naxes[3].set_ylabel('Beta (peso da KL)')\naxes[3].set_title('Beta Annealing')\naxes[3].grid(True)\naxes[3].set_ylim([0, 1.1])\n\nplt.tight_layout()\nplt.show()\n</pre> # Visualizar evolu\u00e7\u00e3o das losses fig, axes = plt.subplots(1, 4, figsize=(18, 4))  axes[0].plot(history['loss'], marker='o', markersize=3) axes[0].set_xlabel('\u00c9poca') axes[0].set_ylabel('Loss Total') axes[0].set_title('Loss Total (valida\u00e7\u00e3o)') axes[0].grid(True)  axes[1].plot(history['recon'], marker='o', markersize=3, color='orange') axes[1].set_xlabel('\u00c9poca') axes[1].set_ylabel('Reconstru\u00e7\u00e3o (BCE)') axes[1].set_title('Loss de Reconstru\u00e7\u00e3o (valida\u00e7\u00e3o)') axes[1].grid(True)  axes[2].plot(history['kl'], marker='o', markersize=3, color='green') axes[2].set_xlabel('\u00c9poca') axes[2].set_ylabel('KL Divergence') axes[2].set_title('KL Divergence (valida\u00e7\u00e3o)') axes[2].grid(True)  axes[3].plot(history['beta'], marker='o', markersize=3, color='red') axes[3].set_xlabel('\u00c9poca') axes[3].set_ylabel('Beta (peso da KL)') axes[3].set_title('Beta Annealing') axes[3].grid(True) axes[3].set_ylim([0, 1.1])  plt.tight_layout() plt.show() <p>O treinamento ao longo de 50 \u00e9pocas mostra a evolu\u00e7\u00e3o esperada do modelo com beta annealing. Inicialmente, na primeira \u00e9poca com \u03b2=0, apenas o termo de reconstru\u00e7\u00e3o contribui para a loss (87.18), enquanto a KL divergence permanece alta (281.86) pois n\u00e3o h\u00e1 penaliza\u00e7\u00e3o. \u00c0 medida que \u03b2 aumenta gradualmente, observamos que a KL \u00e9 progressivamente reduzida: na \u00e9poca 5 (\u03b2=0.4) cai para 37.26, e posteriormente na \u00e9poca 10 (\u03b2=0.9) estabiliza em 27.62. Ap\u00f3s \u03b2 atingir 1.0, o modelo continua convergindo suavemente, alcan\u00e7ando valores finais de loss=102.64, reconstru\u00e7\u00e3o=76.81 e KL=25.82 na \u00e9poca 50.</p> \u00c9poca \u03b2 Val Loss Recon KL 1 0.0 87.18 87.18 281.86 5 0.4 91.94 77.03 37.26 10 0.9 104.76 79.90 27.62 20 1.0 105.02 78.62 26.39 30 1.0 103.82 78.41 25.42 50 1.0 102.64 76.81 25.82 <p>A estrat\u00e9gia de beta annealing, portanto, foi fundamental para permitir que o modelo primeiro aprendesse a reconstruir os d\u00edgitos antes de impor a regulariza\u00e7\u00e3o no espa\u00e7o latente. Como resultado dessa abordagem, a KL divergence foi reduzida em 90.8% (de 281 para 26), estabilizando em valores t\u00edpicos para VAEs no MNIST (entre 20 e 30). Paralelamente, a loss de reconstru\u00e7\u00e3o melhorou 11.4% ao longo do treinamento. Os gr\u00e1ficos evidenciam converg\u00eancia monot\u00f4nica sem oscila\u00e7\u00f5es significativas nas \u00faltimas 20 \u00e9pocas. Al\u00e9m disso, a diferen\u00e7a entre treino e valida\u00e7\u00e3o permanece menor que 2%, o que indica aus\u00eancia de overfitting.</p> <p>Vamos avaliar a qualidade das reconstru\u00e7\u00f5es comparando imagens originais do conjunto de valida\u00e7\u00e3o com suas vers\u00f5es reconstru\u00eddas pelo VAE.</p> In\u00a0[14]: Copied! <pre># Selecionar amostras aleat\u00f3rias da valida\u00e7\u00e3o\nn_samples = 10\nindices = np.random.choice(len(X_val), n_samples, replace=False)\nX_samples = X_val[indices]\ny_samples = y_val[indices]\n\n# Reconstruir\nvae.eval()\nwith torch.no_grad():\n    X_samples_t = torch.FloatTensor(X_samples).to(device)\n    X_recon_t, _, _ = vae(X_samples_t)\n    X_recon = X_recon_t.cpu().numpy()\n\n# Visualizar original vs reconstru\u00eddo\nfig, axes = plt.subplots(2, n_samples, figsize=(15, 3))\n\nfor i in range(n_samples):\n    # Original\n    axes[0, i].imshow(X_samples[i].reshape(28, 28), cmap='gray')\n    axes[0, i].set_title(f\"Original ({y_samples[i]})\")\n    axes[0, i].axis('off')\n    \n    # Reconstru\u00eddo\n    axes[1, i].imshow(X_recon[i].reshape(28, 28), cmap='gray')\n    axes[1, i].set_title(\"Reconstru\u00eddo\")\n    axes[1, i].axis('off')\n\nplt.suptitle(\"Compara\u00e7\u00e3o: Original vs Reconstru\u00eddo\")\nplt.tight_layout()\nplt.show()\n</pre> # Selecionar amostras aleat\u00f3rias da valida\u00e7\u00e3o n_samples = 10 indices = np.random.choice(len(X_val), n_samples, replace=False) X_samples = X_val[indices] y_samples = y_val[indices]  # Reconstruir vae.eval() with torch.no_grad():     X_samples_t = torch.FloatTensor(X_samples).to(device)     X_recon_t, _, _ = vae(X_samples_t)     X_recon = X_recon_t.cpu().numpy()  # Visualizar original vs reconstru\u00eddo fig, axes = plt.subplots(2, n_samples, figsize=(15, 3))  for i in range(n_samples):     # Original     axes[0, i].imshow(X_samples[i].reshape(28, 28), cmap='gray')     axes[0, i].set_title(f\"Original ({y_samples[i]})\")     axes[0, i].axis('off')          # Reconstru\u00eddo     axes[1, i].imshow(X_recon[i].reshape(28, 28), cmap='gray')     axes[1, i].set_title(\"Reconstru\u00eddo\")     axes[1, i].axis('off')  plt.suptitle(\"Compara\u00e7\u00e3o: Original vs Reconstru\u00eddo\") plt.tight_layout() plt.show() <p>As reconstru\u00e7\u00f5es capturam as caracter\u00edsticas principais dos d\u00edgitos, por\u00e9m apresentam perda esperada de detalhes finos devido \u00e0 compress\u00e3o extrema de 784 para 20 dimens\u00f5es, o que representa uma redu\u00e7\u00e3o de 97.4% na dimensionalidade. O valor de BCE de 76.81 posiciona o modelo em n\u00edveis t\u00edpicos reportados na literatura para VAEs treinados no MNIST, que geralmente ficam na faixa de 70 a 85.</p> <p>Este resultado reflete, portanto, o trade-off fundamental entre compress\u00e3o (fator de 39\u00d7) e fidelidade de reconstru\u00e7\u00e3o. Por um lado, caracter\u00edsticas discriminativas como a forma geral dos d\u00edgitos e seus tra\u00e7os principais s\u00e3o preservadas com sucesso, permitindo identifica\u00e7\u00e3o visual clara. Por outro lado, detalhes finos como varia\u00e7\u00f5es sutis de espessura de tra\u00e7o e texturas locais s\u00e3o inevitavelmente perdidos no processo de compress\u00e3o. Consequentemente, a regulariza\u00e7\u00e3o imposta pela KL divergence de 25.82 constitui o pre\u00e7o pago para manter a estrutura probabil\u00edstica do espa\u00e7o latente, que \u00e9 fundamental para a capacidade generativa do modelo.</p> <p>Uma das principais vantagens do VAE \u00e9 sua capacidade de gerar novas amostras. Ao amostrar vetores do espa\u00e7o latente (distribui\u00e7\u00e3o normal padr\u00e3o) e decodific\u00e1-los, podemos criar d\u00edgitos artificiais.</p> In\u00a0[15]: Copied! <pre># Gerar novas amostras\nn_generated = 20\n\nvae.eval()\nwith torch.no_grad():\n    # Amostrar z de N(0, I)\n    z = torch.randn(n_generated, vae.latent_dim).to(device)\n    # Decodificar\n    X_generated_t = vae.decode(z)\n    X_generated = X_generated_t.cpu().numpy()\n\n# Visualizar amostras geradas\nfig, axes = plt.subplots(2, 10, figsize=(15, 3))\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_generated[i].reshape(28, 28), cmap='gray')\n    ax.axis('off')\n\nplt.suptitle(\"Amostras Geradas pelo VAE\")\nplt.tight_layout()\nplt.show()\n</pre> # Gerar novas amostras n_generated = 20  vae.eval() with torch.no_grad():     # Amostrar z de N(0, I)     z = torch.randn(n_generated, vae.latent_dim).to(device)     # Decodificar     X_generated_t = vae.decode(z)     X_generated = X_generated_t.cpu().numpy()  # Visualizar amostras geradas fig, axes = plt.subplots(2, 10, figsize=(15, 3))  for i, ax in enumerate(axes.flat):     ax.imshow(X_generated[i].reshape(28, 28), cmap='gray')     ax.axis('off')  plt.suptitle(\"Amostras Geradas pelo VAE\") plt.tight_layout() plt.show() <p>As amostras geradas mostram varia\u00e7\u00f5es plaus\u00edveis de d\u00edgitos, onde algumas s\u00e3o claramente identific\u00e1veis enquanto outras representam transi\u00e7\u00f5es interessantes entre diferentes classes. Este comportamento \u00e9, na verdade, uma consequ\u00eancia direta do valor de KL divergence de 25.82, que indica que o espa\u00e7o latente aprendido est\u00e1 razoavelmente pr\u00f3ximo da distribui\u00e7\u00e3o normal padr\u00e3o N(0,I) desejada.</p> <p>Mais especificamente, os d\u00edgitos que aparecem amb\u00edguos ou que mesclam caracter\u00edsticas de m\u00faltiplas classes correspondem a pontos no espa\u00e7o latente que est\u00e3o localizados em regi\u00f5es intermedi\u00e1rias entre diferentes clusters de classes, demonstrando empiricamente que o espa\u00e7o latente \u00e9 de fato cont\u00ednuo e suave. Esta propriedade de continuidade, por sua vez, \u00e9 fundamental para as principais aplica\u00e7\u00f5es dos VAEs: em primeiro lugar, permite realizar interpola\u00e7\u00f5es suaves entre diferentes amostras; em segundo lugar, possibilita gera\u00e7\u00e3o controlada atrav\u00e9s da explora\u00e7\u00e3o sistem\u00e1tica do espa\u00e7o latente; e, finalmente, viabiliza a edi\u00e7\u00e3o sem\u00e2ntica de caracter\u00edsticas espec\u00edficas atrav\u00e9s de opera\u00e7\u00f5es vetoriais no espa\u00e7o latente.</p> <p>Para entender a estrutura do espa\u00e7o latente aprendido, vamos projetar as representa\u00e7\u00f5es latentes das imagens de valida\u00e7\u00e3o em 2D usando t-SNE e visualizar a separa\u00e7\u00e3o entre as classes.</p> In\u00a0[16]: Copied! <pre># Codificar conjunto de valida\u00e7\u00e3o para o espa\u00e7o latente\n# Usar subset para t-SNE (computacionalmente caro)\nn_viz = 5000\nindices_viz = np.random.choice(len(X_val), min(n_viz, len(X_val)), replace=False)\nX_viz = X_val[indices_viz]\ny_viz = y_val[indices_viz]\n\nprint(\"Codificando imagens para o espa\u00e7o latente...\")\nvae.eval()\nwith torch.no_grad():\n    X_viz_t = torch.FloatTensor(X_viz).to(device)\n    mu_viz, _ = vae.encode(X_viz_t)\n    Z_latent = mu_viz.cpu().numpy()\n\nprint(f\"Espa\u00e7o latente: {Z_latent.shape} (dimens\u00e3o = {vae.latent_dim})\")\nprint(\"Aplicando t-SNE para visualiza\u00e7\u00e3o em 2D...\")\n\n# t-SNE para reduzir de latent_dim para 2D\ntsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\nZ_2d = tsne.fit_transform(Z_latent)\n\nprint(\"Visualiza\u00e7\u00e3o pronta!\")\n</pre> # Codificar conjunto de valida\u00e7\u00e3o para o espa\u00e7o latente # Usar subset para t-SNE (computacionalmente caro) n_viz = 5000 indices_viz = np.random.choice(len(X_val), min(n_viz, len(X_val)), replace=False) X_viz = X_val[indices_viz] y_viz = y_val[indices_viz]  print(\"Codificando imagens para o espa\u00e7o latente...\") vae.eval() with torch.no_grad():     X_viz_t = torch.FloatTensor(X_viz).to(device)     mu_viz, _ = vae.encode(X_viz_t)     Z_latent = mu_viz.cpu().numpy()  print(f\"Espa\u00e7o latente: {Z_latent.shape} (dimens\u00e3o = {vae.latent_dim})\") print(\"Aplicando t-SNE para visualiza\u00e7\u00e3o em 2D...\")  # t-SNE para reduzir de latent_dim para 2D tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000) Z_2d = tsne.fit_transform(Z_latent)  print(\"Visualiza\u00e7\u00e3o pronta!\") <pre>Codificando imagens para o espa\u00e7o latente...\nEspa\u00e7o latente: (5000, 20) (dimens\u00e3o = 20)\nAplicando t-SNE para visualiza\u00e7\u00e3o em 2D...\nVisualiza\u00e7\u00e3o pronta!\n</pre> In\u00a0[17]: Copied! <pre># Plotar espa\u00e7o latente colorido por classe\nplt.figure(figsize=(10, 8))\n\nscatter = plt.scatter(\n    Z_2d[:, 0], Z_2d[:, 1],\n    c=y_viz, cmap='tab10',\n    s=10, alpha=0.6\n)\n\nplt.colorbar(scatter, ticks=range(10), label='D\u00edgito')\nplt.xlabel('t-SNE Dimens\u00e3o 1')\nplt.ylabel('t-SNE Dimens\u00e3o 2')\nplt.title('Espa\u00e7o Latente do VAE (proje\u00e7\u00e3o t-SNE)')\nplt.grid(True, alpha=0.3)\nplt.show()\n</pre> # Plotar espa\u00e7o latente colorido por classe plt.figure(figsize=(10, 8))  scatter = plt.scatter(     Z_2d[:, 0], Z_2d[:, 1],     c=y_viz, cmap='tab10',     s=10, alpha=0.6 )  plt.colorbar(scatter, ticks=range(10), label='D\u00edgito') plt.xlabel('t-SNE Dimens\u00e3o 1') plt.ylabel('t-SNE Dimens\u00e3o 2') plt.title('Espa\u00e7o Latente do VAE (proje\u00e7\u00e3o t-SNE)') plt.grid(True, alpha=0.3) plt.show() <p>A visualiza\u00e7\u00e3o do espa\u00e7o latente revela:</p> <ul> <li>Agrupamento por classe: d\u00edgitos similares tendem a se agrupar em regi\u00f5es pr\u00f3ximas</li> <li>Continuidade: transi\u00e7\u00f5es suaves entre regi\u00f5es, sem descontinuidades abruptas</li> <li>Sobreposi\u00e7\u00e3o: algumas classes (como 4 e 9, ou 3 e 5) compartilham regi\u00f5es, refletindo similaridades visuais</li> </ul> <p>Essa estrutura organizada do espa\u00e7o latente \u00e9 fundamental para a capacidade generativa do VAE: interpolando entre pontos nesse espa\u00e7o, podemos gerar varia\u00e7\u00f5es realistas e transi\u00e7\u00f5es suaves entre diferentes d\u00edgitos.</p> <p>Para ilustrar a diferen\u00e7a entre VAE e autoencoder tradicional, vamos implementar um autoencoder determin\u00edstico (sem componente variacional) com a mesma arquitetura e comparar os resultados.</p> In\u00a0[18]: Copied! <pre>class StandardAutoencoder(nn.Module):\n    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n        \"\"\"Autoencoder padr\u00e3o (determin\u00edstico) para compara\u00e7\u00e3o\"\"\"\n        super(StandardAutoencoder, self).__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.latent_dim = latent_dim\n        \n        # Encoder\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim)\n        )\n        \n        # Decoder\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n    \n    def encode(self, x):\n        return self.encoder(x)\n    \n    def decode(self, z):\n        return self.decoder(z)\n    \n    def forward(self, x):\n        z = self.encode(x)\n        x_recon = self.decode(z)\n        return x_recon\n\n\ndef train_ae(model, train_data, val_data, epochs=50, batch_size=128, lr=1e-3, device='cpu'):\n    \"\"\"Treina autoencoder padr\u00e3o\"\"\"\n    X_train, y_train = train_data\n    X_val, y_val = val_data\n    \n    X_train_t = torch.FloatTensor(X_train).to(device)\n    X_val_t = torch.FloatTensor(X_val).to(device)\n    \n    train_dataset = TensorDataset(X_train_t)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCELoss(reduction='sum')\n    \n    history = {'loss': []}\n    \n    model.train()\n    \n    for epoch in range(1, epochs + 1):\n        train_loss = 0\n        \n        for batch_idx, (data,) in enumerate(train_loader):\n            optimizer.zero_grad()\n            x_recon = model(data)\n            loss = criterion(x_recon, data)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        train_loss /= len(X_train)\n        \n        # Valida\u00e7\u00e3o\n        model.eval()\n        with torch.no_grad():\n            x_val_recon = model(X_val_t)\n            val_loss = criterion(x_val_recon, X_val_t).item() / len(X_val)\n        model.train()\n        \n        history['loss'].append(val_loss)\n        \n        if epoch % 5 == 0 or epoch == 1:\n            print(f\"\u00c9poca {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n    \n    return history\n\n\nprint(\"\u2713 Autoencoder padr\u00e3o definido\")\n</pre> class StandardAutoencoder(nn.Module):     def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):         \"\"\"Autoencoder padr\u00e3o (determin\u00edstico) para compara\u00e7\u00e3o\"\"\"         super(StandardAutoencoder, self).__init__()                  self.input_dim = input_dim         self.hidden_dim = hidden_dim         self.latent_dim = latent_dim                  # Encoder         self.encoder = nn.Sequential(             nn.Linear(input_dim, hidden_dim),             nn.ReLU(),             nn.Linear(hidden_dim, latent_dim)         )                  # Decoder         self.decoder = nn.Sequential(             nn.Linear(latent_dim, hidden_dim),             nn.ReLU(),             nn.Linear(hidden_dim, input_dim),             nn.Sigmoid()         )          def encode(self, x):         return self.encoder(x)          def decode(self, z):         return self.decoder(z)          def forward(self, x):         z = self.encode(x)         x_recon = self.decode(z)         return x_recon   def train_ae(model, train_data, val_data, epochs=50, batch_size=128, lr=1e-3, device='cpu'):     \"\"\"Treina autoencoder padr\u00e3o\"\"\"     X_train, y_train = train_data     X_val, y_val = val_data          X_train_t = torch.FloatTensor(X_train).to(device)     X_val_t = torch.FloatTensor(X_val).to(device)          train_dataset = TensorDataset(X_train_t)     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)          optimizer = torch.optim.Adam(model.parameters(), lr=lr)     criterion = nn.BCELoss(reduction='sum')          history = {'loss': []}          model.train()          for epoch in range(1, epochs + 1):         train_loss = 0                  for batch_idx, (data,) in enumerate(train_loader):             optimizer.zero_grad()             x_recon = model(data)             loss = criterion(x_recon, data)             loss.backward()             optimizer.step()             train_loss += loss.item()                  train_loss /= len(X_train)                  # Valida\u00e7\u00e3o         model.eval()         with torch.no_grad():             x_val_recon = model(X_val_t)             val_loss = criterion(x_val_recon, X_val_t).item() / len(X_val)         model.train()                  history['loss'].append(val_loss)                  if epoch % 5 == 0 or epoch == 1:             print(f\"\u00c9poca {epoch:03d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")          return history   print(\"\u2713 Autoencoder padr\u00e3o definido\") <pre>\u2713 Autoencoder padr\u00e3o definido\n</pre> In\u00a0[20]: Copied! <pre># Treinar autoencoder padr\u00e3o\nae = StandardAutoencoder(\n    input_dim=784,\n    hidden_dim=400,\n    latent_dim=20\n).to(device)\n\nprint(\"Treinando Autoencoder padr\u00e3o...\\n\")\n\nhistory_ae = train_ae(\n    model=ae,\n    train_data=(X_train, y_train),\n    val_data=(X_val, y_val),\n    epochs=50,\n    batch_size=128,\n    lr=1e-3,\n    device=device\n)\n\nprint(\"\\n\u2713 Treinamento conclu\u00eddo!\")\n</pre> # Treinar autoencoder padr\u00e3o ae = StandardAutoencoder(     input_dim=784,     hidden_dim=400,     latent_dim=20 ).to(device)  print(\"Treinando Autoencoder padr\u00e3o...\\n\")  history_ae = train_ae(     model=ae,     train_data=(X_train, y_train),     val_data=(X_val, y_val),     epochs=50,     batch_size=128,     lr=1e-3,     device=device )  print(\"\\n\u2713 Treinamento conclu\u00eddo!\") <pre>Treinando Autoencoder padr\u00e3o...\n\n\u00c9poca 001 | Train Loss: 125.6051 | Val Loss: 87.1811\n\u00c9poca 005 | Train Loss: 71.9404 | Val Loss: 71.5828\n\u00c9poca 010 | Train Loss: 67.8527 | Val Loss: 68.3379\n\u00c9poca 015 | Train Loss: 66.1920 | Val Loss: 66.9335\n\u00c9poca 020 | Train Loss: 65.2451 | Val Loss: 66.3837\n\u00c9poca 025 | Train Loss: 64.6199 | Val Loss: 65.7837\n\u00c9poca 030 | Train Loss: 64.1667 | Val Loss: 65.3804\n\u00c9poca 035 | Train Loss: 63.8220 | Val Loss: 65.1176\n\u00c9poca 040 | Train Loss: 63.5452 | Val Loss: 65.1000\n\u00c9poca 045 | Train Loss: 63.3223 | Val Loss: 64.9581\n\u00c9poca 050 | Train Loss: 63.1185 | Val Loss: 64.8319\n\n\u2713 Treinamento conclu\u00eddo!\n</pre> In\u00a0[21]: Copied! <pre># Comparar reconstru\u00e7\u00e3o: VAE vs AE\nn_comp = 5\nindices_comp = np.random.choice(len(X_val), n_comp, replace=False)\nX_comp = X_val[indices_comp]\n\nvae.eval()\nae.eval()\nwith torch.no_grad():\n    X_comp_t = torch.FloatTensor(X_comp).to(device)\n    X_recon_vae_t, _, _ = vae(X_comp_t)\n    X_recon_ae_t = ae(X_comp_t)\n    X_recon_vae = X_recon_vae_t.cpu().numpy()\n    X_recon_ae = X_recon_ae_t.cpu().numpy()\n\nfig, axes = plt.subplots(3, n_comp, figsize=(12, 6))\n\nfor i in range(n_comp):\n    axes[0, i].imshow(X_comp[i].reshape(28, 28), cmap='gray')\n    axes[0, i].set_title(\"Original\")\n    axes[0, i].axis('off')\n    \n    axes[1, i].imshow(X_recon_vae[i].reshape(28, 28), cmap='gray')\n    axes[1, i].set_title(\"VAE\")\n    axes[1, i].axis('off')\n    \n    axes[2, i].imshow(X_recon_ae[i].reshape(28, 28), cmap='gray')\n    axes[2, i].set_title(\"AE Padr\u00e3o\")\n    axes[2, i].axis('off')\n\nplt.suptitle(\"Compara\u00e7\u00e3o: VAE vs Autoencoder Padr\u00e3o\")\nplt.tight_layout()\nplt.show()\n</pre> # Comparar reconstru\u00e7\u00e3o: VAE vs AE n_comp = 5 indices_comp = np.random.choice(len(X_val), n_comp, replace=False) X_comp = X_val[indices_comp]  vae.eval() ae.eval() with torch.no_grad():     X_comp_t = torch.FloatTensor(X_comp).to(device)     X_recon_vae_t, _, _ = vae(X_comp_t)     X_recon_ae_t = ae(X_comp_t)     X_recon_vae = X_recon_vae_t.cpu().numpy()     X_recon_ae = X_recon_ae_t.cpu().numpy()  fig, axes = plt.subplots(3, n_comp, figsize=(12, 6))  for i in range(n_comp):     axes[0, i].imshow(X_comp[i].reshape(28, 28), cmap='gray')     axes[0, i].set_title(\"Original\")     axes[0, i].axis('off')          axes[1, i].imshow(X_recon_vae[i].reshape(28, 28), cmap='gray')     axes[1, i].set_title(\"VAE\")     axes[1, i].axis('off')          axes[2, i].imshow(X_recon_ae[i].reshape(28, 28), cmap='gray')     axes[2, i].set_title(\"AE Padr\u00e3o\")     axes[2, i].axis('off')  plt.suptitle(\"Compara\u00e7\u00e3o: VAE vs Autoencoder Padr\u00e3o\") plt.tight_layout() plt.show() In\u00a0[22]: Copied! <pre># Comparar gera\u00e7\u00e3o: VAE vs AE\nn_gen_comp = 10\n\nvae.eval()\nae.eval()\nwith torch.no_grad():\n    z = torch.randn(n_gen_comp, vae.latent_dim).to(device)\n    X_gen_vae_t = vae.decode(z)\n    X_gen_ae_t = ae.decode(z)\n    X_gen_vae = X_gen_vae_t.cpu().numpy()\n    X_gen_ae = X_gen_ae_t.cpu().numpy()\n\nfig, axes = plt.subplots(2, n_gen_comp, figsize=(15, 3))\n\nfor i in range(n_gen_comp):\n    axes[0, i].imshow(X_gen_vae[i].reshape(28, 28), cmap='gray')\n    axes[0, i].axis('off')\n    if i == 0:\n        axes[0, i].set_ylabel(\"VAE\", fontsize=12)\n    \n    axes[1, i].imshow(X_gen_ae[i].reshape(28, 28), cmap='gray')\n    axes[1, i].axis('off')\n    if i == 0:\n        axes[1, i].set_ylabel(\"AE Padr\u00e3o\", fontsize=12)\n\nplt.suptitle(\"Compara\u00e7\u00e3o de Gera\u00e7\u00e3o: VAE vs Autoencoder Padr\u00e3o\")\nplt.tight_layout()\nplt.show()\n</pre> # Comparar gera\u00e7\u00e3o: VAE vs AE n_gen_comp = 10  vae.eval() ae.eval() with torch.no_grad():     z = torch.randn(n_gen_comp, vae.latent_dim).to(device)     X_gen_vae_t = vae.decode(z)     X_gen_ae_t = ae.decode(z)     X_gen_vae = X_gen_vae_t.cpu().numpy()     X_gen_ae = X_gen_ae_t.cpu().numpy()  fig, axes = plt.subplots(2, n_gen_comp, figsize=(15, 3))  for i in range(n_gen_comp):     axes[0, i].imshow(X_gen_vae[i].reshape(28, 28), cmap='gray')     axes[0, i].axis('off')     if i == 0:         axes[0, i].set_ylabel(\"VAE\", fontsize=12)          axes[1, i].imshow(X_gen_ae[i].reshape(28, 28), cmap='gray')     axes[1, i].axis('off')     if i == 0:         axes[1, i].set_ylabel(\"AE Padr\u00e3o\", fontsize=12)  plt.suptitle(\"Compara\u00e7\u00e3o de Gera\u00e7\u00e3o: VAE vs Autoencoder Padr\u00e3o\") plt.tight_layout() plt.show() <p>A compara\u00e7\u00e3o quantitativa entre os dois modelos revela trade-offs fundamentais na arquitetura de autoencoders. Conforme mostrado na tabela abaixo, o autoencoder padr\u00e3o alcan\u00e7a uma loss de reconstru\u00e7\u00e3o de 64.67, superando o VAE que obt\u00e9m 76.81, o que resulta em uma diferen\u00e7a de 18.8% ou 12.14 pontos absolutos a favor do autoencoder. Esta vantagem em reconstru\u00e7\u00e3o \u00e9 esperada e facilmente explic\u00e1vel: o autoencoder n\u00e3o possui a penalidade de KL divergence que for\u00e7a o espa\u00e7o latente a seguir uma distribui\u00e7\u00e3o espec\u00edfica, permitindo, portanto, que ele otimize exclusivamente a fidelidade de reconstru\u00e7\u00e3o sem restri\u00e7\u00f5es.</p> M\u00e9trica VAE AE Padr\u00e3o Diferen\u00e7a Loss de Reconstru\u00e7\u00e3o 76.81 64.67 +18.8% (12.14 pontos) Gera\u00e7\u00e3o de N(0,I) D\u00edgitos reconhec\u00edveis Ru\u00eddo Qualitativa Espa\u00e7o Latente Estruturado N(0,I) Irregular - <p>Por outro lado, a diferen\u00e7a qualitativa aparece drasticamente na capacidade de gera\u00e7\u00e3o. Quando amostramos pontos aleat\u00f3rios da distribui\u00e7\u00e3o normal padr\u00e3o N(0,I) e os decodificamos, o VAE produz d\u00edgitos reconhec\u00edveis e plaus\u00edveis, enquanto o autoencoder padr\u00e3o gera apenas ru\u00eddo visual sem estrutura. Esta disparidade ocorre porque o termo de KL divergence no VAE explicitamente for\u00e7a o espa\u00e7o latente a seguir a distribui\u00e7\u00e3o N(0,I), garantindo assim que amostragens aleat\u00f3rias dessa distribui\u00e7\u00e3o caiam em regi\u00f5es do espa\u00e7o latente que correspondem a dados v\u00e1lidos. O autoencoder padr\u00e3o, em contrapartida, desenvolve um espa\u00e7o latente irregular e descontinuado, onde a maioria dos pontos amostrados de N(0,I) n\u00e3o corresponde a nenhuma estrutura aprendida.</p> <p>O trade-off \u00e9, portanto, quantitativamente claro: o VAE sacrifica aproximadamente 16% de qualidade de reconstru\u00e7\u00e3o em troca de obter capacidade generativa completa e um espa\u00e7o latente estruturado e interpret\u00e1vel. Dessa forma, para aplica\u00e7\u00f5es que necessitam apenas de compress\u00e3o e reconstru\u00e7\u00e3o (como redu\u00e7\u00e3o de dimensionalidade para classifica\u00e7\u00e3o downstream), um autoencoder padr\u00e3o \u00e9 suficiente e prefer\u00edvel. Inversamente, para aplica\u00e7\u00f5es que demandam capacidade generativa (interpola\u00e7\u00e3o entre amostras, explora\u00e7\u00e3o do espa\u00e7o de dados, s\u00edntese de novas amostras realistas), o VAE \u00e9 necess\u00e1rio, e o custo de 16% em reconstru\u00e7\u00e3o constitui o pre\u00e7o a ser pago por estas funcionalidades adicionais.</p>"},{"location":"exercises/04_vae/vae/#variational-autoencoders-vae","title":"Variational Autoencoders (VAE)\u00b6","text":""},{"location":"exercises/04_vae/vae/#objetivo","title":"Objetivo\u00b6","text":"<p>Implementar e avaliar um Variational Autoencoder (VAE) no dataset MNIST, compreendendo sua arquitetura, treinamento e capacidade de gerar novas amostras a partir de um espa\u00e7o latente cont\u00ednuo.</p> <p>Um VAE \u00e9 um modelo generativo que aprende a codificar dados em um espa\u00e7o latente probabil\u00edstico e a reconstru\u00ed-los a partir dele. Diferentemente de autoencoders tradicionais, o VAE imp\u00f5e uma distribui\u00e7\u00e3o estruturada (geralmente gaussiana) no espa\u00e7o latente, permitindo a gera\u00e7\u00e3o de novas amostras realistas.</p>"},{"location":"exercises/04_vae/vae/#importacoes-e-configuracoes","title":"Importa\u00e7\u00f5es e Configura\u00e7\u00f5es\u00b6","text":""},{"location":"exercises/04_vae/vae/#exercicio-1-preparacao-dos-dados","title":"Exerc\u00edcio 1 \u2014 Prepara\u00e7\u00e3o dos Dados\u00b6","text":""},{"location":"exercises/04_vae/vae/#exercicio-2-implementacao-do-vae","title":"Exerc\u00edcio 2 \u2014 Implementa\u00e7\u00e3o do VAE\u00b6","text":""},{"location":"exercises/04_vae/vae/#exercicio-3-treinamento-do-vae","title":"Exerc\u00edcio 3 \u2014 Treinamento do VAE\u00b6","text":""},{"location":"exercises/04_vae/vae/#exercicio-4-avaliacao-reconstrucao-de-imagens","title":"Exerc\u00edcio 4 \u2014 Avalia\u00e7\u00e3o: Reconstru\u00e7\u00e3o de Imagens\u00b6","text":""},{"location":"exercises/04_vae/vae/#exercicio-5-geracao-de-novas-amostras","title":"Exerc\u00edcio 5 \u2014 Gera\u00e7\u00e3o de Novas Amostras\u00b6","text":""},{"location":"exercises/04_vae/vae/#exercicio-6-visualizacao-do-espaco-latente","title":"Exerc\u00edcio 6 \u2014 Visualiza\u00e7\u00e3o do Espa\u00e7o Latente\u00b6","text":""},{"location":"exercises/04_vae/vae/#exercicio-7-opcional-comparacao-com-autoencoder-padrao","title":"Exerc\u00edcio 7 (Opcional) \u2014 Compara\u00e7\u00e3o com Autoencoder Padr\u00e3o\u00b6","text":""},{"location":"exercises/04_vae/vae/#resultados","title":"Resultados\u00b6","text":"<p>A implementa\u00e7\u00e3o final do VAE utilizou uma arquitetura com 652.824 par\u00e2metros trein\u00e1veis, estruturada em um encoder que mapeia os 784 pixels de entrada para 400 neur\u00f4nios ocultos e subsequentemente para os par\u00e2metros \u03bc e log \u03c3\u00b2 de um espa\u00e7o latente de dimens\u00e3o 20, seguido por um decoder sim\u00e9trico que reconstr\u00f3i a imagem original a partir da amostra latente. Esta configura\u00e7\u00e3o resulta, portanto, em uma compress\u00e3o de fator 39\u00d7, equivalente a uma redu\u00e7\u00e3o dimensional de 97.4%.</p> <p>As m\u00e9tricas finais obtidas ap\u00f3s 50 \u00e9pocas de treinamento foram: loss total de 102.64, loss de reconstru\u00e7\u00e3o (BCE) de 76.81, e KL divergence de 25.82. Estes valores est\u00e3o perfeitamente alinhados com os ranges t\u00edpicos reportados na literatura para VAEs treinados no MNIST, onde valores de KL entre 20 e 30 e BCE entre 70 e 85 s\u00e3o considerados padr\u00e3o. Al\u00e9m disso, a aus\u00eancia de overfitting foi confirmada pela proximidade entre as losses de treino e valida\u00e7\u00e3o ao longo de todo o treinamento.</p> <p>A estrat\u00e9gia de beta annealing, implementada com transi\u00e7\u00e3o gradual de \u03b2=0 para \u03b2=1 ao longo das primeiras 10 \u00e9pocas, foi essencial para a estabilidade do treinamento. Esta abordagem permitiu que o modelo primeiro aprendesse a tarefa de reconstru\u00e7\u00e3o antes de impor a regulariza\u00e7\u00e3o completa do espa\u00e7o latente, resultando, consequentemente, em uma redu\u00e7\u00e3o controlada da KL divergence de 281 para 26 (uma redu\u00e7\u00e3o de 90.8%) sem as explos\u00f5es num\u00e9ricas que caracterizavam implementa\u00e7\u00f5es anteriores. Os hiperpar\u00e2metros finais escolhidos foram learning rate de 1e-3, dimens\u00e3o oculta de 400, dimens\u00e3o latente de 20, e per\u00edodo de annealing de 10 \u00e9pocas.</p> <p>A compara\u00e7\u00e3o com um autoencoder padr\u00e3o de arquitetura id\u00eantica revelou o trade-off fundamental entre reconstru\u00e7\u00e3o e capacidade generativa: o autoencoder padr\u00e3o alcan\u00e7ou loss de reconstru\u00e7\u00e3o de 64.67 versus 76.81 do VAE, uma diferen\u00e7a de 16%. No entanto, apenas o VAE consegue gerar amostras realistas a partir de pontos amostrados de N(0,I), enquanto o autoencoder produz ru\u00eddo. Este resultado quantifica precisamente o custo da capacidade generativa: sacrifica-se 16% de fidelidade de reconstru\u00e7\u00e3o para obter um espa\u00e7o latente estruturado que permite gera\u00e7\u00e3o, interpola\u00e7\u00e3o e explora\u00e7\u00e3o sistem\u00e1tica.</p> <p>A implementa\u00e7\u00e3o em PyTorch, por sua vez, trouxe benef\u00edcios significativos em rela\u00e7\u00e3o a abordagens anteriores em NumPy. Em primeiro lugar, garantiu estabilidade num\u00e9rica autom\u00e1tica para opera\u00e7\u00f5es sens\u00edveis como exponenciais e logaritmos. Em segundo lugar, forneceu backpropagation autom\u00e1tica atrav\u00e9s do autograd, eliminando a necessidade de derivadas manuais. Ademais, possibilitou acelera\u00e7\u00e3o via GPU utilizando a NVIDIA RTX 3060 Laptop dispon\u00edvel. Finalmente, ofereceu APIs de alto n\u00edvel para m\u00f3dulos e otimizadores que simplificaram significativamente o c\u00f3digo.</p> <p>Do ponto de vista de insights pr\u00e1ticos, observamos que: (1) beta annealing \u00e9 crucial para evitar tanto o colapso do espa\u00e7o latente (KL\u21920) quanto explos\u00f5es num\u00e9ricas; (2) acelera\u00e7\u00e3o por GPU viabilizou o uso de batch_size=128, acelerando significativamente o treinamento; (3) a dimens\u00e3o latente de 20 mostrou-se suficiente para capturar a variabilidade de 10 classes de d\u00edgitos; (4) o VAE \u00e9 necess\u00e1rio para aplica\u00e7\u00f5es generativas, mas um autoencoder padr\u00e3o \u00e9 suficiente quando apenas compress\u00e3o \u00e9 requerida.</p> <p>As aplica\u00e7\u00f5es pr\u00e1ticas deste modelo incluem, entre outras: gera\u00e7\u00e3o de dados sint\u00e9ticos para aumento de datasets; detec\u00e7\u00e3o de anomalias atrav\u00e9s da an\u00e1lise de amostras com alta loss de reconstru\u00e7\u00e3o; aprendizado de representa\u00e7\u00f5es compactas (embeddings de 20 dimens\u00f5es) para tarefas downstream; interpola\u00e7\u00e3o no espa\u00e7o latente para criar transi\u00e7\u00f5es suaves entre amostras; e compress\u00e3o de dados com estrutura probabil\u00edstica interpret\u00e1vel.</p> <p>Finalmente, extens\u00f5es naturais deste trabalho incluem a implementa\u00e7\u00e3o de Conditional VAEs (CVAE) para gera\u00e7\u00e3o condicionada em labels espec\u00edficos, permitindo controle direto sobre a classe do d\u00edgito gerado; \u03b2-VAEs com controle expl\u00edcito do par\u00e2metro \u03b2 para explorar diferentes pontos no espectro entre reconstru\u00e7\u00e3o e regulariza\u00e7\u00e3o; e Hierarchical VAEs (HVAE) com m\u00faltiplas camadas latentes para capturar estruturas de depend\u00eancia mais complexas nos dados.</p>"},{"location":"projects/generative/generative/","title":"Projeto 3: Modelos Generativos - Stable Diffusion + ControlNet","text":"In\u00a0[1]: Copied! <pre># Fun\u00e7\u00e3o para renderizar diagramas Mermaid no Jupyter/MkDocs\nfrom IPython.display import Image as IPImage, display\nimport base64\n\ndef render_mermaid(mermaid_code, width=800):\n    \"\"\"\n    Renderiza diagrama Mermaid usando mermaid.ink API\n    \"\"\"\n    mermaid_clean = mermaid_code.strip()\n    graphbytes = mermaid_clean.encode(\"utf8\")\n    base64_bytes = base64.b64encode(graphbytes)\n    base64_string = base64_bytes.decode(\"ascii\")\n    url = f\"https://mermaid.ink/img/{base64_string}\"\n    \n    try:\n        display(IPImage(url=url, width=width))\n    except Exception as e:\n        print(f\"Erro ao renderizar diagrama: {e}\")\n        print(f\"\\nC\u00f3digo Mermaid:\\n{mermaid_code}\")\n</pre> # Fun\u00e7\u00e3o para renderizar diagramas Mermaid no Jupyter/MkDocs from IPython.display import Image as IPImage, display import base64  def render_mermaid(mermaid_code, width=800):     \"\"\"     Renderiza diagrama Mermaid usando mermaid.ink API     \"\"\"     mermaid_clean = mermaid_code.strip()     graphbytes = mermaid_clean.encode(\"utf8\")     base64_bytes = base64.b64encode(graphbytes)     base64_string = base64_bytes.decode(\"ascii\")     url = f\"https://mermaid.ink/img/{base64_string}\"          try:         display(IPImage(url=url, width=width))     except Exception as e:         print(f\"Erro ao renderizar diagrama: {e}\")         print(f\"\\nC\u00f3digo Mermaid:\\n{mermaid_code}\") In\u00a0[2]: Copied! <pre>mermaid_sd = \"\"\"\ngraph LR\n    A[Text Prompt] --&gt; B[CLIP Text Encoder]\n    B --&gt; C[Text Embeddings]\n    D[Random Noise] --&gt; E[U-Net]\n    C --&gt; E\n    E --&gt; F[Denoised Latent]\n    F --&gt; G[VAE Decoder]\n    G --&gt; H[Generated Image]\n    \n    style A fill:#e1f5ff\n    style H fill:#c8e6c9\n    style E fill:#fff9c4\n\"\"\"\nrender_mermaid(mermaid_sd, width=700)\n</pre> mermaid_sd = \"\"\" graph LR     A[Text Prompt] --&gt; B[CLIP Text Encoder]     B --&gt; C[Text Embeddings]     D[Random Noise] --&gt; E[U-Net]     C --&gt; E     E --&gt; F[Denoised Latent]     F --&gt; G[VAE Decoder]     G --&gt; H[Generated Image]          style A fill:#e1f5ff     style H fill:#c8e6c9     style E fill:#fff9c4 \"\"\" render_mermaid(mermaid_sd, width=700) In\u00a0[3]: Copied! <pre>mermaid_controlnet = \"\"\"\ngraph TB\n    A[Input Image] --&gt; B[Condition Extractor]\n    B --&gt; C[Canny Edges / Depth Map]\n    C --&gt; D[ControlNet Encoder]\n    E[Text + Noise] --&gt; F[U-Net Original]\n    D --&gt; G[Zero Convolution]\n    G --&gt; F\n    F --&gt; H[Output Latent]\n    \n    style C fill:#ffccbc\n    style D fill:#b3e5fc\n    style F fill:#fff9c4\n\"\"\"\nrender_mermaid(mermaid_controlnet, width=600)\n</pre> mermaid_controlnet = \"\"\" graph TB     A[Input Image] --&gt; B[Condition Extractor]     B --&gt; C[Canny Edges / Depth Map]     C --&gt; D[ControlNet Encoder]     E[Text + Noise] --&gt; F[U-Net Original]     D --&gt; G[Zero Convolution]     G --&gt; F     F --&gt; H[Output Latent]          style C fill:#ffccbc     style D fill:#b3e5fc     style F fill:#fff9c4 \"\"\" render_mermaid(mermaid_controlnet, width=600) In\u00a0[4]: Copied! <pre>mermaid_pipeline = \"\"\"\ngraph TB\n    subgraph Input\n        A[Text Prompt]\n        B[Control Image]\n    end\n    \n    subgraph Text_Processing\n        C[CLIP Encoder]\n        D[Text Embeddings]\n    end\n    \n    subgraph Control_Processing\n        E[Canny/Depth Extractor]\n        F[ControlNet Encoder]\n    end\n    \n    subgraph Diffusion_Process\n        G[Random Noise z_T]\n        H[U-Net + Cross-Attention]\n        I[Denoised Latent z_0]\n    end\n    \n    subgraph Image_Decoding\n        J[VAE Decoder]\n        K[Generated Image]\n    end\n    \n    A --&gt; C --&gt; D --&gt; H\n    B --&gt; E --&gt; F --&gt; H\n    G --&gt; H --&gt; I --&gt; J --&gt; K\n    \n    style A fill:#e1f5ff\n    style B fill:#ffe1e1\n    style K fill:#c8e6c9\n    style H fill:#fff9c4\n\"\"\"\nrender_mermaid(mermaid_pipeline, width=800)\n</pre> mermaid_pipeline = \"\"\" graph TB     subgraph Input         A[Text Prompt]         B[Control Image]     end          subgraph Text_Processing         C[CLIP Encoder]         D[Text Embeddings]     end          subgraph Control_Processing         E[Canny/Depth Extractor]         F[ControlNet Encoder]     end          subgraph Diffusion_Process         G[Random Noise z_T]         H[U-Net + Cross-Attention]         I[Denoised Latent z_0]     end          subgraph Image_Decoding         J[VAE Decoder]         K[Generated Image]     end          A --&gt; C --&gt; D --&gt; H     B --&gt; E --&gt; F --&gt; H     G --&gt; H --&gt; I --&gt; J --&gt; K          style A fill:#e1f5ff     style B fill:#ffe1e1     style K fill:#c8e6c9     style H fill:#fff9c4 \"\"\" render_mermaid(mermaid_pipeline, width=800) In\u00a0[5]: Copied! <pre>!pip install -q \"pandas&gt;=2.2.0\" \"scikit-learn&gt;=1.5.0\" --upgrade\n!pip install -q diffusers[torch] transformers accelerate controlnet_aux opencv-python matplotlib pillow\n</pre> !pip install -q \"pandas&gt;=2.2.0\" \"scikit-learn&gt;=1.5.0\" --upgrade !pip install -q diffusers[torch] transformers accelerate controlnet_aux opencv-python matplotlib pillow In\u00a0[6]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore', category=FutureWarning, module='timm')\nwarnings.filterwarnings('ignore', category=UserWarning, module='controlnet_aux')\nwarnings.filterwarnings('ignore', message='IProgress not found')\nwarnings.filterwarnings('ignore', message='mediapipe')\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport torch\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom diffusers import (\n    StableDiffusionPipeline,\n    StableDiffusionControlNetPipeline,\n    ControlNetModel,\n    UniPCMultistepScheduler\n)\nfrom controlnet_aux import CannyDetector\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")\nif device == \"cuda\":\n    torch.backends.cudnn.benchmark = True\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n</pre> import warnings warnings.filterwarnings('ignore', category=FutureWarning, module='timm') warnings.filterwarnings('ignore', category=UserWarning, module='controlnet_aux') warnings.filterwarnings('ignore', message='IProgress not found') warnings.filterwarnings('ignore', message='mediapipe')  import numpy as np import pandas as pd import sklearn import torch import cv2 from PIL import Image import matplotlib.pyplot as plt from diffusers import (     StableDiffusionPipeline,     StableDiffusionControlNetPipeline,     ControlNetModel,     UniPCMultistepScheduler ) from controlnet_aux import CannyDetector  device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Device: {device}\") if device == \"cuda\":     torch.backends.cudnn.benchmark = True     print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\") <pre>Device: cuda\nGPU Memory: 6.0 GB\n</pre> In\u00a0[7]: Copied! <pre>import os\nimport gc\nos.environ['HF_HOME'] = '/home/gabrielmmh/.cache/huggingface'\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\n\npipe_sd = StableDiffusionPipeline.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    variant=\"fp16\",\n    safety_checker=None\n)\npipe_sd = pipe_sd.to(device)\npipe_sd.enable_attention_slicing(1)\npipe_sd.enable_vae_slicing()\n\ntorch.cuda.empty_cache()\ngc.collect()\nprint(f\"SD 1.5 loaded | GPU: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\")\n</pre> import os import gc os.environ['HF_HOME'] = '/home/gabrielmmh/.cache/huggingface'  model_id = \"runwayml/stable-diffusion-v1-5\"  pipe_sd = StableDiffusionPipeline.from_pretrained(     model_id,     torch_dtype=torch.float16,     low_cpu_mem_usage=True,     variant=\"fp16\",     safety_checker=None ) pipe_sd = pipe_sd.to(device) pipe_sd.enable_attention_slicing(1) pipe_sd.enable_vae_slicing()  torch.cuda.empty_cache() gc.collect() print(f\"SD 1.5 loaded | GPU: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\") <pre>Loading pipeline components...:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>You have disabled the safety checker for &lt;class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'&gt; by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n</pre> <pre>SD 1.5 loaded | GPU: 2057MB\n</pre> In\u00a0[8]: Copied! <pre>def generate_basic_image(prompt, negative_prompt=\"\", num_inference_steps=25, guidance_scale=7.5, seed=None):\n    \"\"\"\n    Gera imagem usando Stable Diffusion b\u00e1sico (otimizado para mem\u00f3ria)\n    \n    Args:\n        prompt: Descri\u00e7\u00e3o do que gerar\n        negative_prompt: O que evitar\n        num_inference_steps: Passos de denoising (reduzido para 25 para economizar mem\u00f3ria)\n        guidance_scale: For\u00e7a do condicionamento textual (7-15 recomendado)\n        seed: Seed para reprodutibilidade\n    \"\"\"\n    generator = torch.Generator(device=device).manual_seed(seed) if seed else None\n    \n    image = pipe_sd(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        generator=generator\n    ).images[0]\n    \n    # Limpar mem\u00f3ria ap\u00f3s gera\u00e7\u00e3o\n    torch.cuda.empty_cache()\n    \n    return image\n</pre> def generate_basic_image(prompt, negative_prompt=\"\", num_inference_steps=25, guidance_scale=7.5, seed=None):     \"\"\"     Gera imagem usando Stable Diffusion b\u00e1sico (otimizado para mem\u00f3ria)          Args:         prompt: Descri\u00e7\u00e3o do que gerar         negative_prompt: O que evitar         num_inference_steps: Passos de denoising (reduzido para 25 para economizar mem\u00f3ria)         guidance_scale: For\u00e7a do condicionamento textual (7-15 recomendado)         seed: Seed para reprodutibilidade     \"\"\"     generator = torch.Generator(device=device).manual_seed(seed) if seed else None          image = pipe_sd(         prompt=prompt,         negative_prompt=negative_prompt,         num_inference_steps=num_inference_steps,         guidance_scale=guidance_scale,         generator=generator     ).images[0]          # Limpar mem\u00f3ria ap\u00f3s gera\u00e7\u00e3o     torch.cuda.empty_cache()          return image In\u00a0[9]: Copied! <pre>prompt_1 = \"modern minimalist chair design, sleek wooden legs, ergonomic seat, product photography, white background, studio lighting, high quality, 4k\"\nnegative_1 = \"blurry, low quality, distorted, ugly, bad anatomy\"\n\nimage_1 = generate_basic_image(prompt_1, negative_1, num_inference_steps=25, guidance_scale=7.5, seed=42)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(image_1)\nplt.axis('off')\nplt.title(\"Cadeira Moderna (SD B\u00e1sico)\\nSteps=25, Guidance=7.5\")\nplt.tight_layout()\nplt.show()\n</pre> prompt_1 = \"modern minimalist chair design, sleek wooden legs, ergonomic seat, product photography, white background, studio lighting, high quality, 4k\" negative_1 = \"blurry, low quality, distorted, ugly, bad anatomy\"  image_1 = generate_basic_image(prompt_1, negative_1, num_inference_steps=25, guidance_scale=7.5, seed=42)  plt.figure(figsize=(8, 8)) plt.imshow(image_1) plt.axis('off') plt.title(\"Cadeira Moderna (SD B\u00e1sico)\\nSteps=25, Guidance=7.5\") plt.tight_layout() plt.show() <pre>  0%|          | 0/25 [00:00&lt;?, ?it/s]</pre> In\u00a0[10]: Copied! <pre>prompt_2 = \"futuristic smartwatch design, OLED display, titanium band, premium materials, product render, professional lighting\"\nnegative_2 = \"blurry, low quality, cartoon, sketch\"\n\nimage_2 = generate_basic_image(prompt_2, negative_2, num_inference_steps=25, guidance_scale=7.5, seed=123)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(image_2)\nplt.axis('off')\nplt.title(\"Smartwatch Futurista\\nSteps=25, Guidance=7.5\")\nplt.tight_layout()\nplt.show()\n</pre> prompt_2 = \"futuristic smartwatch design, OLED display, titanium band, premium materials, product render, professional lighting\" negative_2 = \"blurry, low quality, cartoon, sketch\"  image_2 = generate_basic_image(prompt_2, negative_2, num_inference_steps=25, guidance_scale=7.5, seed=123)  plt.figure(figsize=(8, 8)) plt.imshow(image_2) plt.axis('off') plt.title(\"Smartwatch Futurista\\nSteps=25, Guidance=7.5\") plt.tight_layout() plt.show() <pre>  0%|          | 0/25 [00:00&lt;?, ?it/s]</pre> In\u00a0[11]: Copied! <pre>torch.cuda.empty_cache()\ngc.collect()\n\ncontrolnet = ControlNetModel.from_pretrained(\n    \"lllyasviel/sd-controlnet-canny\",\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True\n)\n\npipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(\n    model_id,\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    variant=\"fp16\",\n    safety_checker=None\n)\npipe_controlnet = pipe_controlnet.to(device)\npipe_controlnet.enable_attention_slicing(1)\npipe_controlnet.enable_vae_slicing()\npipe_controlnet.scheduler = UniPCMultistepScheduler.from_config(pipe_controlnet.scheduler.config)\n\ncanny_detector = CannyDetector()\n\ntorch.cuda.empty_cache()\ngc.collect()\nprint(f\"ControlNet loaded | GPU: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\")\n</pre> torch.cuda.empty_cache() gc.collect()  controlnet = ControlNetModel.from_pretrained(     \"lllyasviel/sd-controlnet-canny\",     torch_dtype=torch.float16,     low_cpu_mem_usage=True )  pipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(     model_id,     controlnet=controlnet,     torch_dtype=torch.float16,     low_cpu_mem_usage=True,     variant=\"fp16\",     safety_checker=None ) pipe_controlnet = pipe_controlnet.to(device) pipe_controlnet.enable_attention_slicing(1) pipe_controlnet.enable_vae_slicing() pipe_controlnet.scheduler = UniPCMultistepScheduler.from_config(pipe_controlnet.scheduler.config)  canny_detector = CannyDetector()  torch.cuda.empty_cache() gc.collect() print(f\"ControlNet loaded | GPU: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\") <pre>Loading pipeline components...:   0%|          | 0/6 [00:00&lt;?, ?it/s]</pre> <pre>You have disabled the safety checker for &lt;class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'&gt; by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n</pre> <pre>ControlNet loaded | GPU: 4829MB\n</pre> In\u00a0[12]: Copied! <pre>def create_canny_condition(image, low_threshold=100, high_threshold=200):\n    \"\"\"Cria condi\u00e7\u00e3o Canny a partir de uma imagem\"\"\"\n    if isinstance(image, Image.Image):\n        image = np.array(image)\n    \n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    \n    edges = cv2.Canny(gray, low_threshold, high_threshold)\n    edges = edges[:, :, None]\n    edges = np.concatenate([edges, edges, edges], axis=2)\n    \n    return Image.fromarray(edges)\n\n\ndef create_simple_sketch(shape=(512, 512), sketch_type=\"chair\"):\n    \"\"\"Cria sketch para usar como condi\u00e7\u00e3o do ControlNet\"\"\"\n    img = np.ones((*shape, 3), dtype=np.uint8) * 255\n    \n    if sketch_type == \"chair\":\n        # Cadeira moderna de escrit\u00f3rio com base girat\u00f3ria\n        cx, cy = 256, 256  # Centro do canvas\n        \n        # Base com rod\u00edzios (estrela 5 pontas)\n        base_y = 420\n        for i in range(5):\n            angle = np.radians(i * 72 - 90)\n            x_end = int(cx + 80 * np.cos(angle))\n            y_end = int(base_y + 25 * np.sin(angle))\n            cv2.line(img, (cx, base_y), (x_end, y_end), (0, 0, 0), 2)\n            cv2.circle(img, (x_end, y_end + 8), 6, (0, 0, 0), 2)  # Rod\u00edzios\n        \n        # Coluna central\n        cv2.line(img, (cx, base_y), (cx, 340), (0, 0, 0), 3)\n        \n        # Assento com estofado (elipse para volume)\n        cv2.ellipse(img, (cx, 320), (70, 25), 0, 0, 360, (0, 0, 0), 2)\n        cv2.ellipse(img, (cx, 315), (65, 20), 0, 180, 360, (0, 0, 0), 1)  # Linha de estofado\n        \n        # Encosto ergon\u00f4mico curvo\n        pts_back = np.array([\n            [cx - 55, 310], [cx - 60, 250], [cx - 55, 180], \n            [cx - 40, 140], [cx, 130], [cx + 40, 140],\n            [cx + 55, 180], [cx + 60, 250], [cx + 55, 310]\n        ], np.int32)\n        cv2.polylines(img, [pts_back], False, (0, 0, 0), 2)\n        \n        # Detalhes do estofado no encosto\n        cv2.ellipse(img, (cx, 220), (45, 60), 0, 0, 360, (0, 0, 0), 1)\n        \n        # Bra\u00e7os\n        # Bra\u00e7o esquerdo\n        cv2.line(img, (cx - 70, 280), (cx - 100, 270), (0, 0, 0), 2)\n        cv2.line(img, (cx - 100, 270), (cx - 100, 260), (0, 0, 0), 2)\n        cv2.line(img, (cx - 100, 260), (cx - 75, 255), (0, 0, 0), 2)\n        # Bra\u00e7o direito\n        cv2.line(img, (cx + 70, 280), (cx + 100, 270), (0, 0, 0), 2)\n        cv2.line(img, (cx + 100, 270), (cx + 100, 260), (0, 0, 0), 2)\n        cv2.line(img, (cx + 100, 260), (cx + 75, 255), (0, 0, 0), 2)\n        \n        # Suportes dos bra\u00e7os\n        cv2.line(img, (cx - 70, 320), (cx - 70, 280), (0, 0, 0), 2)\n        cv2.line(img, (cx + 70, 320), (cx + 70, 280), (0, 0, 0), 2)\n        \n    elif sketch_type == \"watch\":\n        cv2.circle(img, (256, 256), 100, (0, 0, 0), 2)\n        cv2.rectangle(img, (240, 100), (272, 156), (0, 0, 0), 2)\n        cv2.rectangle(img, (240, 356), (272, 412), (0, 0, 0), 2)\n        \n    elif sketch_type == \"bottle\":\n        cv2.rectangle(img, (220, 100), (292, 150), (0, 0, 0), 2)\n        cv2.rectangle(img, (200, 150), (312, 450), (0, 0, 0), 2)\n        \n    return Image.fromarray(img)\n\n\ndef generate_controlnet_image(prompt, canny_image, negative_prompt=\"\", \n                             num_inference_steps=20, guidance_scale=7.5, \n                             controlnet_conditioning_scale=1.0, seed=None):\n    \"\"\"Gera imagem usando ControlNet + Stable Diffusion\"\"\"\n    generator = torch.Generator(device=device).manual_seed(seed) if seed else None\n    \n    image = pipe_controlnet(\n        prompt=prompt,\n        image=canny_image,\n        negative_prompt=negative_prompt,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        controlnet_conditioning_scale=controlnet_conditioning_scale,\n        generator=generator\n    ).images[0]\n    \n    torch.cuda.empty_cache()\n    return image\n</pre> def create_canny_condition(image, low_threshold=100, high_threshold=200):     \"\"\"Cria condi\u00e7\u00e3o Canny a partir de uma imagem\"\"\"     if isinstance(image, Image.Image):         image = np.array(image)          if len(image.shape) == 3:         gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)     else:         gray = image          edges = cv2.Canny(gray, low_threshold, high_threshold)     edges = edges[:, :, None]     edges = np.concatenate([edges, edges, edges], axis=2)          return Image.fromarray(edges)   def create_simple_sketch(shape=(512, 512), sketch_type=\"chair\"):     \"\"\"Cria sketch para usar como condi\u00e7\u00e3o do ControlNet\"\"\"     img = np.ones((*shape, 3), dtype=np.uint8) * 255          if sketch_type == \"chair\":         # Cadeira moderna de escrit\u00f3rio com base girat\u00f3ria         cx, cy = 256, 256  # Centro do canvas                  # Base com rod\u00edzios (estrela 5 pontas)         base_y = 420         for i in range(5):             angle = np.radians(i * 72 - 90)             x_end = int(cx + 80 * np.cos(angle))             y_end = int(base_y + 25 * np.sin(angle))             cv2.line(img, (cx, base_y), (x_end, y_end), (0, 0, 0), 2)             cv2.circle(img, (x_end, y_end + 8), 6, (0, 0, 0), 2)  # Rod\u00edzios                  # Coluna central         cv2.line(img, (cx, base_y), (cx, 340), (0, 0, 0), 3)                  # Assento com estofado (elipse para volume)         cv2.ellipse(img, (cx, 320), (70, 25), 0, 0, 360, (0, 0, 0), 2)         cv2.ellipse(img, (cx, 315), (65, 20), 0, 180, 360, (0, 0, 0), 1)  # Linha de estofado                  # Encosto ergon\u00f4mico curvo         pts_back = np.array([             [cx - 55, 310], [cx - 60, 250], [cx - 55, 180],              [cx - 40, 140], [cx, 130], [cx + 40, 140],             [cx + 55, 180], [cx + 60, 250], [cx + 55, 310]         ], np.int32)         cv2.polylines(img, [pts_back], False, (0, 0, 0), 2)                  # Detalhes do estofado no encosto         cv2.ellipse(img, (cx, 220), (45, 60), 0, 0, 360, (0, 0, 0), 1)                  # Bra\u00e7os         # Bra\u00e7o esquerdo         cv2.line(img, (cx - 70, 280), (cx - 100, 270), (0, 0, 0), 2)         cv2.line(img, (cx - 100, 270), (cx - 100, 260), (0, 0, 0), 2)         cv2.line(img, (cx - 100, 260), (cx - 75, 255), (0, 0, 0), 2)         # Bra\u00e7o direito         cv2.line(img, (cx + 70, 280), (cx + 100, 270), (0, 0, 0), 2)         cv2.line(img, (cx + 100, 270), (cx + 100, 260), (0, 0, 0), 2)         cv2.line(img, (cx + 100, 260), (cx + 75, 255), (0, 0, 0), 2)                  # Suportes dos bra\u00e7os         cv2.line(img, (cx - 70, 320), (cx - 70, 280), (0, 0, 0), 2)         cv2.line(img, (cx + 70, 320), (cx + 70, 280), (0, 0, 0), 2)              elif sketch_type == \"watch\":         cv2.circle(img, (256, 256), 100, (0, 0, 0), 2)         cv2.rectangle(img, (240, 100), (272, 156), (0, 0, 0), 2)         cv2.rectangle(img, (240, 356), (272, 412), (0, 0, 0), 2)              elif sketch_type == \"bottle\":         cv2.rectangle(img, (220, 100), (292, 150), (0, 0, 0), 2)         cv2.rectangle(img, (200, 150), (312, 450), (0, 0, 0), 2)              return Image.fromarray(img)   def generate_controlnet_image(prompt, canny_image, negative_prompt=\"\",                               num_inference_steps=20, guidance_scale=7.5,                               controlnet_conditioning_scale=1.0, seed=None):     \"\"\"Gera imagem usando ControlNet + Stable Diffusion\"\"\"     generator = torch.Generator(device=device).manual_seed(seed) if seed else None          image = pipe_controlnet(         prompt=prompt,         image=canny_image,         negative_prompt=negative_prompt,         num_inference_steps=num_inference_steps,         guidance_scale=guidance_scale,         controlnet_conditioning_scale=controlnet_conditioning_scale,         generator=generator     ).images[0]          torch.cuda.empty_cache()     return image In\u00a0[13]: Copied! <pre># Criar sketch de cadeira\nsketch_chair = create_simple_sketch(sketch_type=\"chair\")\ncanny_chair = create_canny_condition(sketch_chair, low_threshold=50, high_threshold=150)\n\nprompt_3 = \"luxury leather office chair, ergonomic design, chrome base, professional product photography, 4k, high detail\"\nnegative_3 = \"blurry, low quality, cartoon, distorted\"\n\nimage_3 = generate_controlnet_image(\n    prompt_3, \n    canny_chair, \n    negative_3,\n    num_inference_steps=20,\n    guidance_scale=8.0,\n    controlnet_conditioning_scale=1.2,\n    seed=456\n)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\naxes[0].imshow(sketch_chair)\naxes[0].set_title(\"Sketch Original\")\naxes[0].axis('off')\n\naxes[1].imshow(canny_chair)\naxes[1].set_title(\"Canny Edges (Condi\u00e7\u00e3o)\")\naxes[1].axis('off')\n\naxes[2].imshow(image_3)\naxes[2].set_title(\"Resultado Final (ControlNet)\")\naxes[2].axis('off')\n\nplt.suptitle(\"Cadeira Controlada - ControlNet (Steps=20)\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> # Criar sketch de cadeira sketch_chair = create_simple_sketch(sketch_type=\"chair\") canny_chair = create_canny_condition(sketch_chair, low_threshold=50, high_threshold=150)  prompt_3 = \"luxury leather office chair, ergonomic design, chrome base, professional product photography, 4k, high detail\" negative_3 = \"blurry, low quality, cartoon, distorted\"  image_3 = generate_controlnet_image(     prompt_3,      canny_chair,      negative_3,     num_inference_steps=20,     guidance_scale=8.0,     controlnet_conditioning_scale=1.2,     seed=456 )  fig, axes = plt.subplots(1, 3, figsize=(18, 6)) axes[0].imshow(sketch_chair) axes[0].set_title(\"Sketch Original\") axes[0].axis('off')  axes[1].imshow(canny_chair) axes[1].set_title(\"Canny Edges (Condi\u00e7\u00e3o)\") axes[1].axis('off')  axes[2].imshow(image_3) axes[2].set_title(\"Resultado Final (ControlNet)\") axes[2].axis('off')  plt.suptitle(\"Cadeira Controlada - ControlNet (Steps=20)\", fontsize=14) plt.tight_layout() plt.show() <pre>  0%|          | 0/20 [00:00&lt;?, ?it/s]</pre> In\u00a0[14]: Copied! <pre>sketch_watch = create_simple_sketch(sketch_type=\"watch\")\ncanny_watch = create_canny_condition(sketch_watch)\n\nprompt_watch = \"luxury swiss watch, gold case, leather strap, mechanical movement, classic elegant design, professional photography\"\nnegative_watch = \"blurry, low quality, distorted, toy, digital\"\n\nimage_watch = generate_controlnet_image(\n    prompt_watch,\n    canny_watch,\n    negative_watch,\n    num_inference_steps=20,\n    guidance_scale=7.5,\n    controlnet_conditioning_scale=1.0,\n    seed=1000\n)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\naxes[0].imshow(sketch_watch)\naxes[0].set_title(\"Sketch Base\")\naxes[0].axis('off')\n\naxes[1].imshow(canny_watch)\naxes[1].set_title(\"Canny Edges\")\naxes[1].axis('off')\n\naxes[2].imshow(image_watch)\naxes[2].set_title(\"Rel\u00f3gio Cl\u00e1ssico Su\u00ed\u00e7o\")\naxes[2].axis('off')\n\nplt.suptitle(\"Design de Rel\u00f3gio - Controle Estrutural com ControlNet\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> sketch_watch = create_simple_sketch(sketch_type=\"watch\") canny_watch = create_canny_condition(sketch_watch)  prompt_watch = \"luxury swiss watch, gold case, leather strap, mechanical movement, classic elegant design, professional photography\" negative_watch = \"blurry, low quality, distorted, toy, digital\"  image_watch = generate_controlnet_image(     prompt_watch,     canny_watch,     negative_watch,     num_inference_steps=20,     guidance_scale=7.5,     controlnet_conditioning_scale=1.0,     seed=1000 )  fig, axes = plt.subplots(1, 3, figsize=(18, 6)) axes[0].imshow(sketch_watch) axes[0].set_title(\"Sketch Base\") axes[0].axis('off')  axes[1].imshow(canny_watch) axes[1].set_title(\"Canny Edges\") axes[1].axis('off')  axes[2].imshow(image_watch) axes[2].set_title(\"Rel\u00f3gio Cl\u00e1ssico Su\u00ed\u00e7o\") axes[2].axis('off')  plt.suptitle(\"Design de Rel\u00f3gio - Controle Estrutural com ControlNet\", fontsize=14) plt.tight_layout() plt.show() <pre>  0%|          | 0/20 [00:00&lt;?, ?it/s]</pre> In\u00a0[15]: Copied! <pre>sketch_bottle = create_simple_sketch(sketch_type=\"bottle\")\ncanny_bottle = create_canny_condition(sketch_bottle)\n\nprompt_bottle = \"modern stainless steel water bottle, sleek minimalist design, matte black finish, sport cap, premium product photography, white background\"\nnegative_bottle = \"blurry, low quality, distorted, plastic, cheap\"\n\nimage_bottle = generate_controlnet_image(\n    prompt_bottle,\n    canny_bottle,\n    negative_bottle,\n    num_inference_steps=20,\n    guidance_scale=7.5,\n    controlnet_conditioning_scale=1.1,\n    seed=2000\n)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\naxes[0].imshow(sketch_bottle)\naxes[0].set_title(\"Sketch Base\")\naxes[0].axis('off')\n\naxes[1].imshow(canny_bottle)\naxes[1].set_title(\"Canny Edges\")\naxes[1].axis('off')\n\naxes[2].imshow(image_bottle)\naxes[2].set_title(\"Garrafa de \u00c1gua Premium\")\naxes[2].axis('off')\n\nplt.suptitle(\"Design de Garrafa - ControlNet com Formas Verticais\", fontsize=14)\nplt.tight_layout()\nplt.show()\n</pre> sketch_bottle = create_simple_sketch(sketch_type=\"bottle\") canny_bottle = create_canny_condition(sketch_bottle)  prompt_bottle = \"modern stainless steel water bottle, sleek minimalist design, matte black finish, sport cap, premium product photography, white background\" negative_bottle = \"blurry, low quality, distorted, plastic, cheap\"  image_bottle = generate_controlnet_image(     prompt_bottle,     canny_bottle,     negative_bottle,     num_inference_steps=20,     guidance_scale=7.5,     controlnet_conditioning_scale=1.1,     seed=2000 )  fig, axes = plt.subplots(1, 3, figsize=(18, 6)) axes[0].imshow(sketch_bottle) axes[0].set_title(\"Sketch Base\") axes[0].axis('off')  axes[1].imshow(canny_bottle) axes[1].set_title(\"Canny Edges\") axes[1].axis('off')  axes[2].imshow(image_bottle) axes[2].set_title(\"Garrafa de \u00c1gua Premium\") axes[2].axis('off')  plt.suptitle(\"Design de Garrafa - ControlNet com Formas Verticais\", fontsize=14) plt.tight_layout() plt.show() <pre>  0%|          | 0/20 [00:00&lt;?, ?it/s]</pre> In\u00a0[16]: Copied! <pre>comparison_data = {\n    \"Exemplo\": [\"1. Cadeira Moderna\", \"2. Smartwatch\", \"3. Cadeira ControlNet\", \"4. Rel\u00f3gio\", \"5. Garrafa\"],\n    \"T\u00e9cnica\": [\"SD B\u00e1sico\", \"SD B\u00e1sico\", \"SD + ControlNet\", \"SD + ControlNet\", \"SD + ControlNet\"],\n    \"Steps\": [25, 25, 20, 20, 20],\n    \"Guidance\": [7.5, 7.5, 8.0, 7.5, 7.5],\n    \"Control Scale\": [\"-\", \"-\", 1.2, 1.0, 1.1]\n}\npd.DataFrame(comparison_data)\n</pre> comparison_data = {     \"Exemplo\": [\"1. Cadeira Moderna\", \"2. Smartwatch\", \"3. Cadeira ControlNet\", \"4. Rel\u00f3gio\", \"5. Garrafa\"],     \"T\u00e9cnica\": [\"SD B\u00e1sico\", \"SD B\u00e1sico\", \"SD + ControlNet\", \"SD + ControlNet\", \"SD + ControlNet\"],     \"Steps\": [25, 25, 20, 20, 20],     \"Guidance\": [7.5, 7.5, 8.0, 7.5, 7.5],     \"Control Scale\": [\"-\", \"-\", 1.2, 1.0, 1.1] } pd.DataFrame(comparison_data) Out[16]: Exemplo T\u00e9cnica Steps Guidance Control Scale 0 1. Cadeira Moderna SD B\u00e1sico 25 7.5 - 1 2. Smartwatch SD B\u00e1sico 25 7.5 - 2 3. Cadeira ControlNet SD + ControlNet 20 8.0 1.2 3 4. Rel\u00f3gio SD + ControlNet 20 7.5 1.0 4 5. Garrafa SD + ControlNet 20 7.5 1.1"},{"location":"projects/generative/generative/#projeto-3-modelos-generativos-stable-diffusion-controlnet","title":"Projeto 3: Modelos Generativos - Stable Diffusion + ControlNet\u00b6","text":"<p>Autores: Caio Boa, Gabriel Hermida e Pedro Civita T\u00e9cnicas: Stable Diffusion 1.5 + ControlNet Canny</p>"},{"location":"projects/generative/generative/#introducao","title":"Introdu\u00e7\u00e3o\u00b6","text":"<p>Este projeto utiliza Stable Diffusion como modelo base e ControlNet como t\u00e9cnica de controle espacial para gera\u00e7\u00e3o de designs de produtos. A combina\u00e7\u00e3o dessas t\u00e9cnicas permite criar imagens a partir de descri\u00e7\u00f5es textuais enquanto mant\u00e9m controle sobre a estrutura espacial atrav\u00e9s de condi\u00e7\u00f5es visuais.</p>"},{"location":"projects/generative/generative/#objetivos","title":"Objetivos\u00b6","text":"<ol> <li>Implementar pipeline text-to-image com Stable Diffusion 1.5</li> <li>Integrar ControlNet Canny para controle via detec\u00e7\u00e3o de bordas</li> <li>Gerar exemplos de designs variando par\u00e2metros</li> <li>Documentar arquitetura com diagramas</li> <li>Analisar impacto de hiperpar\u00e2metros</li> </ol>"},{"location":"projects/generative/generative/#tecnologias","title":"Tecnologias\u00b6","text":"<ul> <li>Diffusers: Framework para modelos de difus\u00e3o</li> <li>Stable Diffusion 1.5: Modelo text-to-image (runwayml/stable-diffusion-v1-5)</li> <li>ControlNet Canny: Controle espacial (lllyasviel/sd-controlnet-canny)</li> <li>PyTorch: Backend com acelera\u00e7\u00e3o CUDA</li> </ul>"},{"location":"projects/generative/generative/#1-arquitetura-dos-modelos","title":"1. Arquitetura dos Modelos\u00b6","text":"<p>Stable Diffusion \u00e9 um modelo de difus\u00e3o latente que opera no espa\u00e7o comprimido de um autoencoder. Seus tr\u00eas componentes principais s\u00e3o: um VAE que comprime imagens 512\u00d7512 para latentes 64\u00d764\u00d74, reduzindo o custo computacional; um encoder CLIP que transforma prompts textuais em embeddings de 768 dimens\u00f5es; e um U-Net que prediz e remove ru\u00eddo iterativamente, condicionado pelo texto via cross-attention.</p> <p>ControlNet estende o Stable Diffusion adicionando controle espacial atrav\u00e9s de condi\u00e7\u00f5es visuais como bordas Canny, mapas de profundidade ou poses. A arquitetura utiliza uma c\u00f3pia trein\u00e1vel do U-Net que processa a condi\u00e7\u00e3o em paralelo, conectada ao modelo original por Zero Convolutions\u2014camadas inicializadas com zeros que permitem treinamento est\u00e1vel. Neste projeto, o ControlNet Canny controla as bordas dos designs gerados.</p>"},{"location":"projects/generative/generative/#diagramas-da-arquitetura","title":"Diagramas da Arquitetura\u00b6","text":""},{"location":"projects/generative/generative/#2-setup-e-instalacao","title":"2. Setup e Instala\u00e7\u00e3o\u00b6","text":""},{"location":"projects/generative/generative/#3-stable-diffusion-text-to-image","title":"3. Stable Diffusion Text-to-Image\u00b6","text":"<p>O modelo \u00e9 carregado com otimiza\u00e7\u00f5es de mem\u00f3ria: FP16 reduz o uso pela metade, o carregamento direto na GPU evita c\u00f3pias intermedi\u00e1rias, e t\u00e9cnicas de slicing processam aten\u00e7\u00e3o e VAE em chunks menores. Essas configura\u00e7\u00f5es permitem execu\u00e7\u00e3o em GPUs com mem\u00f3ria limitada.</p>"},{"location":"projects/generative/generative/#31-funcao-de-geracao-basica","title":"3.1 Fun\u00e7\u00e3o de Gera\u00e7\u00e3o B\u00e1sica\u00b6","text":""},{"location":"projects/generative/generative/#32-cadeira","title":"3.2 Cadeira\u00b6","text":"<p>Gera\u00e7\u00e3o com Stable Diffusion b\u00e1sico, sem controle espacial, servindo como baseline para compara\u00e7\u00e3o com ControlNet. O modelo executa 25 steps de infer\u00eancia com guidance scale 7.5, valor padr\u00e3o que equilibra fidelidade ao prompt e liberdade criativa.</p>"},{"location":"projects/generative/generative/#33-smartwatch","title":"3.3 Smartwatch\u00b6","text":"<p>Gera\u00e7\u00e3o de produto tecnol\u00f3gico para demonstrar aplica\u00e7\u00e3o em categorias distintas. Mant\u00e9m os mesmos 25 steps e guidance 7.5 do exemplo anterior para permitir compara\u00e7\u00e3o direta entre diferentes tipos de produtos.</p>"},{"location":"projects/generative/generative/#4-controlnet-para-controle-espacial","title":"4. ControlNet para Controle Espacial\u00b6","text":"<p>O ControlNet Canny utiliza detec\u00e7\u00e3o de bordas para guiar a gera\u00e7\u00e3o, permitindo manter controle sobre a composi\u00e7\u00e3o enquanto varia estilos e materiais.</p>"},{"location":"projects/generative/generative/#41-funcoes-auxiliares","title":"4.1 Fun\u00e7\u00f5es Auxiliares\u00b6","text":""},{"location":"projects/generative/generative/#42-cadeira-com-controlnet","title":"4.2 Cadeira com ControlNet\u00b6","text":"<p>Com um sketch em perspectiva 3/4 como condi\u00e7\u00e3o, o ControlNet preserva a estrutura geom\u00e9trica enquanto preenche materiais e texturas. A gera\u00e7\u00e3o usa 20 steps com guidance 8.0 e control scale 1.2, valores que priorizam a ader\u00eancia \u00e0 condi\u00e7\u00e3o estrutural.</p>"},{"location":"projects/generative/generative/#43-relogio-com-controlnet","title":"4.3 Rel\u00f3gio com ControlNet\u00b6","text":"<p>Teste de controle estrutural em formato circular para avaliar precis\u00e3o em propor\u00e7\u00f5es. O control scale retorna ao valor padr\u00e3o 1.0, pois a forma simples do sketch n\u00e3o requer ader\u00eancia t\u00e3o forte quanto o exemplo anterior.</p>"},{"location":"projects/generative/generative/#44-garrafa-com-controlnet","title":"4.4 Garrafa com ControlNet\u00b6","text":"<p>Design com forma vertical alongada para testar consist\u00eancia de propor\u00e7\u00f5es em geometrias diferentes. O control scale 1.1 oferece ader\u00eancia moderada \u00e0 estrutura retangular do sketch.</p>"},{"location":"projects/generative/generative/#5-analise-de-resultados","title":"5. An\u00e1lise de Resultados\u00b6","text":"<p>O pipeline de Stable Diffusion + ControlNet foi aplicado em cinco exemplos de diferentes categorias de produtos. Os experimentos indicam que guidance scale entre 7-8 equilibra criatividade e fidelidade ao prompt, enquanto 20-25 steps de infer\u00eancia produzem qualidade adequada. Para o ControlNet, conditioning scale de 1.0-1.2 oferece controle sem suprimir detalhes.</p> <p>As aplica\u00e7\u00f5es pr\u00e1ticas incluem prototipagem de varia\u00e7\u00f5es de design e explora\u00e7\u00e3o de estilos mantendo estrutura fixa. As limita\u00e7\u00f5es observadas s\u00e3o a precis\u00e3o vari\u00e1vel em dimens\u00f5es exatas, inconsist\u00eancia entre gera\u00e7\u00f5es da mesma prompt e vi\u00e9s do dataset de treinamento.</p> <p>Extens\u00f5es poss\u00edveis incluem combina\u00e7\u00e3o de m\u00faltiplos ControlNets para controle multi-modal, fine-tuning com LoRA para especializa\u00e7\u00e3o, integra\u00e7\u00e3o com modelos text-to-3D e inpainting para edi\u00e7\u00e3o localizada. Do ponto de vista \u00e9tico, \u00e9 necess\u00e1rio verificar designs por pl\u00e1gio, reconhecer vieses do dataset, usar como ferramenta de aux\u00edlio ao designer e indicar o uso de IA em materiais comerciais.</p>"},{"location":"projects/generative/generative/#51-insights-tecnicos","title":"5.1 Insights T\u00e9cnicos\u00b6","text":"<p>O guidance scale ideal situa-se entre 7-8, pois valores abaixo de 5 geram imagens desconectadas do prompt e acima de 12 causam satura\u00e7\u00e3o excessiva. Os 20-25 inference steps s\u00e3o suficientes, com ganhos marginais acima de 30. O conditioning scale do ControlNet entre 1.0-1.2 mant\u00e9m controle sem restringir a criatividade do modelo.</p> <p>As otimiza\u00e7\u00f5es de mem\u00f3ria aplicadas reduziram o consumo de 8-10GB para 3.5-4GB. O FP16 corta o uso pela metade, o low_cpu_mem_usage evita c\u00f3pias intermedi\u00e1rias, o attention_slicing reduz picos em cerca de 30%, o VAE_slicing processa em tiles, e a limpeza de cache entre gera\u00e7\u00f5es libera mem\u00f3ria n\u00e3o utilizada.</p> <p>O ControlNet mant\u00e9m estrutura consistente atrav\u00e9s de varia\u00e7\u00f5es de estilo, separa a defini\u00e7\u00e3o de composi\u00e7\u00e3o do preenchimento de detalhes, facilita itera\u00e7\u00e3o sobre a mesma estrutura e acelera o workflow de design ao permitir m\u00faltiplas varia\u00e7\u00f5es sobre uma base fixa.</p>"}]}