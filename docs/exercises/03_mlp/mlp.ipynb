{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Entendendo Perceptrons Multicamadas (MLPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "\n",
    "**Dados do problema.**\n",
    "Entrada $x=[0.5,\\,-0.2]$, alvo $y=1.0$.\n",
    "Camada oculta (2 neurônios, tanh):\n",
    "\n",
    "$$\n",
    "W^{(1)}=\\begin{bmatrix}0.3&-0.1\\\\ 0.2&0.4\\end{bmatrix},\\quad\n",
    "b^{(1)}=\\begin{bmatrix}0.1\\\\-0.2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Saída (1 neurônio, tanh):\n",
    "\n",
    "$$\n",
    "W^{(2)}=\\begin{bmatrix}0.5&-0.3\\end{bmatrix},\\quad\n",
    "b^{(2)}=0.2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1) *Forward pass*\n",
    "\n",
    "**Pré-ativações na oculta** $z^{(1)}=W^{(1)}x+b^{(1)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{(1)}_1&=0.3\\cdot0.5+(-0.1)\\cdot(-0.2)+0.1=0.27 \\\\\n",
    "z^{(1)}_2&=0.2\\cdot0.5+0.4\\cdot(-0.2)-0.2=-0.18\n",
    "\\end{aligned}\n",
    "\\Longrightarrow\\quad\n",
    "z^{(1)}=\\begin{bmatrix}0.270000\\\\-0.180000\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Ativações na oculta** $a^{(1)}=\\tanh(z^{(1)})$:\n",
    "\n",
    "$$\n",
    "a^{(1)}=\\begin{bmatrix}\\tanh(0.27)\\\\ \\tanh(-0.18)\\end{bmatrix}\n",
    "=\\begin{bmatrix}0.263625\\\\-0.178081\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Pré-ativação na saída** $z^{(2)}=W^{(2)}a^{(1)}+b^{(2)}$:\n",
    "\n",
    "$$\n",
    "z^{(2)}=0.5\\cdot0.263625+(-0.3)\\cdot(-0.178081)+0.2\n",
    "=0.385237\n",
    "$$\n",
    "\n",
    "**Saída** $\\hat y=\\tanh(z^{(2)})=\\tanh(0.385237)=\\mathbf{0.367247}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) *Loss*\n",
    "\n",
    "$$\n",
    "L=(y-\\hat y)^2=(1-0.367247)^2=\\mathbf{0.400377}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3) *Backward pass*\n",
    "\n",
    "Derivada da perda em relação à saída:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat y}=2(\\hat y-y)=2(0.367247-1)=-1.265507.\n",
    "$$\n",
    "\n",
    "Derivada da tanh: $\\frac{d}{dz}\\tanh(z)=1-\\tanh^2(z)$. Logo:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z^{(2)}}=\\frac{\\partial L}{\\partial \\hat y}\\,(1-\\hat y^2)\n",
    "=-1.265507\\cdot(1-0.367247^2)=\\mathbf{-1.094828}.\n",
    "$$\n",
    "\n",
    "**Gradientes da camada de saída**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(2)}}=\\frac{\\partial L}{\\partial z^{(2)}}\\,a^{(1)}\n",
    "=\\begin{bmatrix}-0.288624 & 0.194968\\end{bmatrix},\\quad\n",
    "\\frac{\\partial L}{\\partial b^{(2)}}=\\mathbf{-1.094828}.\n",
    "$$\n",
    "\n",
    "**Propagação para a oculta**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a^{(1)}}=\\frac{\\partial L}{\\partial z^{(2)}}\\,W^{(2)}\n",
    "=\\begin{bmatrix}-0.547414\\\\ 0.328448\\end{bmatrix},\\qquad\n",
    "1-(a^{(1)})^2=\\begin{bmatrix}0.930502\\\\ 0.968287\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z^{(1)}}=\\frac{\\partial L}{\\partial a^{(1)}}\\odot\\bigl(1-(a^{(1)})^2\\bigr)\n",
    "=\\begin{bmatrix}-0.509370\\\\ 0.318032\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "**Gradientes da camada oculta** (produto externo com $x=[0.5,-0.2]$):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(1)}}\n",
    "=\\begin{bmatrix}\n",
    "-0.254685 & 0.101874\\\\\n",
    "\\phantom{-}0.159016 & -0.063606\n",
    "\\end{bmatrix},\\quad\n",
    "\\frac{\\partial L}{\\partial b^{(1)}}=\\begin{bmatrix}-0.509370\\\\ 0.318032\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4) *Atualização de parâmetros*  $(\\eta=\\mathbf{0.1})$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{(2)}_{\\text{novo}}&=W^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial W^{(2)}} \n",
    "=\\begin{bmatrix}0.528862 & -0.319497\\end{bmatrix} \\\\\n",
    "b^{(2)}_{\\text{novo}}&=b^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(2)}}\n",
    "=\\mathbf{0.309483} \\\\\n",
    "W^{(1)}_{\\text{novo}}&=W^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial W^{(1)}} \\\\\n",
    "&=\\begin{bmatrix}\n",
    "0.325468 & -0.110187\\\\\n",
    "0.184098 & \\phantom{-}0.406361\n",
    "\\end{bmatrix}\\\\[2pt]\n",
    "b^{(1)}_{\\text{novo}}&=b^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(1)}}\n",
    "=\\begin{bmatrix}\\phantom{-}0.150937\\\\ -0.231803\\end{bmatrix}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Exercício 2 — MLP *from scratch* (classificação binária em 2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Gerar um dataset 2D com **1 cluster** para a classe 0 e **2 clusters** para a classe 1 (usando `make_classification` em subconjuntos), treinar um **MLP do zero** (NumPy apenas) com 1 camada oculta, loss binário (BCE), e avaliar: perda de treino, acurácia de teste e fronteira de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 5)\n",
    "\n",
    "def plot_scatter(X, y, title=\"\"):\n",
    "    plt.figure()\n",
    "    plt.scatter(X[y==0,0], X[y==0,1], s=10, alpha=0.7, label=\"classe 0\")\n",
    "    plt.scatter(X[y==1,0], X[y==1,1], s=10, alpha=0.7, label=\"classe 1\")\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend(); plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n0, n1 = 500, 500  # total = 1000\n",
    "\n",
    "# Subconjunto só com classe 0 (1 cluster)\n",
    "X0, y0 = make_classification(\n",
    "    n_samples=n0, n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=1, n_classes=2, weights=[1.0, 0.0], class_sep=1.5,\n",
    "    flip_y=0.0, random_state=42\n",
    ")\n",
    "\n",
    "# Subconjunto só com classe 1 (2 clusters)\n",
    "X1, y1 = make_classification(\n",
    "    n_samples=n1, n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=2, n_classes=2, weights=[0.0, 1.0], class_sep=1.5,\n",
    "    flip_y=0.0, random_state=43\n",
    ")\n",
    "\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])  # rótulos {0,1}\n",
    "\n",
    "# embaralhar\n",
    "perm = np.random.permutation(len(X))\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "plot_scatter(X, y, \"Dados 2D — classe 0 (1 cluster) vs classe 1 (2 clusters)\")\n",
    "X.shape, y.shape, y.min(), y.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x): return np.tanh(x)\n",
    "def dtanh(a): return 1.0 - a**2               # a = tanh(z)\n",
    "def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def bce_loss(y_true, y_prob, eps=1e-9):\n",
    "    y_true = y_true.reshape(-1, 1)\n",
    "    y_prob = np.clip(y_prob, eps, 1.0-eps)\n",
    "    return -(y_true*np.log(y_prob) + (1-y_true)*np.log(1-y_prob)).mean()\n",
    "\n",
    "class MLPBinary:\n",
    "    def __init__(self, in_dim=2, hidden=8, lr=0.05, seed=42):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Xavier para tanh: U(-sqrt(6/(fan_in+fan_out)), sqrt(6/(fan_in+fan_out)))\n",
    "        lim1 = np.sqrt(6/(in_dim+hidden))\n",
    "        self.W1 = rng.uniform(-lim1, lim1, size=(in_dim, hidden))\n",
    "        self.b1 = np.zeros((1, hidden))\n",
    "        lim2 = np.sqrt(6/(hidden+1))\n",
    "        self.W2 = rng.uniform(-lim2, lim2, size=(hidden, 1))\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "        self.lr = lr\n",
    "        self.loss_hist = []\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z1 = X @ self.W1 + self.b1          # (N,H)\n",
    "        A1 = tanh(Z1)                        # (N,H)\n",
    "        Z2 = A1 @ self.W2 + self.b2          # (N,1)\n",
    "        A2 = sigmoid(Z2)                     # (N,1)\n",
    "        cache = (X, Z1, A1, Z2, A2)\n",
    "        return A2, cache\n",
    "\n",
    "    def backward(self, cache, y_true):\n",
    "        X, Z1, A1, Z2, A2 = cache\n",
    "        N = X.shape[0]\n",
    "        y = y_true.reshape(-1, 1)\n",
    "\n",
    "        # BCE + sigmoid: dZ2 = (A2 - y) / N\n",
    "        dZ2 = (A2 - y) / N                   # (N,1)\n",
    "        dW2 = A1.T @ dZ2                     # (H,1)\n",
    "        db2 = dZ2.sum(axis=0, keepdims=True) # (1,1)\n",
    "\n",
    "        dA1 = dZ2 @ self.W2.T                # (N,H)\n",
    "        dZ1 = dA1 * dtanh(A1)                # (N,H)\n",
    "        dW1 = X.T @ dZ1                      # (2,H)\n",
    "        db1 = dZ1.sum(axis=0, keepdims=True) # (1,H)\n",
    "\n",
    "        grads = (dW1, db1, dW2, db2)\n",
    "        return grads\n",
    "\n",
    "    def step(self, grads):\n",
    "        dW1, db1, dW2, db2 = grads\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "\n",
    "    def fit(self, X, y, epochs=300, verbose=False):\n",
    "        self.loss_hist.clear()\n",
    "        for ep in range(1, epochs+1):\n",
    "            y_prob, cache = self.forward(X)\n",
    "            loss = bce_loss(y, y_prob)\n",
    "            self.loss_hist.append(float(loss))\n",
    "            grads = self.backward(cache, y)\n",
    "            self.step(grads)\n",
    "            if verbose and ep % 50 == 0:\n",
    "                print(f\"época {ep:03d} | loss={loss:.4f}\")\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        y_prob, _ = self.forward(X)\n",
    "        return y_prob\n",
    "\n",
    "    def predict(self, X, thr=0.5):\n",
    "        return (self.predict_proba(X) >= thr).astype(int).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPBinary(in_dim=2, hidden=8, lr=0.05, seed=42).fit(X_train, y_train, epochs=300, verbose=True)\n",
    "\n",
    "y_pred_test = mlp.predict(X_test)\n",
    "test_acc = (y_pred_test == y_test).mean()\n",
    "\n",
    "print(f\"Acurácia de teste: {test_acc:.3f}\")\n",
    "plt.figure(); plt.plot(mlp.loss_hist); plt.xlabel(\"época\"); plt.ylabel(\"loss (BCE)\"); plt.title(\"Treinamento — perda\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# malha para fronteira\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X[:,0].min()-1, X[:,0].max()+1, 300),\n",
    "    np.linspace(X[:,1].min()-1, X[:,1].max()+1, 300)\n",
    ")\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "probs = mlp.predict_proba(grid).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.contourf(xx, yy, probs, levels=20, alpha=0.4)\n",
    "plt.scatter(X_test[y_test==0,0], X_test[y_test==0,1], s=12, label=\"teste: classe 0\")\n",
    "plt.scatter(X_test[y_test==1,0], X_test[y_test==1,1], s=12, label=\"teste: classe 1\")\n",
    "plt.colorbar(label=\"p(classe 1)\")\n",
    "plt.title(\"Fronteira de decisão (MLP)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "O conjunto foi construído com **1 cluster** para a classe 0 e **2 clusters** para a classe 1, criando regiões não lineares. Um MLP com `tanh` na oculta e `sigmoid` na saída, treinado com **BCE**, modela fronteiras curvadas e supera a limitação do perceptron linear do exercício anterior. A curva de **loss** decresce de forma estável e a fronteira acompanha a geometria (dois aglomerados na classe 1). Resultados podem variar com `hidden`, `lr` e épocas; aumentos moderados em `hidden` ou `epochs` tendem a melhorar a acurácia até saturar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
