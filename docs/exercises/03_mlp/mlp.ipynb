{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Entendendo Perceptrons Multicamadas (MLPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Exercício 1\n",
    "\n",
    "**Dados do problema.**\n",
    "Entrada $x=[0.5,\\,-0.2]$, alvo $y=1.0$.\n",
    "Camada oculta (2 neurônios, tanh):\n",
    "\n",
    "$$\n",
    "W^{(1)}=\\begin{bmatrix}0.3&-0.1\\\\ 0.2&0.4\\end{bmatrix},\\quad\n",
    "b^{(1)}=\\begin{bmatrix}0.1\\\\-0.2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Saída (1 neurônio, tanh):\n",
    "\n",
    "$$\n",
    "W^{(2)}=\\begin{bmatrix}0.5&-0.3\\end{bmatrix},\\quad\n",
    "b^{(2)}=0.2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1) *Forward pass*\n",
    "\n",
    "**Pré-ativações na oculta** $z^{(1)}=W^{(1)}x+b^{(1)}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z^{(1)}_1&=0.3\\cdot0.5+(-0.1)\\cdot(-0.2)+0.1=0.27 \\\\\n",
    "z^{(1)}_2&=0.2\\cdot0.5+0.4\\cdot(-0.2)-0.2=-0.18\n",
    "\\end{aligned}\n",
    "\\Longrightarrow\\quad\n",
    "z^{(1)}=\\begin{bmatrix}0.270000\\\\-0.180000\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Ativações na oculta** $a^{(1)}=\\tanh(z^{(1)})$:\n",
    "\n",
    "$$\n",
    "a^{(1)}=\\begin{bmatrix}\\tanh(0.27)\\\\ \\tanh(-0.18)\\end{bmatrix}\n",
    "=\\begin{bmatrix}0.263625\\\\-0.178081\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Pré-ativação na saída** $z^{(2)}=W^{(2)}a^{(1)}+b^{(2)}$:\n",
    "\n",
    "$$\n",
    "z^{(2)}=0.5\\cdot0.263625+(-0.3)\\cdot(-0.178081)+0.2\n",
    "=0.385237\n",
    "$$\n",
    "\n",
    "**Saída** $\\hat y=\\tanh(z^{(2)})=\\tanh(0.385237)=\\mathbf{0.367247}$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) *Loss*\n",
    "\n",
    "$$\n",
    "L=(y-\\hat y)^2=(1-0.367247)^2=\\mathbf{0.400377}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3) *Backward pass*\n",
    "\n",
    "Derivada da perda em relação à saída:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat y}=2(\\hat y-y)=2(0.367247-1)=-1.265507.\n",
    "$$\n",
    "\n",
    "Derivada da tanh: $\\frac{d}{dz}\\tanh(z)=1-\\tanh^2(z)$. Logo:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z^{(2)}}=\\frac{\\partial L}{\\partial \\hat y}\\,(1-\\hat y^2)\n",
    "=-1.265507\\cdot(1-0.367247^2)=\\mathbf{-1.094828}.\n",
    "$$\n",
    "\n",
    "**Gradientes da camada de saída**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(2)}}=\\frac{\\partial L}{\\partial z^{(2)}}\\,a^{(1)}\n",
    "=\\begin{bmatrix}-0.288624 & 0.194968\\end{bmatrix},\\quad\n",
    "\\frac{\\partial L}{\\partial b^{(2)}}=\\mathbf{-1.094828}.\n",
    "$$\n",
    "\n",
    "**Propagação para a oculta**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a^{(1)}}=\\frac{\\partial L}{\\partial z^{(2)}}\\,W^{(2)}\n",
    "=\\begin{bmatrix}-0.547414\\\\ 0.328448\\end{bmatrix},\\qquad\n",
    "1-(a^{(1)})^2=\\begin{bmatrix}0.930502\\\\ 0.968287\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z^{(1)}}=\\frac{\\partial L}{\\partial a^{(1)}}\\odot\\bigl(1-(a^{(1)})^2\\bigr)\n",
    "=\\begin{bmatrix}-0.509370\\\\ 0.318032\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "**Gradientes da camada oculta** (produto externo com $x=[0.5,-0.2]$):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W^{(1)}}\n",
    "=\\begin{bmatrix}\n",
    "-0.254685 & 0.101874\\\\\n",
    "\\phantom{-}0.159016 & -0.063606\n",
    "\\end{bmatrix},\\quad\n",
    "\\frac{\\partial L}{\\partial b^{(1)}}=\\begin{bmatrix}-0.509370\\\\ 0.318032\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4) *Atualização de parâmetros*  $(\\eta=\\mathbf{0.1})$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{(2)}_{\\text{novo}}&=W^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial W^{(2)}} \n",
    "=\\begin{bmatrix}0.528862 & -0.319497\\end{bmatrix} \\\\\n",
    "b^{(2)}_{\\text{novo}}&=b^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(2)}}\n",
    "=\\mathbf{0.309483} \\\\\n",
    "W^{(1)}_{\\text{novo}}&=W^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial W^{(1)}} \\\\\n",
    "&=\\begin{bmatrix}\n",
    "0.325468 & -0.110187\\\\\n",
    "0.184098 & \\phantom{-}0.406361\n",
    "\\end{bmatrix}\\\\[2pt]\n",
    "b^{(1)}_{\\text{novo}}&=b^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(1)}}\n",
    "=\\begin{bmatrix}\\phantom{-}0.150937\\\\ -0.231803\\end{bmatrix}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Exercício 2 — MLP *from scratch* (classificação binária em 2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Gerar um dataset 2D com **1 cluster** para a classe 0 e **2 clusters** para a classe 1 (usando `make_classification` em subconjuntos), treinar um **MLP do zero** (NumPy apenas) com 1 camada oculta, loss binário (BCE), e avaliar: perda de treino, acurácia de teste e fronteira de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n0, n1 = 500, 500  # total=1000\n",
    "\n",
    "# Subconjunto só com classe 0 (1 cluster)\n",
    "X0, y0 = make_classification(\n",
    "    n_samples=n0, n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=1, n_classes=2, weights=[1.0, 0.0],\n",
    "    class_sep=1.5, flip_y=0.0, random_state=42\n",
    ")\n",
    "\n",
    "# Subconjunto só com classe 1 (2 clusters)\n",
    "X1, y1 = make_classification(\n",
    "    n_samples=n1, n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=2, n_classes=2, weights=[0.0, 1.0],\n",
    "    class_sep=1.5, flip_y=0.0, random_state=43\n",
    ")\n",
    "\n",
    "# Junta e embaralha\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.hstack([y0, y1])  # rótulos {0,1}\n",
    "\n",
    "perm = np.random.permutation(len(X))\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "# (segurança) se por acaso seus rótulos estiverem em {-1,+1}, converte para {0,1}\n",
    "if set(np.unique(y)) == {-1, 1}:\n",
    "    y = ((y + 1) // 2).astype(int)\n",
    "\n",
    "print(X.shape, y.shape, np.bincount(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Opcional (recomendado): padronizar usando estatísticas do treino\n",
    "mu = X_train.mean(axis=0)\n",
    "sigma = X_train.std(axis=0) + 1e-8\n",
    "X_train = (X_train - mu) / sigma\n",
    "X_test  = (X_test  - mu) / sigma\n",
    "\n",
    "print(\"train:\", X_train.shape, \"test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Ativações ---------\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def dtanh(a):\n",
    "    # derivada em termos da ativação a = tanh(z)\n",
    "    return 1.0 - a**2\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0.0, z)\n",
    "\n",
    "def drelu(z):\n",
    "    return (z > 0.0).astype(z.dtype)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    # estável numericamente\n",
    "    z = z - z.max(axis=1, keepdims=True)\n",
    "    e = np.exp(z)\n",
    "    return e / e.sum(axis=1, keepdims=True)\n",
    "\n",
    "# --------- Perdas ---------\n",
    "def bce_loss(y_true, y_prob, eps=1e-9):\n",
    "    # y_true shape: (N,1) com {0,1}; y_prob shape: (N,1)\n",
    "    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n",
    "    return -(y_true*np.log(y_prob) + (1-y_true)*np.log(1-y_prob)).mean()\n",
    "\n",
    "def ce_loss(y_true_onehot, y_prob, eps=1e-9):\n",
    "    # y_true_onehot shape: (N,C); y_prob shape: (N,C)\n",
    "    y_prob = np.clip(y_prob, eps, 1.0 - eps)\n",
    "    return -(y_true_onehot * np.log(y_prob)).sum(axis=1).mean()\n",
    "\n",
    "# --------- Utilitários ---------\n",
    "def one_hot(y, num_classes=None):\n",
    "    y = y.astype(int).ravel()\n",
    "    if num_classes is None:\n",
    "        num_classes = int(y.max()) + 1\n",
    "    out = np.zeros((y.shape[0], num_classes), dtype=float)\n",
    "    out[np.arange(y.shape[0]), y] = 1.0\n",
    "    return out\n",
    "\n",
    "def xavier_limit(fan_in, fan_out):\n",
    "    return np.sqrt(6.0 / (fan_in + fan_out))\n",
    "\n",
    "def he_limit(fan_in):\n",
    "    # He uniform\n",
    "    return np.sqrt(6.0 / fan_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    MLP genérico (NumPy puro).\n",
    "    - layer_sizes: lista com dimensões [in, h1, ..., hK, out]\n",
    "    - activations: lista de strings para cada camada oculta + saída (len = len(layer_sizes)-1)\n",
    "        opções: 'tanh', 'relu', 'sigmoid', 'softmax' (use 'sigmoid' p/ binário e 'softmax' p/ multiclasse)\n",
    "    - loss: 'bce' (binário; requer saída 'sigmoid') ou 'ce' (multiclasse; requer saída 'softmax')\n",
    "    - l2: regularização L2 (lambda), default 0.0\n",
    "    - lr: taxa de aprendizado\n",
    "    - seed: reprodutibilidade\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, activations, loss, lr=0.05, l2=0.0, seed=42):\n",
    "        assert len(layer_sizes) >= 2, \"Precisa de pelo menos entrada e saída\"\n",
    "        assert len(activations) == len(layer_sizes) - 1, \"Uma ativação por camada\"\n",
    "        self.sizes = list(layer_sizes)\n",
    "        self.acts = list(activations)\n",
    "        self.loss_name = loss\n",
    "        self.lr = lr\n",
    "        self.l2 = float(l2)\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.params = self._init_params()\n",
    "        self.loss_hist = []\n",
    "\n",
    "        # mapear nomes para funções\n",
    "        self._act = {\n",
    "            'tanh': (tanh, dtanh),\n",
    "            'relu': (relu, None),     # derivative usa Z\n",
    "            'sigmoid': (sigmoid, None), # derivative calculada via BCE no topo; não usada nas ocultas\n",
    "            'softmax': (softmax, None)\n",
    "        }\n",
    "\n",
    "        if loss == 'bce':\n",
    "            assert self.acts[-1] == 'sigmoid', \"BCE requer saída 'sigmoid'\"\n",
    "        elif loss == 'ce':\n",
    "            assert self.acts[-1] == 'softmax', \"CE requer saída 'softmax'\"\n",
    "        else:\n",
    "            raise ValueError(\"loss deve ser 'bce' ou 'ce'\")\n",
    "\n",
    "    def _init_params(self):\n",
    "        params = []\n",
    "        for l in range(len(self.sizes) - 1):\n",
    "            fan_in, fan_out = self.sizes[l], self.sizes[l+1]\n",
    "            act = self.acts[l]\n",
    "            if act in ('tanh', 'sigmoid', 'softmax'):\n",
    "                lim = xavier_limit(fan_in, fan_out)\n",
    "            elif act == 'relu':\n",
    "                lim = he_limit(fan_in)\n",
    "            else:\n",
    "                lim = xavier_limit(fan_in, fan_out)\n",
    "\n",
    "            W = self.rng.uniform(-lim, lim, size=(fan_in, fan_out))\n",
    "            b = np.zeros((1, fan_out))\n",
    "            params.append({'W': W, 'b': b})\n",
    "        return params\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"\n",
    "        Retorna A_list, Z_list:\n",
    "        A_list[0] = X\n",
    "        para l>=1: Z_list[l] = A_list[l-1] @ W_l + b_l; A_list[l] = act(Z_list[l])\n",
    "        \"\"\"\n",
    "        A_list = [X]\n",
    "        Z_list = [None]\n",
    "        for l, layer in enumerate(self.params, start=1):\n",
    "            W, b = layer['W'], layer['b']\n",
    "            Z = A_list[-1] @ W + b\n",
    "            act_name = self.acts[l-1]\n",
    "            if act_name == 'tanh':\n",
    "                A = tanh(Z)\n",
    "            elif act_name == 'relu':\n",
    "                A = relu(Z)\n",
    "            elif act_name == 'sigmoid':\n",
    "                A = sigmoid(Z)\n",
    "            elif act_name == 'softmax':\n",
    "                A = softmax(Z)\n",
    "            else:\n",
    "                raise ValueError(f\"Ativação desconhecida: {act_name}\")\n",
    "            Z_list.append(Z); A_list.append(A)\n",
    "        return A_list, Z_list\n",
    "\n",
    "    def _compute_loss(self, y_true, Aout):\n",
    "        if self.loss_name == 'bce':\n",
    "            loss = bce_loss(y_true.reshape(-1,1), Aout)\n",
    "        else:  # 'ce'\n",
    "            if Aout.ndim == 1 or Aout.shape[1] == 1:\n",
    "                raise ValueError(\"CE requer probabilidades (N,C) e rótulos one-hot (N,C)\")\n",
    "            loss = ce_loss(y_true, Aout)\n",
    "\n",
    "        # L2\n",
    "        if self.l2 > 0.0:\n",
    "            l2_sum = sum((layer['W']**2).sum() for layer in self.params)\n",
    "            loss = loss + 0.5 * self.l2 * l2_sum / y_true.shape[0]\n",
    "        return float(loss)\n",
    "\n",
    "    def _backward(self, A_list, Z_list, y_true):\n",
    "        grads = [None] * len(self.params)\n",
    "        N = A_list[0].shape[0]\n",
    "\n",
    "        # dZ da camada de saída\n",
    "        Aout = A_list[-1]\n",
    "        if self.loss_name == 'bce':\n",
    "            # BCE + sigmoid => dZ = (Aout - y)/N\n",
    "            y = y_true.reshape(-1,1)\n",
    "            dZ = (Aout - y) / N\n",
    "        else:  # 'ce' + softmax => dZ = (Aout - Y)/N\n",
    "            Y = y_true\n",
    "            dZ = (Aout - Y) / N\n",
    "\n",
    "        # camadas de trás para frente\n",
    "        for l in reversed(range(len(self.params))):\n",
    "            A_prev = A_list[l]\n",
    "            W = self.params[l]['W']\n",
    "\n",
    "            dW = A_prev.T @ dZ\n",
    "            db = dZ.sum(axis=0, keepdims=True)\n",
    "\n",
    "            # L2\n",
    "            if self.l2 > 0.0:\n",
    "                dW = dW + self.l2 * W / N\n",
    "\n",
    "            grads[l] = {'dW': dW, 'db': db}\n",
    "\n",
    "            if l > 0:  # propagar para trás se não for a primeira camada\n",
    "                dA_prev = dZ @ W.T\n",
    "                act_name = self.acts[l-1]  # ativação da camada l\n",
    "                if act_name == 'tanh':\n",
    "                    dZ = dA_prev * dtanh(A_list[l])\n",
    "                elif act_name == 'relu':\n",
    "                    dZ = dA_prev * drelu(Z_list[l])\n",
    "                elif act_name in ('sigmoid', 'softmax'):\n",
    "                    # não usamos sigmoid/softmax em ocultas neste design; se usar, trate aqui:\n",
    "                    a = A_list[l]\n",
    "                    if act_name == 'sigmoid':\n",
    "                        dZ = dA_prev * a * (1 - a)\n",
    "                    else:\n",
    "                        raise ValueError(\"Softmax em camada oculta não suportado\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Ativação desconhecida: {act_name}\")\n",
    "        return grads\n",
    "\n",
    "    def _step(self, grads):\n",
    "        for layer, g in zip(self.params, grads):\n",
    "            layer['W'] -= self.lr * g['dW']\n",
    "            layer['b'] -= self.lr * g['db']\n",
    "\n",
    "    def fit(self, X, y, epochs=300, verbose=False):\n",
    "        \"\"\"\n",
    "        y:\n",
    "          - binário/BCE: array (N,) ou (N,1) com {0,1}\n",
    "          - CE: one-hot (N,C)\n",
    "        \"\"\"\n",
    "        self.loss_hist.clear()\n",
    "        for ep in range(1, epochs+1):\n",
    "            A_list, Z_list = self._forward(X)\n",
    "            loss = self._compute_loss(y, A_list[-1])\n",
    "            self.loss_hist.append(loss)\n",
    "            grads = self._backward(A_list, Z_list, y)\n",
    "            self._step(grads)\n",
    "            if verbose and (ep % 50 == 0 or ep == 1):\n",
    "                print(f\"época {ep:03d} | loss={loss:.4f}\")\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        A_list, _ = self._forward(X)\n",
    "        return A_list[-1]\n",
    "\n",
    "    def predict(self, X, thr=0.5):\n",
    "        proba = self.predict_proba(X)\n",
    "        # BCE binário\n",
    "        if self.loss_name == 'bce':\n",
    "            return (proba >= thr).astype(int).ravel()\n",
    "        # CE multiclasse\n",
    "        return np.argmax(proba, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo binário: 2 -> 8 -> 1, tanh + sigmoid, BCE\n",
    "mlp_bin = MLP(layer_sizes=[2, 8, 1],\n",
    "              activations=['tanh', 'sigmoid'],\n",
    "              loss='bce',\n",
    "              lr=0.05, l2=0.0, seed=42)\n",
    "\n",
    "mlp_bin.fit(X_train, y_train, epochs=300, verbose=True)\n",
    "y_pred = mlp_bin.predict(X_test)\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(f\"Acurácia (binário): {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary_model(model, X, y, title=\"Fronteira de decisão\"):\n",
    "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    proba = model.predict_proba(grid)\n",
    "    if proba.ndim == 1 or proba.shape[1] == 1:\n",
    "        zz = proba.reshape(xx.shape)\n",
    "    else:\n",
    "        zz = proba.max(axis=1).reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.4)\n",
    "    plt.colorbar(cs)\n",
    "    # pontos\n",
    "    if y.ndim == 2 and y.shape[1] > 1:\n",
    "        y_plot = np.argmax(y, axis=1)\n",
    "    else:\n",
    "        y_plot = y.ravel()\n",
    "    for cls in np.unique(y_plot):\n",
    "        pts = X[y_plot==cls]\n",
    "        plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")\n",
    "    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary_model(mlp_bin, X_train, y_train, title=\"Fronteira de decisão (treino)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, len(mlp_bin.loss_hist)+1), mlp_bin.loss_hist, marker=\"o\", ms=3)\n",
    "plt.xlabel(\"Época\"); plt.ylabel(\"Loss (treino)\")\n",
    "plt.title(\"MLP — curva de perda (BCE)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_bin(y_true, y_pred):\n",
    "    tn = np.sum((y_true==0) & (y_pred==0))\n",
    "    fp = np.sum((y_true==0) & (y_pred==1))\n",
    "    fn = np.sum((y_true==1) & (y_pred==0))\n",
    "    tp = np.sum((y_true==1) & (y_pred==1))\n",
    "    return np.array([[tn, fp],\n",
    "                     [fn, tp]])\n",
    "\n",
    "y_pred_tr = mlp_bin.predict(X_train)\n",
    "y_pred_te = mlp_bin.predict(X_test)\n",
    "\n",
    "acc_tr = (y_pred_tr == y_train).mean()\n",
    "acc_te = (y_pred_te == y_test).mean()\n",
    "cm_te  = confusion_matrix_bin(y_test, y_pred_te)\n",
    "\n",
    "print(f\"Acurácia (treino): {acc_tr:.3f}\")\n",
    "print(f\"Acurácia (teste) : {acc_te:.3f}\")\n",
    "print(\"Matriz de confusão (teste) [[TN, FP],[FN, TP]]:\\n\", cm_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary_with_errors(model, X, y, title=\"Fronteira (teste) + erros\"):\n",
    "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    zz = model.predict_proba(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    cs = plt.contourf(xx, yy, zz, levels=20, alpha=0.35)\n",
    "    plt.colorbar(cs)\n",
    "\n",
    "    y_hat = model.predict(X)\n",
    "    err = y_hat != y\n",
    "\n",
    "    for cls in np.unique(y):\n",
    "        pts = X[(y==cls) & (~err)]\n",
    "        plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"classe {cls}\")\n",
    "\n",
    "    if err.any():\n",
    "        plt.scatter(X[err,0], X[err,1], s=40, facecolors='none', edgecolors='r', linewidths=1.2, label=\"erros\")\n",
    "\n",
    "    plt.title(title); plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_boundary_with_errors(mlp_bin, X_test, y_test, title=\"Fronteira de decisão (teste) + erros\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "O MLP de 1 camada oculta aprendeu uma **fronteira não linear** que separa razoavelmente o único cluster da classe 0 dos **dois clusters** da classe 1. A **perda caiu** de \\~0.62 para \\~0.52 e a **acurácia de teste** ficou \\~0.62, compatível com a **sobreposição** visível entre as classes: há regiões onde ambos os rótulos são plausíveis, e uma fronteira suave (tanh + sigmoid) não resolve todas. Ainda assim, o modelo captura a estrutura multimodal, melhor que um classificador linear. Ganhos adicionais viriam de mais capacidade (mais neurônios/camadas), regularização e ajuste de taxa de aprendizado/épocas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Exercício 3 — MLP multiclasse (3 classes, 4 features)\n",
    "\n",
    "Gerar 1500 amostras, 3 classes, 4 features; 2 clusters para a classe 0, 3 para a classe 1 e 4 para a classe 2 (combinando subconjuntos). Treinar a **mesma MLP** do Ex. 2 (código idêntico), trocando apenas saída e função de perda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# 1500 = 500 por classe\n",
    "n0 = n1 = n2 = 500\n",
    "F = 4\n",
    "\n",
    "# Classe 0: 2 clusters\n",
    "X0, y0 = make_classification(\n",
    "    n_samples=n0, n_features=F, n_informative=F, n_redundant=0,\n",
    "    n_classes=2, n_clusters_per_class=2, weights=[1.0, 0.0],\n",
    "    class_sep=1.6, flip_y=0.0, random_state=10\n",
    ")\n",
    "y0[:] = 0\n",
    "\n",
    "# Classe 1: 3 clusters\n",
    "X1, y1 = make_classification(\n",
    "    n_samples=n1, n_features=F, n_informative=F, n_redundant=0,\n",
    "    n_classes=2, n_clusters_per_class=3, weights=[0.0, 1.0],\n",
    "    class_sep=1.5, flip_y=0.0, random_state=11\n",
    ")\n",
    "y1[:] = 1\n",
    "\n",
    "# Classe 2: 4 clusters\n",
    "X2, y2 = make_classification(\n",
    "    n_samples=n2, n_features=F, n_informative=F, n_redundant=0,\n",
    "    n_classes=2, n_clusters_per_class=4, weights=[0.0, 1.0],\n",
    "    class_sep=1.4, flip_y=0.0, random_state=12\n",
    ")\n",
    "y2[:] = 2\n",
    "\n",
    "X = np.vstack([X0, X1, X2])\n",
    "y = np.concatenate([y0, y1, y2]).astype(int)\n",
    "\n",
    "perm = rng.permutation(len(X))\n",
    "X, y = X[perm], y[perm]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# padronização por estatística do treino\n",
    "mu, sig = X_train.mean(axis=0), X_train.std(axis=0) + 1e-8\n",
    "X_train = (X_train - mu) / sig\n",
    "X_test  = (X_test  - mu) / sig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saída com 3 neurônios, tanh nas ocultas, softmax na saída, CE\n",
    "mlp_mc = MLP(layer_sizes=[X_train.shape[1], 16, 3],\n",
    "             activations=['tanh', 'softmax'],\n",
    "             loss='ce', lr=0.05, l2=0.0, seed=42)\n",
    "\n",
    "# rótulos one-hot para CE\n",
    "def one_hot(y, C=None):\n",
    "    C = int(y.max())+1 if C is None else C\n",
    "    Y = np.zeros((y.size, C)); Y[np.arange(y.size), y] = 1.0\n",
    "    return Y\n",
    "\n",
    "Y_train = one_hot(y_train, 3)\n",
    "Y_test  = one_hot(y_test, 3)\n",
    "\n",
    "mlp_mc.fit(X_train, Y_train, epochs=400, verbose=True)\n",
    "\n",
    "y_pred = mlp_mc.predict(X_test)          # argmax\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(f\"Acurácia (teste, 3 classes): {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mlp_mc.loss_hist); plt.xlabel(\"época\"); plt.ylabel(\"loss (CE)\")\n",
    "plt.title(\"Curva de treino — CE\"); plt.show()\n",
    "\n",
    "Z = PCA(n_components=2, random_state=0).fit_transform(X_test)\n",
    "y_hat = mlp_mc.predict(X_test)\n",
    "for c in np.unique(y_test):\n",
    "    pts = Z[y_test==c]\n",
    "    plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\")\n",
    "plt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\")\n",
    "plt.legend(); plt.title(\"Teste (PCA 2D): rótulo real vs. predito\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "O modelo com uma única camada oculta conseguiu reduzir a perda de **1.34 → 0.68** ao longo de 400 épocas, mostrando aprendizado consistente. A curva de treino apresenta decaimento suave, mas ainda relativamente lento, indicando que a capacidade de representação é limitada.\n",
    "\n",
    "No teste, a acurácia atingiu **72,3%** em um problema de 3 classes. O gráfico de PCA revela sobreposição considerável entre os rótulos verdadeiros e preditos, evidenciando dificuldade em separar regiões mais confusas do espaço. Esse resultado reflete a limitação de um MLP raso: apesar de capturar padrões não lineares, a fronteira de decisão ainda não é suficientemente expressiva para classes com forte sobreposição."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Exercício 4 — MLP mais profundo (≥2 camadas ocultas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 -> 32 -> 16 -> 3 (tanh nas ocultas, softmax na saída)\n",
    "mlp_deep = MLP(layer_sizes=[X_train.shape[1], 32, 16, 3],\n",
    "               activations=['tanh', 'tanh', 'softmax'],\n",
    "               loss='ce', lr=0.05, l2=1e-4, seed=42)\n",
    "\n",
    "Y_train = one_hot(y_train, 3)\n",
    "Y_test  = one_hot(y_test, 3)\n",
    "\n",
    "mlp_deep.fit(X_train, Y_train, epochs=500, verbose=True)\n",
    "y_pred = mlp_deep.predict(X_test)\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(f\"Acurácia (teste, 2 ocultas): {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mlp_deep.loss_hist); plt.xlabel(\"época\"); plt.ylabel(\"loss (CE)\")\n",
    "plt.title(\"Curva de treino — MLP profundo\"); plt.show()\n",
    "\n",
    "Z = PCA(n_components=2, random_state=0).fit_transform(X_test)\n",
    "y_hat = mlp_mc.predict(X_test)\n",
    "for c in np.unique(y_test):\n",
    "    pts = Z[y_test==c]\n",
    "    plt.scatter(pts[:,0], pts[:,1], s=12, label=f\"true {c}\")\n",
    "plt.scatter(Z[:,0], Z[:,1], c=y_hat, s=8, cmap=\"tab10\", alpha=0.25, label=\"pred\")\n",
    "plt.legend(); plt.title(\"Teste (PCA 2D): rótulo real vs. predito\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Com duas camadas ocultas, a perda caiu de **1.29 → 0.47** em 500 épocas, num ritmo mais acelerado e contínuo que no caso anterior. A curva de treino mostra melhora clara na capacidade de ajuste.\n",
    "\n",
    "No teste, a acurácia subiu para **80,7%**, superando o modelo raso. A projeção em PCA também evidencia maior alinhamento entre rótulos reais e predições, embora ainda haja sobreposição entre classes vizinhas. Esse resultado confirma o ganho de expressividade ao aumentar a profundidade: o MLP profundo consegue capturar fronteiras de decisão mais complexas e se adapta melhor ao problema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
