{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# Preparação e Análise de Dados para Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "\n",
    "## Objetivo\n",
    "\n",
    "Explorar separabilidade de classes em 2D, projetar dados 5D para 2D com PCA e preparar o dataset **Spaceship Titanic** para redes neurais com ativação `tanh`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Exercício 1 — Dados 2D (4 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 100\n",
    "params = {\n",
    "    0: {\"mean\": [2, 3],  \"std\": [0.8, 2.5]},\n",
    "    1: {\"mean\": [5, 6],  \"std\": [1.2, 1.9]},\n",
    "    2: {\"mean\": [8, 1],  \"std\": [0.9, 0.9]},\n",
    "    3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]},\n",
    "}\n",
    "\n",
    "Xs, ys = [], []\n",
    "for c, p in params.items():\n",
    "    mean = np.array(p[\"mean\"])\n",
    "    std = np.array(p[\"std\"])\n",
    "    Xc = np.random.randn(N, 2) * std + mean\n",
    "    Xs.append(Xc)\n",
    "    ys.append(np.full(N, c))\n",
    "\n",
    "X = np.vstack(Xs)\n",
    "y = np.hstack(ys)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for c in params.keys():\n",
    "    plt.scatter(X[y==c,0], X[y==c,1], s=12, label=f\"Classe {c}\", alpha=0.8)\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.title(\"Exercício 1 — Distribuição 2D (4 classes)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "As quatro classes se distribuem em regiões distintas do plano, cada uma concentrada em torno de um centro específico. Embora o eixo *x1* contribua fortemente para a separação, a variabilidade em *x2* cria dispersões diferentes entre os grupos. Esse cenário exige a combinação de múltiplas fronteiras de decisão ou o uso de modelos não lineares para capturar de forma adequada a estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Superfícies de decisão com MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(16,), activation=\"tanh\", max_iter=2000, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X[:,0].min()-1, X[:,0].max()+1, 300),\n",
    "    np.linspace(X[:,1].min()-1, X[:,1].max()+1, 300),\n",
    ")\n",
    "ZZ = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.contourf(xx, yy, ZZ, alpha=0.25, levels=[-0.5,0.5,1.5,2.5,3.5])\n",
    "for c in params.keys():\n",
    "    plt.scatter(X[y==c,0], X[y==c,1], s=10, label=f\"Classe {c}\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.title(\"Exercício 1 — Superfícies de decisão (MLP tanh)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Exercício 2 — Dados 5D (A/B) + PCA(2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "muA = np.array([0, 0, 0, 0, 0])\n",
    "SigmaA = np.array([\n",
    "    [1.0, 0.8, 0.1, 0.0, 0.0],\n",
    "    [0.8, 1.0, 0.3, 0.0, 0.0],\n",
    "    [0.1, 0.3, 1.0, 0.5, 0.0],\n",
    "    [0.0, 0.0, 0.5, 1.0, 0.2],\n",
    "    [0.0, 0.0, 0.0, 0.2, 1.0]\n",
    "])\n",
    "\n",
    "muB = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\n",
    "SigmaB = np.array([\n",
    "    [1.5, -0.7, 0.2, 0.0, 0.0],\n",
    "    [-0.7, 1.5, 0.4, 0.0, 0.0],\n",
    "    [0.2, 0.4, 1.5, 0.6, 0.0],\n",
    "    [0.0, 0.0, 0.6, 1.5, 0.3],\n",
    "    [0.0, 0.0, 0.0, 0.3, 1.5]\n",
    "])\n",
    "\n",
    "XA = np.random.multivariate_normal(muA, SigmaA, size=500)\n",
    "XB = np.random.multivariate_normal(muB, SigmaB, size=500)\n",
    "X5 = np.vstack([XA, XB])\n",
    "y5 = np.hstack([np.zeros(500), np.ones(500)])\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X2 = pca.fit_transform(X5)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X2[y5==0,0], X2[y5==0,1], s=10, label=\"Classe A\")\n",
    "plt.scatter(X2[y5==1,0], X2[y5==1,1], s=10, label=\"Classe B\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.title(\"Exercício 2 — PCA (5D → 2D)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "A projeção via PCA mostra que as classes A e B têm centros deslocados, mas ainda apresentam forte sobreposição devido à variância dentro de cada grupo. Essa configuração torna a separação linear pouco eficaz, já que não existe um hiperplano simples que separe bem as duas classes. Modelos mais expressivos, que incorporam não-linearidades, são mais adequados para capturar as fronteiras complexas observadas no espaço reduzido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Exercício 3 — Spaceship Titanic: pré-processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "O dataset **Spaceship Titanic** foi proposto em uma competição do Kaggle e tem como objetivo prever a variável *Transported*, que indica se um passageiro foi levado para outra dimensão durante a viagem. \n",
    "\n",
    "Os dados incluem atributos numéricos, como `Age` e os gastos a bordo (`RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`), e atributos categóricos, como `HomePlanet`, `CryoSleep`, `Destination`, `VIP` e `Cabin`. Antes do treinamento de uma rede neural, é necessário inspecionar a base para identificar tipos de variáveis, valores ausentes e definir as transformações adequadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "plt.rcParams[\"figure.figsize\"] = (8,3.5)\n",
    "\n",
    "csv_path = \"data/train.csv\"\n",
    "assert os.path.exists(csv_path), f\"Arquivo não encontrado: {csv_path}\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "target_col = \"Transported\"\n",
    "num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "cat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Cabin\"]\n",
    "\n",
    "# Colunas de gastos com forte assimetria (usar log1p)\n",
    "spend_cols = [\"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tipos detectados e relatório de valores ausentes:\")\n",
    "display(df[num_cols + cat_cols + [target_col]].dtypes)\n",
    "\n",
    "na_count = df[num_cols + cat_cols + [target_col]].isna().sum().sort_values(ascending=False)\n",
    "na_pct = (na_count / len(df)).round(3)\n",
    "missing_report = pd.DataFrame({\"missing\": na_count, \"pct\": na_pct})\n",
    "display(missing_report[missing_report[\"missing\"] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "A inspeção revelou que os atributos de gastos (`RoomService`, `FoodCourt`, `ShoppingMall`, `Spa`, `VRDeck`) e `Age` são numéricos, enquanto `HomePlanet`, `CryoSleep`, `Destination`, `VIP` e `Cabin` são categóricos. \n",
    "\n",
    "O relatório de valores ausentes mostrou taxas próximas de 2% em quase todas as colunas. Isso indica a necessidade de imputação sistemática: a mediana para atributos numéricos, robusta a outliers, e o valor mais frequente para categóricos, preservando consistência.\n",
    "\n",
    "Além disso, os atributos numéricos não seguem todos a mesma distribuição: `Age` tem assimetria moderada, enquanto os gastos apresentam caudas longas e forte concentração em zero. Essa diferença motiva estratégias específicas de transformação, ilustradas a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_before_after(series, transformer, title_after):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3.5))\n",
    "    ax1.hist(series.dropna().values, bins=40)\n",
    "    ax1.set_title(f\"{series.name} — original\")\n",
    "    tr = transformer.fit_transform(series.to_frame())\n",
    "    ax2.hist(tr.ravel(), bins=40)\n",
    "    ax2.set_title(f\"{series.name} — {title_after}\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "age_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "plot_before_after(df[\"Age\"], age_pipe, \"padronizada (StandardScaler)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "food_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"log\", FunctionTransformer(np.log1p, validate=False)),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "plot_before_after(df[\"FoodCourt\"], food_pipe, \"log1p + padronizada\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Os gráficos confirmam o efeito das transformações. Em `Age`, a padronização centralizou a distribuição em torno de zero e ajustou a escala para desvio padrão um, tornando os valores mais adequados para funções de ativação como `tanh`. \n",
    "\n",
    "Já em `FoodCourt`, a aplicação de `log1p` antes da padronização reduziu o impacto de caudas longas e outliers, resultando em uma distribuição mais equilibrada. O mesmo raciocínio pode ser aplicado às demais variáveis de gastos.\n",
    "\n",
    "Por fim, os atributos categóricos serão tratados com imputação do valor mais frequente e convertidos em indicadores binários via one-hot encoding. O dataset resultante torna-se mais homogêneo e apropriado para redes neurais, favorecendo a estabilidade do gradiente e o desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "\n",
    "Os experimentos mostraram que, em dados sintéticos 2D, a separação linear não é suficiente, exigindo funções de ativação não lineares para capturar fronteiras mais complexas. Na projeção dos dados 5D em 2D, a sobreposição causada por correlações entre atributos reforça essa limitação e destaca a necessidade de modelos mais expressivos. Já no caso do Spaceship Titanic, o pré-processamento com imputação, codificação categórica e padronização numérica foi fundamental para tornar o conjunto compatível com redes neurais baseadas em `tanh`, garantindo maior estabilidade no treinamento e melhor capacidade de generalização.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
