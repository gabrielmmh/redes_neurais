{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# Preparação e Análise de Dados para Redes Neurais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "\n",
    "## Objetivo\n",
    "\n",
    "Explorar separabilidade de classes em 2D, projetar dados 5D para 2D com PCA e preparar o dataset **Spaceship Titanic** para redes neurais com ativação `tanh`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Exercício 1 — Dados 2D (4 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "N = 100\n",
    "params = {\n",
    "    0: {\"mean\": [2, 3],  \"std\": [0.8, 2.5]},\n",
    "    1: {\"mean\": [5, 6],  \"std\": [1.2, 1.9]},\n",
    "    2: {\"mean\": [8, 1],  \"std\": [0.9, 0.9]},\n",
    "    3: {\"mean\": [15, 4], \"std\": [0.5, 2.0]},\n",
    "}\n",
    "\n",
    "Xs, ys = [], []\n",
    "for c, p in params.items():\n",
    "    mean = np.array(p[\"mean\"])\n",
    "    std = np.array(p[\"std\"])\n",
    "    Xc = np.random.randn(N, 2) * std + mean\n",
    "    Xs.append(Xc)\n",
    "    ys.append(np.full(N, c))\n",
    "\n",
    "X = np.vstack(Xs)\n",
    "y = np.hstack(ys)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for c in params.keys():\n",
    "    plt.scatter(X[y==c,0], X[y==c,1], s=12, label=f\"Classe {c}\", alpha=0.8)\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.title(\"Exercício 1 — Distribuição 2D (4 classes)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "As quatro classes se distribuem em regiões distintas do plano, cada uma concentrada em torno de um centro específico. Embora o eixo *x1* contribua fortemente para a separação, a variabilidade em *x2* cria dispersões diferentes entre os grupos. Esse cenário exige a combinação de múltiplas fronteiras de decisão ou o uso de modelos não lineares para capturar de forma adequada a estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Superfícies de decisão com MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(16,), activation=\"tanh\", max_iter=2000, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X[:,0].min()-1, X[:,0].max()+1, 300),\n",
    "    np.linspace(X[:,1].min()-1, X[:,1].max()+1, 300),\n",
    ")\n",
    "ZZ = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.contourf(xx, yy, ZZ, alpha=0.25, levels=[-0.5,0.5,1.5,2.5,3.5])\n",
    "for c in params.keys():\n",
    "    plt.scatter(X[y==c,0], X[y==c,1], s=10, label=f\"Classe {c}\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.title(\"Exercício 1 — Superfícies de decisão (MLP tanh)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Exercício 2 — Dados 5D (A/B) + PCA(2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "muA = np.array([0, 0, 0, 0, 0])\n",
    "SigmaA = np.array([\n",
    "    [1.0, 0.8, 0.1, 0.0, 0.0],\n",
    "    [0.8, 1.0, 0.3, 0.0, 0.0],\n",
    "    [0.1, 0.3, 1.0, 0.5, 0.0],\n",
    "    [0.0, 0.0, 0.5, 1.0, 0.2],\n",
    "    [0.0, 0.0, 0.0, 0.2, 1.0]\n",
    "])\n",
    "\n",
    "muB = np.array([1.5, 1.5, 1.5, 1.5, 1.5])\n",
    "SigmaB = np.array([\n",
    "    [1.5, -0.7, 0.2, 0.0, 0.0],\n",
    "    [-0.7, 1.5, 0.4, 0.0, 0.0],\n",
    "    [0.2, 0.4, 1.5, 0.6, 0.0],\n",
    "    [0.0, 0.0, 0.6, 1.5, 0.3],\n",
    "    [0.0, 0.0, 0.0, 0.3, 1.5]\n",
    "])\n",
    "\n",
    "XA = np.random.multivariate_normal(muA, SigmaA, size=500)\n",
    "XB = np.random.multivariate_normal(muB, SigmaB, size=500)\n",
    "X5 = np.vstack([XA, XB])\n",
    "y5 = np.hstack([np.zeros(500), np.ones(500)])\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X2 = pca.fit_transform(X5)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X2[y5==0,0], X2[y5==0,1], s=10, label=\"Classe A\")\n",
    "plt.scatter(X2[y5==1,0], X2[y5==1,1], s=10, label=\"Classe B\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.title(\"Exercício 2 — PCA (5D → 2D)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "A projeção via PCA mostra que as classes A e B têm centros deslocados, mas ainda apresentam forte sobreposição devido à variância dentro de cada grupo. Essa configuração torna a separação linear pouco eficaz, já que não existe um hiperplano simples que separe bem as duas classes. Modelos mais expressivos, que incorporam não-linearidades, são mais adequados para capturar as fronteiras complexas observadas no espaço reduzido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Exercício 3 — Spaceship Titanic: pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, os\n",
    "\n",
    "csv_path = \"data/train.csv\"\n",
    "assert os.path.exists(csv_path), f\"Arquivo não encontrado: {csv_path}\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "target_col = \"Transported\"\n",
    "num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "cat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Cabin\"]\n",
    "\n",
    "print(\"Objetivo do dataset:\")\n",
    "print(\"- Prever 'Transported' (se o passageiro foi transportado para outra dimensão).\")\n",
    "print(\"\\nColunas numéricas:\", num_cols)\n",
    "print(\"Colunas categóricas:\", cat_cols)\n",
    "\n",
    "print(\"\\nTipos detectados (amostra):\")\n",
    "print(df[num_cols + cat_cols + [target_col]].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "na_count = df[num_cols + cat_cols + [target_col]].isna().sum().sort_values(ascending=False)\n",
    "na_pct = (na_count / len(df)).round(3)\n",
    "missing_report = pd.DataFrame({\"missing\": na_count, \"pct\": na_pct})\n",
    "missing_report = missing_report[missing_report[\"missing\"] > 0]\n",
    "missing_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "csv_path = \"data/train.csv\"\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    print(\"Arquivo não encontrado:\", csv_path)\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    target_col = \"Transported\"\n",
    "    num_cols = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\"]\n",
    "    cat_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Cabin\"]\n",
    "\n",
    "    if df[target_col].dtype != int:\n",
    "        df[target_col] = df[target_col].astype(int)\n",
    "\n",
    "    X = df[num_cols + cat_cols]\n",
    "    y = df[target_col].values\n",
    "\n",
    "    try:\n",
    "        cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "    num_transform = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "\n",
    "    cat_transform = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", cat_encoder)\n",
    "    ])\n",
    "\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", num_transform, num_cols),\n",
    "        (\"cat\", cat_transform, cat_cols)\n",
    "    ], remainder=\"drop\")\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    Xt = pre.fit_transform(X_train)\n",
    "    Xv = pre.transform(X_val)\n",
    "\n",
    "    print(\"Shape treino:\", Xt.shape, \" | Shape validação:\", Xv.shape)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3.5))\n",
    "    ax1.hist(df[\"Age\"].dropna(), bins=40)\n",
    "    ax1.set_title(\"Age — original\")\n",
    "\n",
    "    age_pipe = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ])\n",
    "    age_scaled = age_pipe.fit_transform(df[[\"Age\"]])\n",
    "\n",
    "    ax2.hist(age_scaled.ravel(), bins=40)\n",
    "    ax2.set_title(\"Age — padronizada\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def plot_before_after(series):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3.5))\n",
    "    ax1.hist(series.dropna().values, bins=40)\n",
    "    ax1.set_title(f\"{series.name} — original\")\n",
    "    pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                     (\"scaler\", StandardScaler())])\n",
    "    scaled = pipe.fit_transform(series.to_frame())\n",
    "    ax2.hist(scaled.ravel(), bins=40)\n",
    "    ax2.set_title(f\"{series.name} — padronizada\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_before_after(df[\"Age\"])\n",
    "plot_before_after(df[\"FoodCourt\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "O dataset **Spaceship Titanic** tem como objetivo prever a variável *Transported*, que indica se um passageiro foi levado para outra dimensão durante a viagem. As variáveis numéricas incluem atributos como `Age`, `RoomService`, `FoodCourt`, `ShoppingMall`, `Spa` e `VRDeck`, enquanto `HomePlanet`, `CryoSleep`, `Destination`, `VIP` e `Cabin` são categóricas. A inspeção inicial mostra valores ausentes distribuídos em várias colunas, com taxas em torno de 2% em atributos como `CryoSleep`, `HomePlanet` e `Age`. Para tratar essas lacunas, adotou-se a imputação da mediana nos atributos numéricos, por ser robusta a outliers, e do valor mais frequente nos categóricos, garantindo consistência sem gerar categorias artificiais.\n",
    "\n",
    "Na etapa de transformação, as variáveis categóricas foram convertidas em indicadores binários via one-hot encoding, enquanto os atributos numéricos foram padronizados para média zero e desvio um. Essa escolha é adequada porque a função de ativação `tanh` é centrada em zero e responde melhor quando as entradas estão nessa escala. Os histogramas de `Age` e `FoodCourt` antes e depois da padronização confirmam o efeito: os dados, que antes exibiam diferentes magnitudes e dispersões, foram centralizados e normalizados, tornando o conjunto mais homogêneo. Assim, o dataset resultante está preparado para ser usado no treinamento de redes neurais, favorecendo a estabilidade do gradiente e o desempenho do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Conclusões\n",
    "\n",
    "Os experimentos mostraram que, em dados sintéticos 2D, a separação linear não é suficiente, exigindo funções de ativação não lineares para capturar fronteiras mais complexas. Na projeção dos dados 5D em 2D, a sobreposição causada por correlações entre atributos reforça essa limitação e destaca a necessidade de modelos mais expressivos. Já no caso do Spaceship Titanic, o pré-processamento com imputação, codificação categórica e padronização numérica foi fundamental para tornar o conjunto compatível com redes neurais baseadas em `tanh`, garantindo maior estabilidade no treinamento e melhor capacidade de generalização.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
