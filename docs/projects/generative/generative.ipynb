{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 3: Modelos Generativos - Stable Diffusion + ControlNet\n",
    "\n",
    "**Autores:** Caio Boa, Gabriel Hermida e Pedro Civita  \n",
    "**Técnicas:** Stable Diffusion 1.5 + ControlNet Canny\n",
    "\n",
    "---\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Este projeto utiliza Stable Diffusion como modelo base e ControlNet como técnica de controle espacial para geração de designs de produtos. A combinação dessas técnicas permite criar imagens a partir de descrições textuais enquanto mantém controle sobre a estrutura espacial através de condições visuais.\n",
    "\n",
    "### Objetivos\n",
    "\n",
    "1. Implementar pipeline text-to-image com Stable Diffusion 1.5\n",
    "2. Integrar ControlNet Canny para controle via detecção de bordas\n",
    "3. Gerar exemplos de designs variando parâmetros\n",
    "4. Documentar arquitetura com diagramas\n",
    "5. Analisar impacto de hiperparâmetros\n",
    "\n",
    "### Tecnologias\n",
    "\n",
    "- **Diffusers**: Framework para modelos de difusão\n",
    "- **Stable Diffusion 1.5**: Modelo text-to-image (runwayml/stable-diffusion-v1-5)\n",
    "- **ControlNet Canny**: Controle espacial (lllyasviel/sd-controlnet-canny)\n",
    "- **PyTorch**: Backend com aceleração CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Arquitetura dos Modelos\n",
    "\n",
    "Stable Diffusion é um modelo de difusão latente que opera no espaço comprimido de um autoencoder. Seus três componentes principais são: um VAE que comprime imagens 512×512 para latentes 64×64×4, reduzindo o custo computacional; um encoder CLIP que transforma prompts textuais em embeddings de 768 dimensões; e um U-Net que prediz e remove ruído iterativamente, condicionado pelo texto via cross-attention.\n",
    "\n",
    "ControlNet estende o Stable Diffusion adicionando controle espacial através de condições visuais como bordas Canny, mapas de profundidade ou poses. A arquitetura utiliza uma cópia treinável do U-Net que processa a condição em paralelo, conectada ao modelo original por Zero Convolutions—camadas inicializadas com zeros que permitem treinamento estável. Neste projeto, o ControlNet Canny controla as bordas dos designs gerados.\n",
    "\n",
    "### Diagramas da Arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para renderizar diagramas Mermaid no Jupyter/MkDocs\n",
    "from IPython.display import Image as IPImage, display\n",
    "import base64\n",
    "\n",
    "def render_mermaid(mermaid_code, width=800):\n",
    "    \"\"\"\n",
    "    Renderiza diagrama Mermaid usando mermaid.ink API\n",
    "    \"\"\"\n",
    "    mermaid_clean = mermaid_code.strip()\n",
    "    graphbytes = mermaid_clean.encode(\"utf8\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    url = f\"https://mermaid.ink/img/{base64_string}\"\n",
    "    \n",
    "    try:\n",
    "        display(IPImage(url=url, width=width))\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao renderizar diagrama: {e}\")\n",
    "        print(f\"\\nCódigo Mermaid:\\n{mermaid_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mermaid_sd = \"\"\"\n",
    "graph LR\n",
    "    A[Text Prompt] --> B[CLIP Text Encoder]\n",
    "    B --> C[Text Embeddings]\n",
    "    D[Random Noise] --> E[U-Net]\n",
    "    C --> E\n",
    "    E --> F[Denoised Latent]\n",
    "    F --> G[VAE Decoder]\n",
    "    G --> H[Generated Image]\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style H fill:#c8e6c9\n",
    "    style E fill:#fff9c4\n",
    "\"\"\"\n",
    "render_mermaid(mermaid_sd, width=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mermaid_controlnet = \"\"\"\n",
    "graph TB\n",
    "    A[Input Image] --> B[Condition Extractor]\n",
    "    B --> C[Canny Edges / Depth Map]\n",
    "    C --> D[ControlNet Encoder]\n",
    "    E[Text + Noise] --> F[U-Net Original]\n",
    "    D --> G[Zero Convolution]\n",
    "    G --> F\n",
    "    F --> H[Output Latent]\n",
    "    \n",
    "    style C fill:#ffccbc\n",
    "    style D fill:#b3e5fc\n",
    "    style F fill:#fff9c4\n",
    "\"\"\"\n",
    "render_mermaid(mermaid_controlnet, width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mermaid_pipeline = \"\"\"\n",
    "graph TB\n",
    "    subgraph Input\n",
    "        A[Text Prompt]\n",
    "        B[Control Image]\n",
    "    end\n",
    "    \n",
    "    subgraph Text_Processing\n",
    "        C[CLIP Encoder]\n",
    "        D[Text Embeddings]\n",
    "    end\n",
    "    \n",
    "    subgraph Control_Processing\n",
    "        E[Canny/Depth Extractor]\n",
    "        F[ControlNet Encoder]\n",
    "    end\n",
    "    \n",
    "    subgraph Diffusion_Process\n",
    "        G[Random Noise z_T]\n",
    "        H[U-Net + Cross-Attention]\n",
    "        I[Denoised Latent z_0]\n",
    "    end\n",
    "    \n",
    "    subgraph Image_Decoding\n",
    "        J[VAE Decoder]\n",
    "        K[Generated Image]\n",
    "    end\n",
    "    \n",
    "    A --> C --> D --> H\n",
    "    B --> E --> F --> H\n",
    "    G --> H --> I --> J --> K\n",
    "    \n",
    "    style A fill:#e1f5ff\n",
    "    style B fill:#ffe1e1\n",
    "    style K fill:#c8e6c9\n",
    "    style H fill:#fff9c4\n",
    "\"\"\"\n",
    "render_mermaid(mermaid_pipeline, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup e Instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"pandas>=2.2.0\" \"scikit-learn>=1.5.0\" --upgrade\n",
    "!pip install -q diffusers[torch] transformers accelerate controlnet_aux opencv-python matplotlib pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='timm')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='controlnet_aux')\n",
    "warnings.filterwarnings('ignore', message='IProgress not found')\n",
    "warnings.filterwarnings('ignore', message='mediapipe')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    ControlNetModel,\n",
    "    UniPCMultistepScheduler\n",
    ")\n",
    "from controlnet_aux import CannyDetector\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stable Diffusion Text-to-Image\n",
    "\n",
    "O modelo é carregado com otimizações de memória: FP16 reduz o uso pela metade, o carregamento direto na GPU evita cópias intermediárias, e técnicas de slicing processam atenção e VAE em chunks menores. Essas configurações permitem execução em GPUs com memória limitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "os.environ['HF_HOME'] = '/home/gabrielmmh/.cache/huggingface'\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe_sd = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    variant=\"fp16\",\n",
    "    safety_checker=None\n",
    ")\n",
    "pipe_sd = pipe_sd.to(device)\n",
    "pipe_sd.enable_attention_slicing(1)\n",
    "pipe_sd.enable_vae_slicing()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"SD 1.5 loaded | GPU: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Função de Geração Básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basic_image(prompt, negative_prompt=\"\", num_inference_steps=25, guidance_scale=7.5, seed=None):\n",
    "    \"\"\"\n",
    "    Gera imagem usando Stable Diffusion básico (otimizado para memória)\n",
    "    \n",
    "    Args:\n",
    "        prompt: Descrição do que gerar\n",
    "        negative_prompt: O que evitar\n",
    "        num_inference_steps: Passos de denoising (reduzido para 25 para economizar memória)\n",
    "        guidance_scale: Força do condicionamento textual (7-15 recomendado)\n",
    "        seed: Seed para reprodutibilidade\n",
    "    \"\"\"\n",
    "    generator = torch.Generator(device=device).manual_seed(seed) if seed else None\n",
    "    \n",
    "    image = pipe_sd(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "    \n",
    "    # Limpar memória após geração\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cadeira\n",
    "\n",
    "Geração com Stable Diffusion básico, sem controle espacial, servindo como baseline para comparação com ControlNet. O modelo executa 25 steps de inferência com guidance scale 7.5, valor padrão que equilibra fidelidade ao prompt e liberdade criativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = \"modern minimalist chair design, sleek wooden legs, ergonomic seat, product photography, white background, studio lighting, high quality, 4k\"\n",
    "negative_1 = \"blurry, low quality, distorted, ugly, bad anatomy\"\n",
    "\n",
    "image_1 = generate_basic_image(prompt_1, negative_1, num_inference_steps=25, guidance_scale=7.5, seed=42)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_1)\n",
    "plt.axis('off')\n",
    "plt.title(\"Cadeira Moderna (SD Básico)\\nSteps=25, Guidance=7.5\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Smartwatch\n",
    "\n",
    "Geração de produto tecnológico para demonstrar aplicação em categorias distintas. Mantém os mesmos 25 steps e guidance 7.5 do exemplo anterior para permitir comparação direta entre diferentes tipos de produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = \"futuristic smartwatch design, OLED display, titanium band, premium materials, product render, professional lighting\"\n",
    "negative_2 = \"blurry, low quality, cartoon, sketch\"\n",
    "\n",
    "image_2 = generate_basic_image(prompt_2, negative_2, num_inference_steps=25, guidance_scale=7.5, seed=123)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image_2)\n",
    "plt.axis('off')\n",
    "plt.title(\"Smartwatch Futurista\\nSteps=25, Guidance=7.5\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ControlNet para Controle Espacial\n",
    "\n",
    "O ControlNet Canny utiliza detecção de bordas para guiar a geração, permitindo manter controle sobre a composição enquanto varia estilos e materiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-canny\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "pipe_controlnet = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    variant=\"fp16\",\n",
    "    safety_checker=None\n",
    ")\n",
    "pipe_controlnet = pipe_controlnet.to(device)\n",
    "pipe_controlnet.enable_attention_slicing(1)\n",
    "pipe_controlnet.enable_vae_slicing()\n",
    "pipe_controlnet.scheduler = UniPCMultistepScheduler.from_config(pipe_controlnet.scheduler.config)\n",
    "\n",
    "canny_detector = CannyDetector()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"ControlNet loaded | GPU: {torch.cuda.memory_allocated(0) / 1024**2:.0f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_canny_condition(image, low_threshold=100, high_threshold=200):\n",
    "    \"\"\"Cria condição Canny a partir de uma imagem\"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        image = np.array(image)\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = image\n",
    "    \n",
    "    edges = cv2.Canny(gray, low_threshold, high_threshold)\n",
    "    edges = edges[:, :, None]\n",
    "    edges = np.concatenate([edges, edges, edges], axis=2)\n",
    "    \n",
    "    return Image.fromarray(edges)\n",
    "\n",
    "\n",
    "def create_simple_sketch(shape=(512, 512), sketch_type=\"chair\"):\n",
    "    \"\"\"Cria sketch para usar como condição do ControlNet\"\"\"\n",
    "    img = np.ones((*shape, 3), dtype=np.uint8) * 255\n",
    "    \n",
    "    if sketch_type == \"chair\":\n",
    "        # Cadeira moderna de escritório com base giratória\n",
    "        cx, cy = 256, 256  # Centro do canvas\n",
    "        \n",
    "        # Base com rodízios (estrela 5 pontas)\n",
    "        base_y = 420\n",
    "        for i in range(5):\n",
    "            angle = np.radians(i * 72 - 90)\n",
    "            x_end = int(cx + 80 * np.cos(angle))\n",
    "            y_end = int(base_y + 25 * np.sin(angle))\n",
    "            cv2.line(img, (cx, base_y), (x_end, y_end), (0, 0, 0), 2)\n",
    "            cv2.circle(img, (x_end, y_end + 8), 6, (0, 0, 0), 2)  # Rodízios\n",
    "        \n",
    "        # Coluna central\n",
    "        cv2.line(img, (cx, base_y), (cx, 340), (0, 0, 0), 3)\n",
    "        \n",
    "        # Assento com estofado (elipse para volume)\n",
    "        cv2.ellipse(img, (cx, 320), (70, 25), 0, 0, 360, (0, 0, 0), 2)\n",
    "        cv2.ellipse(img, (cx, 315), (65, 20), 0, 180, 360, (0, 0, 0), 1)  # Linha de estofado\n",
    "        \n",
    "        # Encosto ergonômico curvo\n",
    "        pts_back = np.array([\n",
    "            [cx - 55, 310], [cx - 60, 250], [cx - 55, 180], \n",
    "            [cx - 40, 140], [cx, 130], [cx + 40, 140],\n",
    "            [cx + 55, 180], [cx + 60, 250], [cx + 55, 310]\n",
    "        ], np.int32)\n",
    "        cv2.polylines(img, [pts_back], False, (0, 0, 0), 2)\n",
    "        \n",
    "        # Detalhes do estofado no encosto\n",
    "        cv2.ellipse(img, (cx, 220), (45, 60), 0, 0, 360, (0, 0, 0), 1)\n",
    "        \n",
    "        # Braços\n",
    "        # Braço esquerdo\n",
    "        cv2.line(img, (cx - 70, 280), (cx - 100, 270), (0, 0, 0), 2)\n",
    "        cv2.line(img, (cx - 100, 270), (cx - 100, 260), (0, 0, 0), 2)\n",
    "        cv2.line(img, (cx - 100, 260), (cx - 75, 255), (0, 0, 0), 2)\n",
    "        # Braço direito\n",
    "        cv2.line(img, (cx + 70, 280), (cx + 100, 270), (0, 0, 0), 2)\n",
    "        cv2.line(img, (cx + 100, 270), (cx + 100, 260), (0, 0, 0), 2)\n",
    "        cv2.line(img, (cx + 100, 260), (cx + 75, 255), (0, 0, 0), 2)\n",
    "        \n",
    "        # Suportes dos braços\n",
    "        cv2.line(img, (cx - 70, 320), (cx - 70, 280), (0, 0, 0), 2)\n",
    "        cv2.line(img, (cx + 70, 320), (cx + 70, 280), (0, 0, 0), 2)\n",
    "        \n",
    "    elif sketch_type == \"watch\":\n",
    "        cv2.circle(img, (256, 256), 100, (0, 0, 0), 2)\n",
    "        cv2.rectangle(img, (240, 100), (272, 156), (0, 0, 0), 2)\n",
    "        cv2.rectangle(img, (240, 356), (272, 412), (0, 0, 0), 2)\n",
    "        \n",
    "    elif sketch_type == \"bottle\":\n",
    "        cv2.rectangle(img, (220, 100), (292, 150), (0, 0, 0), 2)\n",
    "        cv2.rectangle(img, (200, 150), (312, 450), (0, 0, 0), 2)\n",
    "        \n",
    "    return Image.fromarray(img)\n",
    "\n",
    "\n",
    "def generate_controlnet_image(prompt, canny_image, negative_prompt=\"\", \n",
    "                             num_inference_steps=20, guidance_scale=7.5, \n",
    "                             controlnet_conditioning_scale=1.0, seed=None):\n",
    "    \"\"\"Gera imagem usando ControlNet + Stable Diffusion\"\"\"\n",
    "    generator = torch.Generator(device=device).manual_seed(seed) if seed else None\n",
    "    \n",
    "    image = pipe_controlnet(\n",
    "        prompt=prompt,\n",
    "        image=canny_image,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "        generator=generator\n",
    "    ).images[0]\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Cadeira com ControlNet\n",
    "\n",
    "Com um sketch em perspectiva 3/4 como condição, o ControlNet preserva a estrutura geométrica enquanto preenche materiais e texturas. A geração usa 20 steps com guidance 8.0 e control scale 1.2, valores que priorizam a aderência à condição estrutural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar sketch de cadeira\n",
    "sketch_chair = create_simple_sketch(sketch_type=\"chair\")\n",
    "canny_chair = create_canny_condition(sketch_chair, low_threshold=50, high_threshold=150)\n",
    "\n",
    "prompt_3 = \"luxury leather office chair, ergonomic design, chrome base, professional product photography, 4k, high detail\"\n",
    "negative_3 = \"blurry, low quality, cartoon, distorted\"\n",
    "\n",
    "image_3 = generate_controlnet_image(\n",
    "    prompt_3, \n",
    "    canny_chair, \n",
    "    negative_3,\n",
    "    num_inference_steps=20,\n",
    "    guidance_scale=8.0,\n",
    "    controlnet_conditioning_scale=1.2,\n",
    "    seed=456\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "axes[0].imshow(sketch_chair)\n",
    "axes[0].set_title(\"Sketch Original\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(canny_chair)\n",
    "axes[1].set_title(\"Canny Edges (Condição)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(image_3)\n",
    "axes[2].set_title(\"Resultado Final (ControlNet)\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Cadeira Controlada - ControlNet (Steps=20)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Relógio com ControlNet\n",
    "\n",
    "Teste de controle estrutural em formato circular para avaliar precisão em proporções. O control scale retorna ao valor padrão 1.0, pois a forma simples do sketch não requer aderência tão forte quanto o exemplo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_watch = create_simple_sketch(sketch_type=\"watch\")\n",
    "canny_watch = create_canny_condition(sketch_watch)\n",
    "\n",
    "prompt_watch = \"luxury swiss watch, gold case, leather strap, mechanical movement, classic elegant design, professional photography\"\n",
    "negative_watch = \"blurry, low quality, distorted, toy, digital\"\n",
    "\n",
    "image_watch = generate_controlnet_image(\n",
    "    prompt_watch,\n",
    "    canny_watch,\n",
    "    negative_watch,\n",
    "    num_inference_steps=20,\n",
    "    guidance_scale=7.5,\n",
    "    controlnet_conditioning_scale=1.0,\n",
    "    seed=1000\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "axes[0].imshow(sketch_watch)\n",
    "axes[0].set_title(\"Sketch Base\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(canny_watch)\n",
    "axes[1].set_title(\"Canny Edges\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(image_watch)\n",
    "axes[2].set_title(\"Relógio Clássico Suíço\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Design de Relógio - Controle Estrutural com ControlNet\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Garrafa com ControlNet\n",
    "\n",
    "Design com forma vertical alongada para testar consistência de proporções em geometrias diferentes. O control scale 1.1 oferece aderência moderada à estrutura retangular do sketch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_bottle = create_simple_sketch(sketch_type=\"bottle\")\n",
    "canny_bottle = create_canny_condition(sketch_bottle)\n",
    "\n",
    "prompt_bottle = \"modern stainless steel water bottle, sleek minimalist design, matte black finish, sport cap, premium product photography, white background\"\n",
    "negative_bottle = \"blurry, low quality, distorted, plastic, cheap\"\n",
    "\n",
    "image_bottle = generate_controlnet_image(\n",
    "    prompt_bottle,\n",
    "    canny_bottle,\n",
    "    negative_bottle,\n",
    "    num_inference_steps=20,\n",
    "    guidance_scale=7.5,\n",
    "    controlnet_conditioning_scale=1.1,\n",
    "    seed=2000\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "axes[0].imshow(sketch_bottle)\n",
    "axes[0].set_title(\"Sketch Base\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(canny_bottle)\n",
    "axes[1].set_title(\"Canny Edges\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(image_bottle)\n",
    "axes[2].set_title(\"Garrafa de Água Premium\")\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Design de Garrafa - ControlNet com Formas Verticais\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análise de Resultados\n",
    "\n",
    "O pipeline de Stable Diffusion + ControlNet foi aplicado em cinco exemplos de diferentes categorias de produtos. Os experimentos indicam que guidance scale entre 7-8 equilibra criatividade e fidelidade ao prompt, enquanto 20-25 steps de inferência produzem qualidade adequada. Para o ControlNet, conditioning scale de 1.0-1.2 oferece controle sem suprimir detalhes.\n",
    "\n",
    "As aplicações práticas incluem prototipagem de variações de design e exploração de estilos mantendo estrutura fixa. As limitações observadas são a precisão variável em dimensões exatas, inconsistência entre gerações da mesma prompt e viés do dataset de treinamento.\n",
    "\n",
    "Extensões possíveis incluem combinação de múltiplos ControlNets para controle multi-modal, fine-tuning com LoRA para especialização, integração com modelos text-to-3D e inpainting para edição localizada. Do ponto de vista ético, é necessário verificar designs por plágio, reconhecer vieses do dataset, usar como ferramenta de auxílio ao designer e indicar o uso de IA em materiais comerciais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    \"Exemplo\": [\"1. Cadeira Moderna\", \"2. Smartwatch\", \"3. Cadeira ControlNet\", \"4. Relógio\", \"5. Garrafa\"],\n",
    "    \"Técnica\": [\"SD Básico\", \"SD Básico\", \"SD + ControlNet\", \"SD + ControlNet\", \"SD + ControlNet\"],\n",
    "    \"Steps\": [25, 25, 20, 20, 20],\n",
    "    \"Guidance\": [7.5, 7.5, 8.0, 7.5, 7.5],\n",
    "    \"Control Scale\": [\"-\", \"-\", 1.2, 1.0, 1.1]\n",
    "}\n",
    "pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Insights Técnicos\n",
    "\n",
    "O guidance scale ideal situa-se entre 7-8, pois valores abaixo de 5 geram imagens desconectadas do prompt e acima de 12 causam saturação excessiva. Os 20-25 inference steps são suficientes, com ganhos marginais acima de 30. O conditioning scale do ControlNet entre 1.0-1.2 mantém controle sem restringir a criatividade do modelo.\n",
    "\n",
    "As otimizações de memória aplicadas reduziram o consumo de 8-10GB para 3.5-4GB. O FP16 corta o uso pela metade, o low_cpu_mem_usage evita cópias intermediárias, o attention_slicing reduz picos em cerca de 30%, o VAE_slicing processa em tiles, e a limpeza de cache entre gerações libera memória não utilizada.\n",
    "\n",
    "O ControlNet mantém estrutura consistente através de variações de estilo, separa a definição de composição do preenchimento de detalhes, facilita iteração sobre a mesma estrutura e acelera o workflow de design ao permitir múltiplas variações sobre uma base fixa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
